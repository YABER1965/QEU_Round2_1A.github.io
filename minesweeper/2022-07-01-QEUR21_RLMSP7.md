---
title: QEUR21_RLMSP7:　本実験～畳み込みRT法プラスDQN-ERでマインスイーパー(その2)
date: 2022-07-01
tags: ["QEUシステム", "メトリックス", "Python言語", "強化学習", "マインスイーパー", "ディープラーニング"]
excerpt: Python言語によるマインスイーパーの強化学習
---

## QEUR21_RLMSP7:　本実験～畳み込みRT法プラスDQN-ERでマインスイーパー(その2)

## ～　SOART、もっと「ひねりが欲しい」・・・　～

### ・・・　つづきです　・・・

D先生 ： “私は別に「SOARTメトリックス推し」じゃないです。なんというか・・・、**ちょっとSOART法は「うまくいっていない」**というか・・・。”

QEU:FOUNDER ： “もうちょっとだけ「ひねり」が必要なんだよね、SOARTは・・・。外観検査機の亮度の補正には使えますが・・・。本来ならば、RTメトリックスのY1は「第1主成分」、Y2は「第2主成分」そして第三メトリックスY3が「SN比」であるべきなんです。”

![imageRL2-26-1](/2022-07-01-QEUR21_RMLSP7/imageRL2-26-1.jpg)

D先生 ： “現状のスキームでは標準ベクトル1件と計測対象ベクトル1件の比較でやっているでしょ？**インプット情報は2件で、アウトプットのメトリックスは3件・・・。そのような変換は情報学上ありえない・・・。**”

![imageRL2-26-2](/2022-07-01-QEUR21_RMLSP7/imageRL2-26-2.jpg)

QEU:FOUNDER ： “だから現状のSOART法ではうまく行かない・・・。もっと計測対象ベクトルのインプットを増やすとなると、**おそらく「誤差因子」を加味してインプット情報量を増やすべき**でしょうね。・・・とはいっても現状のテーマではしょうがないので、相変わらずにSOARTメトリックスを入力させてやってみました。”

```python
# ----------------
# MINE_SWEEPERゲームのシミュレーションシステム
# step本2 : RTを動的メトリックスとして活用する
# step本2 : dqn_wER_msGame(agent_full_SOART).py
# タイプB : 使用している状態(STATE)は、SOARTメトリックス（ベンド、ライン、データム）です。
# 注意 : 本実験用です
# Environment(Kylie Ying_minesweeper.py)は以下のWebからの引用です
# https://www.youtube.com/watch?v=8ext9G7xspg&t=9144s
# ---------------- 
# 数値計算のﾗｲﾌﾞﾗﾘ読み込み
import math
import numpy as np
import copy, random, time
import pandas as pd
from collections import deque, Counter
from scipy.special import softmax
from IPython.display import clear_output
# ----------------
import torch
import torch.nn.functional
import torch.utils.data
# ---------------- 
import matplotlib.pyplot as plt
#%matplotlib inline
# ------
# environment
import Kylie_Ying_minesweeper_env2

#=================================================
# READ Convolutional CSV FILES
#=================================================
# 畳み込み部品パターンファイルを読み込み
def read_csvfile(file_readcsv, max_idx, max_col): 
 
    # ---------------------------
    # 畳み込み部品パターンファイルの読み込み
    df = pd.read_csv(file_readcsv, header=None) 
    #print(df)
    # ---------------------------
    # 原因系Xs
    mx_Xs = df.iloc[0:max_idx,0:max_col].values
    #print("----- mx_Xs:{0} -----".format(file_readcsv))
    #print(mx_Xs)
  
    return np.array(mx_Xs)

# ---------------------------
# 畳み込み処理を実施する
def apply_kernel(row, col, kernel, img_tensor):
    return (img_tensor[row:row+max_cnv_idx,col:col+max_cnv_col] * kernel).sum()

# ---------------------------
# soaRTメトリックスを計算する(テンソル活用版)
def calc_soaRT(len_temp, max_jy_index, tsr_sig_matrix, tsr_tani_array): 

    # ---------------------------
    # 変数の初期化
    L1_loss = torch.nn.L1Loss()
    L2_loss = torch.nn.MSELoss()
    tsr_zero = torch.zeros(max_jy_index)
    btY1_yarray, Y2_yarray, Y3_yarray = [], [], []

    # 繰り返し
    for i in range(len_temp):

        y = tsr_sig_matrix[i]
        x = tsr_tani_array

        # 回転を計測
        xx = torch.dot(x,x)
        xy = torch.dot(x,y)
        beta = xy/xx
        #print("i:{}, beta:{}".format(i,beta))

        # 体積ひずみを計測
        vol_xtani = L2_loss(beta*x, tsr_zero).item()
        vol_ysig  = L2_loss(y, tsr_zero).item()
        if vol_xtani > 0.01:
            ratio_vol = np.sqrt(vol_ysig/vol_xtani)
        else:
            ratio_vol = 1.0

        # せん断ひずみを計測
        mDistance   = L1_loss(y, beta*ratio_vol*x)
        #print("mDistance: ", mDistance.item())
        #print("yres:{}".format(yres))
        
        btY1_yarray.append(beta.item())
        Y2_yarray.append(ratio_vol)
        Y3_yarray.append(mDistance.item())

    return btY1_yarray, Y2_yarray, Y3_yarray

# ---------------- 
# メトリックスを束ねて2X2行列(マトリックス)を生成する
def mx_generation(board, signal_kernels, tani_kernel): 

    # 変数を初期化する
    tsr_state  = torch.tensor(board)
    #print(tsr_state)

    # ------------------
    # feature-engineeringの計算(新RT法)
    # 単位空間用の畳み込みテンソルを生成する  
    tsr_tani_matrix = torch.zeros(4, cmax_jy_idx).float()
    tsr_tani_matrix[0,:] = torch.tensor([[apply_kernel(i,j, tani_kernel, tsr_state) for j in [0,1,2]] for i in [0,1,2]]).flatten()
    tsr_tani_matrix[1,:] = torch.tensor([[apply_kernel(i,j, tani_kernel, tsr_state) for j in [0,1,2]] for i in [6,7,8]]).flatten()
    tsr_tani_matrix[2,:] = torch.tensor([[apply_kernel(i,j, tani_kernel, tsr_state) for j in [6,7,8]] for i in [0,1,2]]).flatten()
    tsr_tani_matrix[3,:] = torch.tensor([[apply_kernel(i,j, tani_kernel, tsr_state) for j in [6,7,8]] for i in [6,7,8]]).flatten()
    #print("----- tsr_tani_matrix -----")
    #print(tsr_tani_matrix)
    
    # ------------------
    # 信号空間用の畳み込みテンソルを生成する 
    tsr_sig_matrix  = torch.zeros(4, len_cmx, cmax_jy_idx).float()
    calc_conv       = torch.zeros(4, cmax_jy_idx).float()
    for iCnv in range(len_cmx):     # len_cmx
        # ---------------------------
        # CSVファイル(実験)情報を読み込み表示する
        kernel = signal_kernels[iCnv]
        calc_conv[0,:] = torch.tensor([[apply_kernel(i,j, kernel, tsr_state) for j in [0,1,2]] for i in [0,1,2]]).flatten()
        calc_conv[1,:] = torch.tensor([[apply_kernel(i,j, kernel, tsr_state) for j in [0,1,2]] for i in [6,7,8]]).flatten()
        calc_conv[2,:] = torch.tensor([[apply_kernel(i,j, kernel, tsr_state) for j in [6,7,8]] for i in [0,1,2]]).flatten()
        calc_conv[3,:] = torch.tensor([[apply_kernel(i,j, kernel, tsr_state) for j in [6,7,8]] for i in [6,7,8]]).flatten()
        # -----
        if iCnv == 0:
            for iCell in range(4):
                tsr_sig_matrix[iCell, iCnv, :] = calc_conv[iCell]
        elif iCnv == 1:
            for iCell in range(4):
                tsr_sig_matrix[iCell, iCnv, :] = calc_conv[iCell]
        elif iCnv == 2:
            for iCell in range(4):
                tsr_sig_matrix[iCell, iCnv, :] = calc_conv[iCell]
        elif iCnv == 3:
            for iCell in range(4):
                tsr_sig_matrix[iCell, iCnv, :] = calc_conv[iCell]
        elif iCnv == 4:
            for iCell in range(4):
                tsr_sig_matrix[iCell, iCnv, :] = calc_conv[iCell]
        elif iCnv == 5:
            for iCell in range(4):
                tsr_sig_matrix[iCell, iCnv, :] = calc_conv[iCell]
    
    # ------------------
    # 信号空間用の畳み込みテンソルを生成する 
    btY1_matrix, Y2_matrix, Y3_matrix = np.zeros([4, len_cmx]), np.zeros([4, len_cmx]), np.zeros([4, len_cmx])
    for iCell in range(4):     # len_cmx
        # newRTメトリックスを計算する(テンソル活用版)
        btY1_matrix[iCell, :], Y2_matrix[iCell, :], Y3_matrix[iCell, :] = calc_soaRT(len_cmx, cmax_jy_idx, tsr_sig_matrix[iCell], tsr_tani_matrix[iCell])

    # ------------------
    # STATEを生成する 
    state = np.hstack([btY1_matrix.flatten(), Y2_matrix.flatten(), Y3_matrix.flatten()])
    #print("---- state ----")
    #print(state)
    
    return state 

#=================================================
# function for finding addresses
#=================================================
# 一次元 -> 二次元アドレスに変更する
def decode_address(num_address, dim_size):

    row = int(num_address / dim_size + 0.001)
    col = int(num_address - row * dim_size + 0.001)
    print("row:{}, col:{}".format(row, col))

    return row, col
    
# --------------------------------------------------
# 実行しやすいアクションの一覧(-2値)を作成する
def avail_action(Calc_board, dim_size):

    # --------------------------------------------------
    # 値-2のタイル群のアドレスを抽出し、命令候補群とする
    movables, nums_avail = [], []
    iCnt = 0
    for row in range(dim_size):
        for col in range(dim_size):
            if Calc_board[row, col] > -2.1 and Calc_board[row, col] < -1.9:
                movables.append((row,col))
                nums_avail.append(int(row*dim_size+col))
                iCnt = iCnt + 1
    # ----------
    #print("movables:",movables)
    
    return movables, nums_avail

#=================================================
# Deep Learning Model class            
#=================================================
# PyTorchのDLの定義
class Net(torch.nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = torch.nn.Linear(dim_input, 128)
        self.fc2 = torch.nn.Linear(128, 128)
        self.fc3 = torch.nn.Linear(128, dim_output)
        
    def forward(self, x):
        x = torch.nn.functional.relu(self.fc1(x))
        x = torch.nn.functional.relu(self.fc2(x))
        x = self.fc3(x)
        return x

#=================================================
# Calculation class(1) : DQN_Solver:          
#=================================================
# Solving the minesweeper in Deep Q-learning
class DQN_Solver:

    def __init__(self, state_size, action_size):
    
        self.state_size  = state_size
        self.action_size = action_size
        self.memory  = deque(maxlen=MEMORY_CAPACITY)
        self.gamma   = GAMMA
        self.epsilon = EPSILON
        self.e_decay = EPDECAY
        self.e_min   = EPMIN
        self.learning_rate = LRATE

        # --------------------------------------------------
        # crate instance for input
        self.model = Net()
        # --------------------------------------------------
        # Save and load the model via state_dict
        #self.model.load_state_dict(torch.load(file_input_model))
        # --------------------------------------------------
        # set validaiton mode
        self.model.eval()

    # ----------------
    # PyTorchモデルの保存  
    #def save_models(self):
        #torch.save(self.model.state_dict(), file_output_model)

    # ----------------
    # メモリを蓄積する    
    def remember_memory(self, state, action, reward, state_next, movables_next, nums_avail_next, done):
        self.memory.append((state, action, reward, state_next, movables_next, nums_avail_next, done))

    # ----------------
    # アクション(-2)から命令を選択する
    def choose_action(self, movables, nums_avail, state, dim_size, iCnt_turn):

        a_row, a_col, Qvalue = -99, -99, -0.0001
            
        # --------------------------------------------------
        # オリジナル操作方法 -> 開いている場所のアドレスを入力します
        #  --- string_rep(print_board) ---
        #    0  1  2  3  4  5  6  7  8  9  10  11  
        # ----------------------------------------
        # 0 |1 |  |  |  |  |  |  |  |  |1 |0 |0 |
        # 1 |  |  |  |  |  |  |  |  |  |1 |0 |0 |
        # 2 |  |  |  |  |2 |1 |1 |1 |1 |1 |0 |0 |
        # 3 |  |  |  |  |1 |0 |0 |0 |0 |0 |0 |0 |
        # 4 |  |  |  |  |1 |0 |0 |0 |0 |0 |0 |0 |
        # 5 |  |  |  |  |1 |0 |0 |0 |0 |0 |0 |0 |
        # 6 |1 |1 |1 |  |2 |1 |0 |0 |0 |0 |0 |0 |
        # 7 |0 |1 |1 |2 |  |1 |0 |0 |0 |0 |0 |0 |
        # 8 |0 |1 |  |2 |1 |1 |0 |0 |0 |0 |0 |0 |
        # 9 |0 |1 |1 |1 |0 |0 |0 |1 |1 |1 |0 |0 |
        # 10 |0 |0 |0 |0 |0 |0 |0 |2 |  |2 |0 |0 |
        # 11 |0 |0 |0 |0 |0 |0 |0 |2 |  |2 |0 |0 |
        # ----------------------------------------
        #
        # --- string_rep(visible_board_2) ---
        #    0    1    2    3    4    5    6    7    8    9    10   11   
        # ----------------------------------------------------------------
        # 0 |-10 |-10 |-10 |-10 |-10 |-10 |-10 |-10 |-2  |1   |0.1 |0.1 |
        # 1 |-10 |-10 |-10 |-10 |-2  |-2  |-2  |-2  |-2  |1   |0.1 |0.1 |
        # 2 |-10 |-10 |-10 |-2  |2   |1   |1   |1   |1   |1   |0.1 |0.1 |
        # 3 |-10 |-10 |-10 |-2  |1   |0.1 |0.1 |0.1 |0.1 |0.1 |0.1 |0.1 |
        # 4 |-10 |-10 |-10 |-2  |1   |0.1 |0.1 |0.1 |0.1 |0.1 |0.1 |0.1 |
        # 5 |-2  |-2  |-2  |-2  |1   |0.1 |0.1 |0.1 |0.1 |0.1 |0.1 |0.1 |
        # 6 |1   |1   |1   |-2  |2   |1   |0.1 |0.1 |0.1 |0.1 |0.1 |0.1 |
        # 7 |0.1 |1   |1   |2   |-2  |1   |0.1 |0.1 |0.1 |0.1 |0.1 |0.1 |
        # 8 |0.1 |1   |-2  |2   |1   |1   |0.1 |0.1 |0.1 |0.1 |0.1 |0.1 |
        # 9 |0.1 |1   |1   |1   |0.1 |0.1 |0.1 |1   |1   |1   |0.1 |0.1 |
        # 10 |0.1 |0.1 |0.1 |0.1 |0.1 |0.1 |0.1 |2   |-2  |2   |0.1 |0.1 |
        # 11 |0.1 |0.1 |0.1 |0.1 |0.1 |0.1 |0.1 |2   |-2  |2   |0.1 |0.1 |
        # ----------------------------------------------------------------
        # メトリックスを束ねて2X2行列(マトリックス)を生成する
        #print("movables: ", movables)
        #print("nums_avail: ", nums_avail)

        # ----------------------------------------
        # 最適命令の選択
        if self.epsilon <= random.random():
            # DQNによる選択
            acType  = "machine"
            iCnt = 0
            for a in nums_avail:
                # [array]-action を生成します。
                #print("state:",state)
                #print("a:",a)
                temp_s_a = np.hstack([state, [a]])
                if iCnt == 0:
                    mx_input = np.array([temp_s_a])  # 状態(S)行動(A)マトリックス
                else:
                    mx_input = np.append(mx_input, np.array([temp_s_a]), axis=0)  # 状態(S)行動(A)マトリックス
                iCnt = iCnt + 1
            # --------------------------------------------------
            # generate new 'x'
            x_input_tensor = torch.from_numpy(mx_input).float()
            # predict 'y'
            with torch.no_grad():
                y_pred_tensor = self.model(x_input_tensor)
            # convert tensor to numpy
            y_pred      = y_pred_tensor.data.numpy().flatten()
            Qvalue      = np.max(y_pred)
            temp_action = np.argmax(y_pred)
            a_order     = nums_avail[temp_action]
            a_row, a_col  = decode_address(a_order, dim_size)
        else: 
            # 乱数による選択
            acType  = "random"
            a_order = np.random.choice(nums_avail)
            a_row, a_col  = decode_address(a_order, dim_size)

        return a_row, a_col, a_order, acType, Qvalue

    # ----------------
    # (REPLAY EXPERIENCE)学習する
    def replay_experience(self, batch_size):

        # set training parameters
        optimizer = torch.optim.SGD(self.model.parameters(), lr=self.learning_rate)
        criterion = torch.nn.L1Loss()
        # --------------------------------------------------
        batch_size = min(batch_size, len(self.memory))
        minibatch = random.sample(self.memory, batch_size)
        X, Y = np.array([]), np.array([])
        for ibat in range(batch_size):
            state, action, reward, state_next, movables_next, nums_avail_next, done = minibatch[ibat]
            #print("nums_avail_next: ",nums_avail_next)
            # [array]-action を結合します。
            state_action_curr = np.hstack([state, [action]])
            #print("state_action_curr",state_action_curr)
            if done:
                target_f = reward
            else:
                mx_input = []  # 状態(state)と行動(action)マトリックス
                iCnt = 0
                for imov in nums_avail_next:
                    # [array]-action を結合します。
                    temp_s_a = np.hstack([state_next, [imov]])
                    if iCnt == 0:
                        mx_input = [temp_s_a]  # 状態(S)行動(A)マトリックス
                    else:
                        mx_input = np.concatenate([mx_input, [temp_s_a]], axis=0)  # 状態(S)行動(A)マトリックス
                    iCnt = iCnt + 1
                    # ----
                mx_input = np.array(mx_input)
                #print("--- mx_input ---")
                #print(mx_input)
                # --------------------------------------------------
                # generate new 'x'
                x_input_tensor = torch.from_numpy(mx_input).float()
                # predict 'y'
                with torch.no_grad():
                    y_pred_tensor = self.model(x_input_tensor)
                # convert tensor to numpy
                y_pred = y_pred_tensor.data.numpy().flatten()
                np_n_r_max = np.amax(y_pred)
                target_f = reward + self.gamma * np_n_r_max
            # -----
            if ibat == 0:
                X = [state_action_curr]  # 状態(S)行動(A)マトリックス
            else:
                X = np.append(X, [state_action_curr], axis=0)  # 状態(S)行動(A)マトリックス
            Y = np.append(Y,target_f)

        # --------------------------------------------------
        # TRAINING
        # convert numpy array to tensor
        state_action_next = torch.from_numpy(X).float()
        expected_q_value = torch.from_numpy(Y.reshape(-1, 1)).float()
        
        # --- building model ---
        q_value = self.model(state_action_next)

        # calculate loss
        loss = criterion(q_value, expected_q_value)

        # update weights
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Show progress
        print('learn done -- [epsilon: {0}, loss: {1}]'.format(self.epsilon, loss))
        
        if self.epsilon > self.e_min:
            self.epsilon *= self.e_decay

        return self.epsilon, loss.data.numpy()

#=================================================
# Calculation class(2) : Agent
#=================================================
class Agent():
    # -----
    def __init__(self):

        # 記録用パラメタ類
        self.dim_size, self.num_bombs = dim_size, num_bombs

        # イプシロンの設定
        self.epsilon    = EPSILON     # ε-Greedy

        # --------------------------------------------------
        # 記録用パラメタ類(プレイベース)
        self.arr_iplay     = []  # count game play    プレイ番号
        self.arr_csv_play  = []  # name game play    プレイファイル名のリスト
        self.arr_maxturn   = []  # turn game play    ターン数
        self.arr_maxscore  = []  # rl_score game play    報酬の総和
        self.arr_victory   = []  # victory    勝利したか
        self.arr_loss      = []  # DQN-Experience Replay学習
        self.arr_epsilon   = []  # ε-Greedy  

        # ---------------------------
        # 記録用パラメタ類(ターンベース)
        self.arr_icount    = []    # ターン・カウンタリスト
        self.orders_row    = []    # 指示リスト(row)
        self.orders_col    = []    # 指示リスト(col)
        self.arr_orders    = []    # 指示リスト(アドレス)
        self.arr_scores    = []    # ゲームスコアリスト
        self.arr_dones     = []    # ゲームオーバーフラグ
        self.mx_state      = []    # 状態マトリックス
        self.arr_ym10      = []    # 「-10値」の数
        self.arr_ym2       = []    # 「-2値」の数
        self.arr_numadr    = []    # 最大アドレス数
        self.mx_addr       = []    # アドレスのマトリックス
        self.arr_predQV    = []    # Q値のリスト
        
        # --------------------------------------------------
        # 畳み込み用部品(8種類)を読み込む
        for i_cnv in range(max_cnv_parts):    # max_cnv_parts
            # -----
            # 畳み込みファイル
            file_cnv_input = folder_cvinput + code_cnv_input[i_cnv] + ".csv"  # ファイルパス名の生成 
            mx_conv = read_csvfile(file_cnv_input, max_cnv_idx, max_cnv_col)
            if i_cnv == 0:    
                tsr_bend1 = torch.tensor(mx_conv).float()
            elif i_cnv == 1:
                tsr_bend2 = torch.tensor(mx_conv).float()
            elif i_cnv == 2:
                tsr_bend3 = torch.tensor(mx_conv).float()
            elif i_cnv == 3:
                tsr_bend4 = torch.tensor(mx_conv).float()
            elif i_cnv == 4:
                tsr_line1 = torch.tensor(mx_conv).float()
            elif i_cnv == 5:
                tsr_line2 = torch.tensor(mx_conv).float()
            elif i_cnv == 6:
                tsr_datum1 = torch.tensor(mx_conv).float()
            elif i_cnv == 7:
                tsr_datum2 = torch.tensor(mx_conv).float()

        # --------------------------------------------------
        # 畳み込みカーネルを生成する
        self.signal_kernels = torch.stack([tsr_bend1, tsr_bend2, tsr_bend3, tsr_bend4, tsr_line1, tsr_line2])
        self.tani_kernel    = tsr_datum1 + tsr_datum2
        #print(self.signal_kernels)

        # ---------------------------
        # Q値の分析用(プレイベース)
        self.arr_numQV     = []  # QV値のN数
        self.arr_maxQV     = []  # QV値の最大値
        self.arr_q25QV     = []  # QV値の4分の1値
        self.arr_q75QV     = []  # QV値の4分の3値

        # ---------------------------
        # CSV-playlist用のリスト
        #self.arr_usefile    = np.zeros(num_episodes)        # usefile    CSVファイルを出力するか use 1, unuse 0
        #self.arr_perform    = np.zeros(num_episodes)        # performance   命令の一致率
        
        # --------------------------------------------------
        # エピソードを実行する
        for iCnt_play in range(num_episodes):       # num_episodes
        
            maxturn, maxscore, flg_victory = self.get_episode(iCnt_play)

            # ----------
            # DQN-Experience Replay学習
            if maxturn > 4:          
                val_epsilon, val_loss = dql_solver.replay_experience(BATCH_SIZE)

                # ----------
                # ゲームデータの出力
                code_csvout = "NA"
                if iCnt_play%40 == 0:
                    # CSV ファイル (file_csvout) として出力する
                    code_csvout = "minesweeper_play{0}_score{1}.csv".format(iCnt_play, maxscore)       # file name  
                    #print("メトリックス保管用CSVファイル名 ：{0}".format(code_csvout))
                    #code_csvout = self.save_csvGAME(iCnt_play, code_csvout)        # CSVファイルに保存する

                # ----------
                # 記録用パラメタ類(プレイベース)の追加
                self.arr_iplay.append(iCnt_play)        # count game play    プレイ番号
                self.arr_csv_play.append(code_csvout)       # name game play    プレイファイル名のリスト
                self.arr_maxturn.append(maxturn)        # max_turn   ゲームのターン数
                self.arr_maxscore.append(maxscore)      # rl_score game play    最終プレイスコア
                self.arr_victory.append(flg_victory)    # victory    勝利したか
                self.arr_loss.append(val_loss)          # DQN-Experience Replay学習
                self.arr_epsilon.append(val_epsilon)    # イプシロン       
                # ----------
                # Q値の保管(プレイベース)
                self.arr_numQV.append(maxturn)     # QV値のN数
                self.arr_maxQV.append(np.max(self.arr_predQV))                 # QV値の最大値
                self.arr_q25QV.append(np.percentile(self.arr_predQV, q=25))    # QV値の4分の1値
                self.arr_q75QV.append(np.percentile(self.arr_predQV, q=75))    # QV値の4分の3値

            # ----------
            # しばらくすれば表示が消えます
            if iCnt_play%20 == 0:
                time.sleep(SLEEP_TIME)
                clear_output(wait=True)

        # --------------------------------------------------
        # 学習履歴を出力する
        fig = plt.figure(figsize=(14, 6))
        ax1 = fig.add_subplot(1, 2, 1)
        ax1.set_title('learning transition : epsilon')
        ax1.plot(self.arr_iplay, self.arr_epsilon, label="epsilon", color="blue")
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('epsilon')
        ax1.legend(loc='best')
        # -----
        ax2 = fig.add_subplot(1, 2, 2)
        ax2.set_title('learning transition : Learning loss')
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('Loss amount')
        ax2.plot(self.arr_iplay, self.arr_loss, label="loss", color="blue")
        ax2.legend(loc='best')
        plt.show()

        # --------------------------------------------------
        # グラフを表示する
        self.show_graph(num_episodes)

        # --------------------------------------------------
        # ゲームプレイリストをCSVファイルに保存する
        #print("ゲームプレイリストをCSVファイルに保存する")
        #self.save_playlist()

    # --------------------------------------------------
    # エピソードを運用する
    def get_episode(self, iCnt_play):

        # ---------------------------
        # 機械学習用のパラメタ
        row, col, num_addr, Qvalue = -1, -1, -1, 0
        reward, done, flg_victory = 0, False, False
        num_remain, maxturn, maxscore = self.dim_size ** 2, 0, 0
        Calc_board_next = np.zeros([self.dim_size, self.dim_size])
        state, state_next = np.zeros(48), np.zeros(48)
        
        # ---------------------------
        # ゲームをリセットする
        Calc_board, movables, nums_avail = self.reset(iCnt_play)
        # 異常処理（nums_avail）
        if len(nums_avail) == 0:
            return maxturn, maxscore, flg_victory
        # ---------------------------
        # メトリックスを束ねて2X2行列(マトリックス)を生成する
        state = mx_generation(Calc_board, self.signal_kernels, self.tani_kernel)

        # ---------------------------
        # ゲームをプレイする
        iCnt_turn = 0
        while len(gamepanel.dug) < self.dim_size ** 2 - self.num_bombs:

            # 命令と状態(STATE)を作成する
            row, col, a_order, acType, Qvalue = dql_solver.choose_action(movables, nums_avail, state, self.dim_size, iCnt_turn)
            print("iCnt_turn:{0}, row:{1}, col:{2}, acType:{3}".format(iCnt_turn, row, col, acType))
            if row < 0 or row >= self.dim_size or col < 0 or col >= self.dim_size:
                print("Invalid location. Try again.")
                continue
            # ---------------------------
            # if it's valid, we dig
            done = gamepanel.dig(row, col)
            # 掘ったあとの状態の出力
            #num_remain  = self.dim_size ** 2 - self.num_bombs - len(gamepanel.dug)
            gamepanel.print_board()
            visible_board_next = gamepanel.print_board_2(iCnt_play, iCnt_turn)
            # ----------------
            # 状態(STATE)を作成する
            for iRow in range(self.dim_size):    
                for jCol in range(self.dim_size):
                    Calc_board_next[iRow, jCol] = float(visible_board_next[iRow][jCol])
            # 発行可能な命令のリストを作成する
            movables_next, nums_avail_next  = avail_action(Calc_board_next, self.dim_size)
            # メトリックスを束ねて2X2行列(マトリックス)を生成する
            state_next = mx_generation(Calc_board_next, self.signal_kernels, self.tani_kernel)

            # ---------------------------
            # CCを定義する
            board_flatten = np.array(Calc_board).flatten()
            cc = Counter(board_flatten)
            # 記録用リストを追記する
            self.arr_icount.append(iCnt_turn)   # ターン・カウンタリスト
            self.orders_row.append(row)         # 指示リスト(ROW)
            self.orders_col.append(col)         # 指示リスト(COL)
            self.arr_orders.append(a_order)     # 指示リスト(アドレス)
            self.arr_acType.append(acType)      # 指示のタイプ
            #self.arr_scores.append(reward)      # ゲームスコアリスト   
            self.arr_dones.append(done)     # ゲームオーバーフラグ
            if iCnt_turn == 0:
                self.mx_state       = [state]            # 状態マトリックス(Current)  
                self.mx_state_next  = [state_next]       # 状態マトリックス(Next) 
            else:
                self.mx_state = np.concatenate([self.mx_state, [state]], axis=0)       # 状態マトリックス(Current)   
                self.mx_state_next = np.concatenate([self.mx_state_next, [state_next]], axis=0) # 状態マトリックス(Next) 
            self.arr_ym10.append(cc[-10])       # 「-10値」の数
            self.arr_ym2.append(cc[-2])         # 「-2値」の数
            self.arr_numadr.append(len(nums_avail))    # 最大アドレス数
            self.arr_predQV.append(Qvalue)      # Q値のリスト
            # ----------------
            # 実行継続するか分岐
            if done == True or len(nums_avail_next) == 0: 
                # dug a bomb ahhhhhhh
                break # (game over rip)
            else:
                reward = 1
                # Experience Replay配列に保管する
                dql_solver.remember_memory(state, a_order, reward, state_next, movables_next, nums_avail_next, done)
                self.arr_scores.append(reward)       # ゲームスコアリスト
                # count of game turn
                Calc_board = Calc_board_next
                movables   = movables_next
                nums_avail = nums_avail_next
                state      = state_next
                iCnt_turn  = iCnt_turn + 1

        # --------------------------------------------------
        # 2 ways to end loop, lets check which one
        if done == True or len(nums_avail_next) == 0:
            if len(nums_avail_next) == 0:
                reward = 0
            else:
                # Experience Replay配列に保管する
                dql_solver.remember_memory(state, a_order, reward, state_next, movables_next, nums_avail_next, done)
                reward = -100 
                print("-------- SORRY GAME OVER :( --------")
                # let's reveal the whole board!
                gamepanel.dug = [(r,c) for r in range(self.dim_size) for c in range(self.dim_size)]
                gamepanel.print_board()
                gamepanel.print_board_2(iCnt_play, iCnt_turn)
        else:
            reward = 100
            print("CONGRATULATIONS!!!! YOU ARE VICTORIOUS!")
            flg_victory = True
            # Experience Replay配列に保管する
            dql_solver.remember_memory(state, a_order, reward, state_next, movables_next, nums_avail_next, done)
        # --------------------------------------------------  
        # 記録用リストを追記する
        self.arr_scores.append(reward)       # ゲームスコアリスト
        # プレイデータの引継ぎ
        maxturn, maxscore = iCnt_turn, np.sum(self.arr_scores)

        return maxturn, maxscore, flg_victory

    # ----------------
    # ゲームをﾘｾｯﾄする
    def reset(self, iCnt_play):

        # ----------------
        # 記録用パラメタ類のリセット
        self.arr_icount    = []       # ターン・カウンタリスト
        self.orders_row    = []       # 指示リスト
        self.orders_col    = []       # 指示リスト
        self.arr_orders    = []       # 指示リスト(アドレス)
        self.arr_acType    = []       # 指示のタイプ
        self.arr_scores    = []       # ゲームスコアリスト
        self.arr_dones     = []       # ゲームオーバーフラグ
        self.mx_state      = []       # 状態マトリックス
        self.arr_ym10      = []       # 「-10値」の数
        self.arr_ym2       = []       # 「-2値」の数
        self.arr_numadr    = []       # 最大アドレス数
        self.mx_addr       = []       # アドレスのマトリックス
        self.arr_predQV    = []       # Q値のリスト 
        Calc_board    = np.zeros([self.dim_size, self.dim_size])
        
        # ----------------
        # リセットする
        gamepanel.reset()

        # ----------------
        # 1回目の命令を発行する
        select_row, select_col = 0, 0
        for r in [1,11,5,7,9,3]:
            for c in [1,7,5,3,9,11]:
                if gamepanel.board[r][c] != '*':
                    select_row, select_col = r, c
                    break
        # 1回目掘る
        done = gamepanel.dig(select_row, select_col)
        
        # ----------------
        #print("リセット： len(gamepanel.dug): {0}".format(len(gamepanel.dug))
        gamepanel.print_board()
        visible_board = gamepanel.print_board_2(iCnt_play, 0)
        # ----------------
        # 状態(STATE)を作成する
        for iRow in range(self.dim_size):    
            for jCol in range(self.dim_size):
                Calc_board[iRow, jCol] = float(visible_board[iRow][jCol])
        #print("-- Calc_board--iCnt_play{0}, iCnt_turn:{1} --".format(iCnt_play, iCnt_turn))
        #print(Calc_board)
        
        # ----------------
        # 発行可能な命令のリスト
        movables, nums_avail  = avail_action(Calc_board, self.dim_size)

        return Calc_board, movables, nums_avail
        
    # ----------------
    # 学習結果のグラフ化
    def show_graph(self, num_episodes):

        fig2 = plt.figure(figsize=(14, 12))
        # -----
        ax1 = fig2.add_subplot(2, 2, 1)
        ax1.set_title('DQN parameter transition: QValue')
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('QValue')
        ax1.plot(self.arr_iplay, self.arr_maxQV, label="max_QV", color="blue")
        ax1.plot(self.arr_iplay, self.arr_q25QV, label="q25_QV", color="red")
        ax1.plot(self.arr_iplay, self.arr_q75QV, label="q75_QV", color="green")
        ax1.grid(True)
        ax1.legend(loc='best')
        # -----
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_maxturn).rolling(window=12, center=True).mean()
        ax3 = fig2.add_subplot(2, 2, 2)
        ax3.set_title('learning transition : maxturn')
        ax3.set_xlabel('#episode')
        ax3.set_ylabel('maxturn')
        ax3.grid(True)
        ax3.plot(self.arr_iplay, self.arr_maxturn, label="original", color="blue")
        ax3.plot(self.arr_iplay, y_rolling, label="moving", color="red")
        ax3.legend(loc='best')
        # -----
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_maxscore).rolling(window=12, center=True).mean()
        ax4 = fig2.add_subplot(2, 2, 3)
        ax4.set_title('learning transition : maxscore')
        ax4.set_xlabel('#episode')
        ax4.set_ylabel('maxscore')
        ax4.grid(True)
        ax4.plot(self.arr_iplay, self.arr_maxscore, label="original", color="blue")
        ax4.plot(self.arr_iplay, y_rolling, label="moving", color="red")
        ax4.legend(loc='best')
        # -----
        fig2.tight_layout()
        #fig.savefig("./AAC_img.png")
        plt.show()


#=================================================
# main function            
#=================================================
if __name__ == "__main__":

    # ---------------------------
    # ゲームの初期化
    dim_size, num_bombs = 12, 10

    # ---------------------------
    # パラメタ設定の初期化
    state_size     = 6*4 + 6*4 + 6*4 
    action_size    = 1
    dim_input      = 6*4 + 6*4 + 6*4 + 1
    dim_output     = 1

    # ---------------------------
    # RT用マトリックスの初期化
    max_cnv_parts  = 8
    len_cmx        = 6
    cmax_jy_idx    = 3*3 
    max_cnv_idx, max_cnv_col = 4, 4

    # ---------------------------
    # ハイパーパラメタ
    BATCH_SIZE = 64                         # サンプルサイズ
    LRATE   = 0.001                         # 学習率
    EPSILON = 0.95                          # greedy policy
    EPDECAY = 0.999
    EPMIN   = 0.01
    GAMMA   = 0.95                          # reward discount
    MEMORY_CAPACITY = 400                   # メモリ容量
    # ---------------------------
    SLEEP_TIME      = 0.01
    num_episodes    = 15000                 # 繰り返し回数

    # ---------------------------
    # 畳み込み部品フォルダ名の指定
    folder_cvinput = "./ARRAY_RTCNN/"  # My project folder

    # ---------------------------
    # 畳み込み部品の読み込み
    nam_cnv_input   = "畳み込み部品のCSVデータ" 
    code_cnv_input  = ["NA"] * max_cnv_parts # CSVコードの指定
    code_cnv_input[0] = "bend1_cnv" # CSVコードの指定
    code_cnv_input[1] = "bend2_cnv" # CSVコードの指定
    code_cnv_input[2] = "bend3_cnv" # CSVコードの指定
    code_cnv_input[3] = "bend4_cnv" # CSVコードの指定
    code_cnv_input[4] = "line1_cnv" # CSVコードの指定
    code_cnv_input[5] = "line2_cnv" # CSVコードの指定
    code_cnv_input[6] = "datum1_cnv" # CSVコードの指定
    code_cnv_input[7] = "datum2_cnv" # CSVコードの指定

    # ---------------------------
    # 環境設定
    gamepanel = Kylie_Ying_minesweeper_env2.Board(dim_size, num_bombs)

    # ---------------------------
    # フォルダ名の指定
    foldername = "./ML_csvout/"  # My project folder

    # ---------------------------
    # dql_solverのインスタンス化
    dql_solver  = DQN_Solver(state_size, action_size)

    # ---------------------------
    # 出力用:ゲームプレイリスト用のCSVファイルを定義する
    nam_csv_playlist    = "ゲームプレイリストのCSVデータ" 
    comment = "third"
    code_csv_playlist   = "train_playlist_DQNER_{}.csv".format(comment) # CSVコードの指定
    file_csv_playlist   = foldername + code_csv_playlist  # ファイルパス名の生成
    print("------------ 計測データ出力用のCSV名 -------------")   
    print("ゲームプレイリストのCSVファイル名 ：{0}".format(file_csv_playlist))

    # ---------------------------
    # 出力用:Pytorchモデルのファイル名
    comment_output_model = "initial"
    code_output_model = "model_msgame_DQNER_{0}.pt".format(comment_output_model)  # モデルの指定
    file_output_model = foldername + code_output_model  # ファイルパス名の生成

    # ---------------------------
    # DQN-ERで学習する
    Agent()

```

D先生 ： “いままでの議論の理由により、今回は良い結果が期待できないが・・・。結果をドン！！”

**（学習曲線）**

![imageRL2-26-3](/2022-07-01-QEUR21_RMLSP7/imageRL2-26-3.jpg)

**（パフォーマンス推移）**

![imageRL2-26-4](/2022-07-01-QEUR21_RMLSP7/imageRL2-26-4.jpg)

QEU:FOUNDER ： “ね・・・、いまいちでしょ？ちょっと無理な学習をさせているんじゃないかな？強調するけど、SOART法は外観検査自動機では使えますよ。Y2は使わないけど・・・。”

D先生 ： “SOARTメトリックスの2代目（SOART2）に期待ですね。”

![imageRL2-26-5](/2022-07-01-QEUR21_RMLSP7/imageRL2-26-5.jpg)

QEU:FOUNDER ： “一応は(新メトリックスの)アイデアはあります。やるかどうかはテーマに恵まれるか次第です。さあ・・・、マインスイーパーのプロジェクトの最後として、D先生の提案どおりに**「Eエリアを追加」**してやってみましょう。”

## ～　まとめ　～

C部長 : “ますます熱い・・・。イケメンバトルです。”

![imageRL2-26-6](/2022-07-01-QEUR21_RMLSP7/imageRL2-26-6.jpg)

D先生： “やっぱり、これでしょう・・・。”

[![MOVIE1](http://img.youtube.com/vi/JzZgG9JKrYw/0.jpg)](http://www.youtube.com/watch?v=JzZgG9JKrYw "【遭遇】水道橋博士が松井一郎にガン無視されたって！愛知県・金山駅北口（2022年7月1日）")

C部長 : “イケ麺（左）担当として、ちょっと矛盾を感じませんか？。”

D先生： “思いっきり感じた・・・（笑）。両方ともに私の担当だから・・・。”

![imageRL2-26-7](/2022-07-01-QEUR21_RMLSP7/imageRL2-26-7.jpg)

C部長 : “そういえば、最近はこういうこと（↑）も起きていますからねぇ・・・。”

QEU:FOUNDER ： “いやぁ・・・、熱い、暑いということで・・・。”
