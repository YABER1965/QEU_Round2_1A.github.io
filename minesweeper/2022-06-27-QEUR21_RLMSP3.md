---
title: QEUR21_RLMSP3:　予備実験～畳み込みRTメトリックスでマインスイーパー(その3)
date: 2022-06-27
tags: ["QEUシステム", "メトリックス", "Python言語", "強化学習", "マインスイーパー", "ディープラーニング"]
excerpt: Python言語によるマインスイーパーの強化学習
---

## QEUR21_RLMSP3:　予備実験～畳み込みRTメトリックスでマインスイーパー(その3)

## ～　DQN-ERを参考に・・・　～

D先生 ： “すでにゲームのデータを収集できました。それでは学習をしましょう。データの構造を見るに、Qmax法は無理でしょうね。”

**（Qmax vs SARSA）**

![imageRL2-22-1](/2022-06-27-QEUR21_RMLSP3/imageRL2-22-1.jpg)

**（Qmax vs SARSA）**

![imageRL2-22-2](/2022-06-27-QEUR21_RMLSP3/imageRL2-22-2.jpg)

QEU:FOUNDER ： “学習は古風ですがSARSAにします。あくまで「比較実験」なのでね・・・。Qmaxの場合には**「取りうる行動のすべてを予測する」**必要がありますから、比較実験は難しい。あとは状態（STATE）について・・・。ボード情報を「解析用ボード」に変換していますので・・・。”

![imageRL2-22-3](/2022-06-27-QEUR21_RMLSP3/imageRL2-22-3.jpg)

D先生 ： “あと、いうべきことは・・・。”

QEU:FOUNDER ： “ＳＡＲＳＡと紹介したが、実はＳＡＲＳＡでもありません。プログラムのロジックにはα（学習率）はありません。実は、DQN with Experience Replay(DQN-ER)を流用しています。DQN-ERというのは、ゲームデータを貯めこみ、それをサンプリングして「教師あり学習」の要領で計算します。それでは、プログラムをドン・・・。”

```python
# ----------------
# MINE_SWEEPERゲームのDL学習システム
# step2 : RTを動的メトリックスとして活用し、学習する
# step2 : ディープラーニングでプレイデータをSARSAで学習させる
# step2 : training_deeplearning_SARSA_msgame(agent).py
# 注意 : 予備実験用です
# Environment(Kylie Ying_minesweeper.py)は以下のWebからの引用です
# https://www.youtube.com/watch?v=8ext9G7xspg&t=9144s
# ---------------- 
#%matplotlib inline
# 数値計算のﾗｲﾌﾞﾗﾘ読み込み
import math
import numpy as np
import pandas as pd
import copy, random, time
from IPython.display import clear_output
from matplotlib import pyplot as plt
# ----------------
import torch
import torch.nn.functional
import torch.utils.data

#=================================================
# difinition of function
#=================================================
# ゲームプレイリスト保管用のCSVファイルを読み込む
def read_playlist(file_readcsv): 
 
    # 画像異常検出ファイルの読み込み
    df = pd.read_csv(file_readcsv) 
    #print(df)
    # -----
    # 強化学習可能なレコードを使用する
    target = df.index[df['code'] == "REMOVED"]
    #print("target: ",target)
    df_use = df.drop(target)
    max_play = len(df_use)
    #print("データ量 max_play",max_play)

    # --------------------------------------------------
    # 選択項目の読み込み
    #　基本情報-["iplay","maxturn","maxscore","victory","code"]
    arr_iplay   = df_use.loc[:,"iplay"].values
    arr_gsturn  = df_use.loc[:,"maxturn"].values
    arr_gscore  = df_use.loc[:,"maxscore"].values
    arr_code    = df_use.loc[:,"code"].values
    
    return max_play, arr_iplay, arr_gsturn, arr_gscore, arr_code

# --------------------------------------------------
# ゲームプレイリスト保管用のCSVファイルを読み込む
def read_GAMEresult(file_readcsv): 
 
    # 画像異常検出ファイルの読み込み
    df_use = pd.read_csv(file_readcsv) 
    #print(df)
    max_turn = len(df_use)
    #print("データ量 max_turn: ",max_turn)

    # --------------------------------------------------
    # 選択項目の読み込み
    #　基本情報-["iplay","maxturn","maxscore","victory","code"]
    arr_turn    = df_use.loc[:,"turn"].values
    arr_score   = df_use.loc[:,"score"].values
    arr_rows    = df_use.loc[:,"row"].values
    arr_cols    = df_use.loc[:,"col"].values
    arr_dones   = df_use.loc[:,"done"].values
    mx_state    = df_use.loc[:,"bc0":"sc23"].values
    mx_state_next = df_use.loc[:,"bn0":"sn23"].values

    # --------------------------------------------------
    # 一次元アドレスに変更する
    arr_address = encode_address(arr_rows, arr_cols, dim_size, max_turn)

    return arr_turn, arr_score, arr_address, arr_dones, mx_state, mx_state_next

# --------------------------------------------------
# 二次元 -> 一次元アドレスに変更する
def encode_address(rows, cols, dim_size, max_turn):

    arr_address = []
    for i in range(max_turn):
        num_address = int(dim_size*rows[i] + cols[i] + 0.001)
        arr_address.append(num_address)

    return arr_address

# --------------------------------------------------
# 一次元 -> 二次元アドレスに変更する
def decode_address(num_address, dim_size):

    row = int(num_address / dim_size + 0.001)
    col = int(num_address - row * dim_size + 0.001)

    return row, col

#=================================================
# Deep Learning Model class            
#=================================================
# PyTorchのDLの定義
class Net(torch.nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = torch.nn.Linear(dim_input, 128)
        self.fc2 = torch.nn.Linear(128, 128)
        self.fc3 = torch.nn.Linear(128, dim_output)
        
    def forward(self, x):
        x = torch.nn.functional.relu(self.fc1(x))
        x = torch.nn.functional.relu(self.fc2(x))
        x = self.fc3(x)
        return x
        
#=================================================
# difinition of class
#=================================================
# PyTorchのDLで機械学習を行う
class calc_DLQValue:

    def __init__(self):

        # --------------------------------------------------
        # crate instance for input
        self.model = Net()
        # --------------------------------------------------
        # Save and load the model via state_dict
        #self.model.load_state_dict(torch.load(file_input_model))
        # --------------------------------------------------
        # set validation mode
        self.model.eval()

        # ---------------------------
        # set parameters
        self.gamma          = VAL_GAMMA
        self.learning_rate  = LRATE

        # ---------------------------
        # 累積リストを初期化する
        self.acc_turn      = np.array([])        # ゲームターン(累積リスト)
        self.acc_score     = np.array([])        # ゲームスコア(累積リスト)
        self.acc_address   = np.array([])        # 命令、アドレス(累積リスト)
        self.acc_dones     = np.array([])        # ゲームオーバー(累積リスト)
        self.acc_state      = np.array([])        # 状態(累積リスト) 
        self.acc_state_next = np.array([])        # 次の状態(累積リスト) 
        
        # ---------------------------
        # ゲームプレイリスト保管用のCSVファイルを読み込む
        self.max_play, self.arr_iplay, self.arr_gsturn, self.arr_gscore, self.arr_code = read_playlist(file_csv_learn)
        print("max_play : ",self.max_play)
        #print("arr_code : ",self.arr_code)

        # ---------------------------
        # ゲーム結果データの累積プロセス
        self.acc_process()

        # ---------------------------
        # CSVデータの読み込みSARSAを計算する
        loss = self.calc_semiSARSA()

        # --------------------------------------------------
        # PyTorchモデルの保存
        torch.save(self.model.state_dict(), file_output_model)

    # ---------------------------
    # SARSAもどきの計算
    def calc_semiSARSA(self):

        # set training parameters
        optimizer = torch.optim.SGD(self.model.parameters(), lr=self.learning_rate)
        #optimizer = torch.optim.Adagrad(self.model.parameters(), lr=self.learning_rate)
        #optimizer = torch.optim.RMSprop(self.model.parameters(), lr=self.learning_rate)
        #optimizer = torch.optim.Adadelta(self.model.parameters(), lr=self.learning_rate)
        #optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)
        #optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate)
        # -----
        criterion = torch.nn.L1Loss()
        #criterion = torch.nn.MSELoss()
        #criterion = torch.nn.SmoothL1Loss(reduction="sum")

        # --------------------------------------------------
        batch_size = len(self.acc_score)
        X, Y = np.array([]), np.array([])
        for ibat in range(batch_size):
            state       = self.acc_state[ibat,:]
            action      = self.acc_address[ibat]
            reward      = self.acc_score[ibat]
            next_state  = self.acc_state_next[ibat,:]
            done        = self.acc_dones[ibat]
            state_action_curr = np.hstack([state, [action]])
            #print("state_action_curr",state_action_curr)
            # [state-action]を生成します。
            if done:
                target_f = reward
            else:
                mx_input = np.hstack([next_state, [action]])
                #print("--- mx_input, ibat: {} ---".format(ibat))
                #print(mx_input)
                # --------------------------------------------------
                # generate new 'x'
                x_input_tensor = torch.from_numpy(mx_input).float()
                # predict 'y'
                with torch.no_grad():
                    y_pred_tensor = self.model(x_input_tensor)
                # convert tensor to numpy
                y_pred = y_pred_tensor.data.numpy().flatten()
                target_f = reward + self.gamma * y_pred.item()
            # -----
            if ibat == 0:
                X = [state_action_curr]  # 状態(S)行動(A)マトリックス
            else:
                X = np.append(X, [state_action_curr], axis=0)  # 状態(S)行動(A)マトリックス
            Y = np.append(Y,target_f)

        # --------------------------------------------------
        # TRAINING
        # convert numpy array to tensor
        state_action_next = torch.from_numpy(X).float()
        expected_q_value = torch.from_numpy(Y.reshape(-1, 1)).float()
        #print("state_action_next:",state_action_next)
        #print("expected_q_value:",expected_q_value)
        
        # --- building model ---
        q_value = self.model(state_action_next)

        # calculate loss
        loss = criterion(q_value, expected_q_value)

        # update weights
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Show progress
        print('learn done -- [loss: {}]'.format(loss))

        return loss.data.numpy()

    # ----------------
    # ゲーム結果データの累積プロセス
    def acc_process(self):
    
        for iPlay in range(self.max_play):      # self.max_play
        
            # ---------------------------
            # ゲーム結果保管用のCSVファイルを読み込む
            file_csv_input = foldername + self.arr_code[iPlay]   # ファイルパス名の生成
            arr_turn, arr_score, arr_address, arr_dones, mx_state, mx_state_next = read_GAMEresult(file_csv_input)
            #print("iPlay: {},arr_turn: {}".format(iPlay, arr_turn))
            #print("mx_state : ",mx_state)
    
            if iPlay == 0:
                # ---------------------------
                # 累積リストを初期化する
                self.acc_turn      = arr_turn        # ゲームターン(累積リスト)
                self.acc_score     = arr_score        # ゲームスコア(累積リスト)
                self.acc_address   = arr_address      # 命令、アドレス(累積リスト)
                self.acc_dones     = arr_dones        # ゲームオーバー(累積リスト)
                self.acc_state      = mx_state        # 状態(累積リスト) 
                self.acc_state_next = mx_state_next        # 次の状態(累積リスト) 
            else:
                # ---------------------------
                # 累積リストを初期化する
                self.acc_turn      = np.hstack([self.acc_turn, arr_turn])        # ゲームターン(累積リスト)
                self.acc_score     = np.hstack([self.acc_score, arr_score])        # ゲームスコア(累積リスト)
                self.acc_address   = np.hstack([self.acc_address, arr_address])      # 命令、アドレス(累積リスト)
                self.acc_dones     = np.hstack([self.acc_dones, arr_dones])        # ゲームオーバー(累積リスト)
                self.acc_state      = np.concatenate([self.acc_state, mx_state], axis=0)        # 状態(累積リスト) 
                self.acc_state_next = np.concatenate([self.acc_state_next, mx_state_next], axis=0)        # 次の状態(累積リスト) 

        # ---------------------------
        # ゲーム結果保管用のCSVファイルを読み込む
        #print('self.acc_turn', self.acc_turn)        # ゲームターン(累積リスト)
        #print('self.acc_score', self.acc_score)        # ゲームスコア(累積リスト)
        #print('self.acc_address', self.acc_address)       # 命令、アドレス(累積リスト)
        #print('self.acc_state', self.acc_state)        # 状態(累積リスト) 
        #print('self.acc_state_next', self.acc_state_next)        # 次の状態(累積リスト) 
    
#=================================================
# main function            
#=================================================
if __name__  == '__main__':

    # --------------------------------------------------
    # SARSA学習パラメタの指定
    VAL_GAMMA       = 0.95
    LRATE           = 0.005         # 学習率
    SLEEP_TIME      = 0.05          # スリープ間隔

    # ---------------------------
    # パラメタ設定(2)の初期化
    dim_input       = 6*4 + 6*4 + 1
    dim_output      = 1
    dim_size        = 12 

    # ---------------------------
    # フォルダ名の指定
    foldername = "./ML_csvout/"  # My project folder

    # --------------------------------------------------
    # ゲームプレイリスト入力用のCSVファイルを定義する
    comment = "second"
    nam_csv_learn = "ゲームプレイリスト入力用ののCSVデータ" 
    code_csv_learn = "train_playlist_{0}.csv".format(comment)  # CSVコードの指定
    file_csv_learn = foldername + code_csv_learn  # ファイルパス名の生成
    print("------------ 計測データ出力用のCSV名 -------------")   
    print("ゲームプレイリスト入力用のCSVファイル名 ：{0}".format(file_csv_learn))

    # --------------------------------------------------
    # [P]:出力用Pytorchモデルのファイル名
    #comment = "initial"
    code_output_model = "model_msgame_SARSA_{0}.pt".format(comment)  # モデルの指定
    file_output_model = foldername + code_output_model  # ファイルパス名の生成 

    # --------------------------------------------------
    # 機械学習を行う
    calc_DLQValue()

```

QEU:FOUNDER ： “アウトプットはＰｙＴｏｒｃｈ学習モデルが出てくるだけです。特にいうことはないですね。いきなりですが・・・、カンパください・・・。”

### [＞寄付のお願い(Donate me)＜](https://www.paypal.com/paypalme/QEUglobal?v=1&utm_source=unp&utm_medium=email&utm_campaign=RT000481&utm_unptid=29844400-7613-11ec-ac72-3cfdfef0498d&ppid=RT000481&cnac=HK&rsta=en_GB%28en-HK%29&cust=5QPFDMW9B2T7Q&unptid=29844400-7613-11ec-ac72-3cfdfef0498d&calc=f860991d89600&unp_tpcid=ppme-social-business-profile-creat-ed&page=main%3Aemail%3ART000481&pgrp=main%3Aemail&e=cl&mchn=em&s=ci&mail=sys&appVersion=1.71.0&xt=104038)

D先生 ： “お願いします。”

## ～　まとめ　～

QEU:FOUNDER ： “政治の安定か・・・。”

[![MOVIE1](http://img.youtube.com/vi/e8aiitAAAZY/0.jpg)](http://www.youtube.com/watch?v=e8aiitAAAZY "このままでは日本は沈没する。死ぬまで安心して年金が欲しい、安定がしたい、その想いが重りとなって日本を沈めていく。安冨歩東大教授。一月万冊")

C部長 : “このデータを見て「アレ？」と思いました・・・。昔、メディアに多く見られた**「若者の右傾化」って何だったんだ？**同時に「高齢者の左翼ガー」のコメントもおかしかった・・・。”

![imageRL2-22-4](/2022-06-27-QEUR21_RMLSP3/imageRL2-22-4.jpg)

QEU:FOUNDER ： “若者の右傾化って、まず「フェイク・ニュース」と断言できますね。**彼らは政治に関心がなかっただけです**。本来はノーコメントのところを、メディアが「風味」を付けたんじゃない？それはそうと、我々としては政治の安定は重要です。”

![imageRL2-22-5](/2022-06-27-QEUR21_RMLSP3/imageRL2-22-5.jpg)

C部長 : “安定は大切です・・・。”

![imageRL2-22-6](/2022-06-27-QEUR21_RMLSP3/imageRL2-22-6.jpg)

QEU:FOUNDER ： “安定こそ命だから・・・。”

![imageRL2-22-7](/2022-06-27-QEUR21_RMLSP3/imageRL2-22-7.jpg)

C部長 : “**年金こそ命・・・。**いままで、こんなに（↓）**がんばってきた**んだから・・・。”

### おっさん（＠QCサークル講話）；「従業員の皆さんにはテレビを見てください。皆が同じように考えてください。」

### オッサン（＠車中、N社検査不正について）：　「“検査不正”っていうのはなァ、（組織外に不正を）漏らしたヤツが悪いんだよ・・・」

### オッサン（海外工場のあいさつにて）：「私の使命はこの会社で終身雇用制を実現することにある・・・。」

QEU:FOUNDER ： “よく頑張ったから・・・。その当時のメディアが垂れ流した価値観を1000倍に拡大して社内に展開し、**その価値観の普及に貢献**しました。”

![imageRL2-22-8](/2022-06-27-QEUR21_RMLSP3/imageRL2-22-8.jpg)

QEU:FOUNDER ： “見事、大輪の花が咲きましたね・・・。表彰状モノです。”
