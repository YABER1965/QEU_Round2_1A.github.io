---
title: QEUR21_SOART9:　本実験～ベースラインを再設定する (その1) 
date: 2022-06-22
tags: ["QEUシステム", "メトリックス", "Python言語", "機械学習", "マハラノビス距離", "DX", "Blender", "SOART法", "異常判別"]
excerpt: Python言語とSOART法を使った異常判別
---

## QEUR21_SOART9:　本実験～ベースラインを再設定する (その1) 

## ～　「一番贅沢」な仕様は？　～

D先生 ： “思わず外観検査自動機の基本ロジックのやり直しになりました。ちょっと、FOUNDERへの「慰め」です。現在世に出ている**AOI（Automatic Optical Inspection）機**って、箱の中に入れて光量も厳密に管理しています。あと、検査対象はプリント基板みたいに・・・。”

![imageRL2-11-1](/2022-06-22-QEUR21_SOART9/imageRL2-11-1.jpg)

QEU:FOUNDER ： “（AOIの検査対象は）厳密な転写が行われている製品ぐらいですね。慰めてくれてありがとう・・・。その意味では、今回の90％程度の正確度は悪くはないです。・・・さて、本題に移りましょう。基本に返って「ベースライン」を設定したいんだ、評価の良しあしの規準となる・・・。Feature-Engineeringのポリシーを変えます。マハラノビス距離を使わず、畳み込みRTメトリックスをじかにSVMにかけます。”

D先生 ： “でも・・・、畳み込みRTメトリックスは1回の処理で6つのデータがでてきます。そんなことをすると入力データの次元が大きくなりすぎませんか？”

![imageRL2-11-2](/2022-06-22-QEUR21_SOART9/imageRL2-11-2.jpg)

QEU:FOUNDER ： “だから、**不良発生部分のみで計測します。これが「究極」・・・（笑）**。同じデータを使って、これ以上の予測はできないはずだよ。それでは、学習データを準備するプログラムをドン！！”

```python
# -------------------- プログラムの始まり ------------------
# -*- coding: utf-8 -*-
# filename: soaRT_collect_L1direct.py
# 複数画像を読み込み、SVM学習の準備のために特徴量をCSVファイルに出力する 
# SOART法を採用しました。ファイブアイズ、RTメトリックスはダイレクト入力
# SOARTメトリックスのうち、Y2（体積比）はマンハッタン(L1)距離を使っている
# ---------------------------------------------------
# モジュールのインポート
import cv2
from fastai.vision.all import *
import seaborn as sns
import pandas as pd
import numpy as np
from scipy.spatial import distance
import matplotlib.pyplot as plt

#=================================================
# SET PARAMETER AND READ PIC NAMES
#=================================================
# 読み込み先（フォルダ名）
foldername = "./ARRAY_RTCNN/"

# iRow: 項目数　、　jCol: 全ピン数
max_iRow_cells = 12
max_jCol_cells = 3*9

# 単位空間の分配ベクトルの定義
bun_temp = np.array([[1,1,2,2,2,2,2,3,3],[1,1,2,2,2,2,2,3,3],[1,1,2,2,2,2,2,3,3],])
arr_bun  = bun_temp.flatten()
#print(arr_bun)

# ファイルを読み込み表示する(画像とラベル)
def read_picnames(file_readcsv): 
 
    # 畳み込みファイルの読み込み
    df = pd.read_csv(file_readcsv) 
    dfL = df.query("camNO == 0")
    dfC = df.query("camNO == 1")
    dfR = df.query("camNO == 2")
    dfU = df.query("camNO == 3")
    dfD = df.query("camNO == 4")
    # ------------------
    # 数字が同じか？
    len_picsL = len(dfL)
    len_picsC = len(dfC)
    len_picsR = len(dfR)
    len_picsU = len(dfU)
    len_picsD = len(dfD)
    #print(len_picsL, len_picsC, len_picsR, len_picsU, len_picsD)
    # ------------------
    # file_nameのリストを生成する
    mx_Xs = df.loc[:,"file_name":"label"].values
    arr_VRfL = dfL.loc[:,"file_name"].values
    arr_VRfC = dfC.loc[:,"file_name"].values
    arr_VRfR = dfR.loc[:,"file_name"].values
    arr_VRfU = dfU.loc[:,"file_name"].values
    arr_VRfD = dfD.loc[:,"file_name"].values
    # ------------------
    # file_nameのリストを生成する
    arr_labL = dfL.loc[:,"label"].values
    arr_labC = dfC.loc[:,"label"].values
    arr_labR = dfR.loc[:,"label"].values
    arr_labU = dfU.loc[:,"label"].values
    arr_labD = dfD.loc[:,"label"].values
    #print("----- mx_Xs -----")
    #print(mx_Xs)
    # ------------------
    # 2次元化でとりまとめる
    mx_VRf = np.vstack([arr_VRfL, arr_VRfC, arr_VRfR, arr_VRfU, arr_VRfD])
    mx_lab = np.vstack([arr_labL, arr_labC, arr_labR, arr_labU, arr_labD])
        
    return len_picsL, mx_VRf, mx_lab
    
# ----------
pics_csv = "labels_all.csv"
file_readcsv = foldername + pics_csv
len_pics, mx_VRf, mx_lab = read_picnames(file_readcsv)
#print(len_pics)
#print(mx_VRf[0])
#print(mx_lab[0])

#=================================================
# READ CONVOLUTION CSV_FILE FUNCTION
#=================================================
# 畳み込みファイルを読み込み表示する
def read_csvfile(file_readcsv): 
 
    # 畳み込みファイルの読み込み
    df = pd.read_csv(file_readcsv) 
    # ------------------
    # 選択項目の読み込み
    # Xsマトリックス
    mx_Xs = df.loc[:,"col1":"col5"].values
    #print("----- mx_Xs -----")
    #print(mx_Xs)
  
    return mx_Xs

#=================================================
# SET BIND PROCESS CSV_FILE FUNCTION
#=================================================
# LCR画像ファイルを読んで、テンソルを出力する
def read_LCRpics(ipic, mx_VRf): 

    #=================================================
    # BIND PROGRAM(1) : ファイル名リストを分解する
    #=================================================
    pic_nameL = mx_VRf[0][ipic]
    pic_nameC = mx_VRf[1][ipic]
    pic_nameR = mx_VRf[2][ipic]
    pic_nameU = mx_VRf[3][ipic]
    pic_nameD = mx_VRf[4][ipic]
    #print("mx_VRf:", mx_VRf)
    #print("pic_nameL:", pic_nameL)

    #=================================================
    # BIND PROGRAM(2) : 画像を読み込む
    #=================================================
    # 左(L)画像
    filename = foldername + pic_nameL + '.png'
    imgL = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)
    img_cropL = imgL[212:856,47:1867]
    # リサイズ
    img_resizeL = cv2.resize(img_cropL , newsize)

    # ------------------
    # 中央(C)画像
    #filename = foldername + pic_nameC + '.png'
    filename = foldername + "average_pic.png"
    imgC = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)
    img_cropC = imgC[212:856,47:1867]
    # リサイズ
    img_resizeC = cv2.resize(img_cropC , newsize)

    # ------------------
    # 右(R)画像
    filename = foldername + pic_nameR + '.png'
    imgR = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)
    img_cropR = imgR[212:856,47:1867]
    # リサイズ
    img_resizeR = cv2.resize(img_cropR , newsize)
    # 画像出力
    #cv2.imshow("image(Right)", img_resizeR)
    #cv2.waitKey(0)
    #cv2.destroyAllWindows()

    # ------------------
    # 上(U)画像
    filename = foldername + pic_nameU + '.png'
    imgU = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)
    img_cropU = imgU[212:856,47:1867]
    # リサイズ
    img_resizeU = cv2.resize(img_cropU , newsize)

    # ------------------
    # 下(D)画像
    filename = foldername + pic_nameD + '.png'
    imgD = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)
    img_cropD = imgD[212:856,47:1867]
    # リサイズ
    img_resizeD = cv2.resize(img_cropD , newsize)

    #=================================================
    # BIND PROGRAM(3) : 変換
    #=================================================
    # 画像データのテンソル化(L,C,R,U,D)
    img_tensorL = tensor(img_resizeL/255)  # 信号空間(左)
    img_tensorC = tensor(img_resizeC/255)  # 単位空間（標準ベクトル）
    img_tensorR = tensor(img_resizeR/255)  # 信号空間(右)
    img_tensorU = tensor(img_resizeU/255)  # 信号空間(上)
    img_tensorD = tensor(img_resizeD/255)  # 単位空間（下）
    
    return img_tensorL, img_tensorC, img_tensorR, img_tensorU, img_tensorD

# ---------------------------
# ファイブアイズ法RTメトリックスを計算する(テンソル活用版)
def calc_deyeRTmet(len_temp, max_jy_index, tsr_sig_matrix, tsr_tani_array): 

    # ---------------------------
    # 変数の初期化
    L1_loss = torch.nn.L1Loss()
    btY1_yarray, Y2_yarray = [], []

    # 繰り返し
    for i in range(len_temp):

        y = tsr_sig_matrix[i]
        x = tsr_tani_array

        xx = torch.dot(x,x)
        xy = torch.dot(x,y)
        beta = xy/xx
        #print("i:{}, beta:{}".format(i,beta))

        mDistance   = L1_loss(y, beta*x)
        #print("i:{}, beta:{}".format(i,beta))
        #print("mDistance: ", mDistance.item())
        #print("yres:{}".format(yres))
        
        btY1_yarray.append(beta.item())
        Y2_yarray.append(mDistance.item())
            
    return round(Y2_yarray[0] - Y2_yarray[1], 5)

# ---------------------------
# 畳み込み処理を実施する
def apply_kernel(row, col, kernel, img_tensor):
    return (img_tensor[row:row+max_cnv_row,col:col+max_cnv_col] * kernel).sum()

# ---------------------------
# soaRTメトリックスを計算する(テンソル活用版)
# Y2(体積比)はマンハッタン距離を使う
def calc_soaRT(len_temp, max_jy_index, tsr_sig_matrix, tsr_tani_array): 

    # ---------------------------
    # 変数の初期化
    L1_loss = torch.nn.L1Loss()
    tsr_zero = torch.zeros(max_jy_index)
    btY1_yarray, Y2_yarray, Y3_yarray = [], [], []

    # 繰り返し
    for i in range(len_temp):

        y = tsr_sig_matrix[i]
        x = tsr_tani_array

        # 回転を計測
        xx = torch.dot(x,x)
        xy = torch.dot(x,y)
        beta = xy/xx
        #print("i:{}, beta:{}".format(i,beta))

        # 体積ひずみを計測
        vol_xtani = L1_loss(beta*x, tsr_zero)
        vol_ysig  = L1_loss(y, tsr_zero)
        ratio_vol = vol_ysig/vol_xtani

        # せん断ひずみを計測
        mDistance   = L1_loss(y, beta*ratio_vol*x)
        #print("mDistance: ", mDistance.item())
        #print("yres:{}".format(yres))
        
        btY1_yarray.append(beta.item())
        Y2_yarray.append(ratio_vol.item())
        Y3_yarray.append(mDistance.item())

    return btY1_yarray, Y2_yarray, Y3_yarray

#=================================================
# SUB PROGRAM(1) : ファイブアイズRTメトリックスの準備
#=================================================
# 1.ファイブアイズRTメトリックス
# 画像リサイズのパラメタ
newsize = (900, 300)
# テンソルイメージ処理のパラメタ
dmax_tsr_col, dmax_tsr_row   = newsize[0], newsize[1]  #(900, 300)
# ストライド量
stride_tsr_row, stride_tsr_col = 5, 5
# RT法処理のサイズ
drtm_tsr_row, drtm_tsr_col = 5, 5
# RTベクトル長と信号空間長
dmax_jy_idx, len_dmx = 5*5, 2
# ---------------------------
# 1.ファイブアイズRTメトリックス: テンソル行列の処理(for)開始点のベクトルを生成
row_range1, col_range1 = [], []
sum_row   = 0
while sum_row <= dmax_tsr_row-drtm_tsr_row:
    row_range1.append(sum_row)
    sum_row  += stride_tsr_row
# -----
sum_col = 0
while sum_col <= dmax_tsr_col-drtm_tsr_col:
    col_range1.append(sum_col)
    sum_col  += stride_tsr_col

#=================================================
# SUB PROGRAM(2) : 畳み込みメトリックスの準備
#=================================================
# 2.畳み込み：画像の処理(for)開始点のベクトルを生成する
# パラメタの設定
max_cnv_parts  = 8
# 画像サンプルのサイズ
max_sp_row, max_sp_col = 60, 180
# 畳み込み部品のサイズ
max_cnv_row, max_cnv_col = 5, 5
# ストライド量
stride_cnv_row, stride_cnv_col = 5, 5
# ---------------------------
# 2.畳み込み：画像の処理(for)開始点のベクトルを生成する
row_range2, col_range2 = [], []
sum_row   = 0
while sum_row <= max_sp_row-max_cnv_row:
    row_range2.append(sum_row)
    sum_row  += stride_cnv_row
# -----
sum_col = 0
while sum_col <= max_sp_col-max_cnv_col:
    col_range2.append(sum_col)
    sum_col  += stride_cnv_col
# ------------------
nam_cnv_input = "基準用CSVデータ" 
code_cnv_input = ["NA"] * 8 # CSVコードの指定
code_cnv_input[0] = "bend1_cnv" # CSVコードの指定
code_cnv_input[1] = "bend2_cnv" # CSVコードの指定
code_cnv_input[2] = "bend3_cnv" # CSVコードの指定
code_cnv_input[3] = "bend4_cnv" # CSVコードの指定
code_cnv_input[4] = "line1_cnv" # CSVコードの指定
code_cnv_input[5] = "line2_cnv" # CSVコードの指定
code_cnv_input[6] = "datum1_cnv" # CSVコードの指定
code_cnv_input[7] = "datum2_cnv" # CSVコードの指定
# ------------------
# 畳み込み処理を実施する
for i in range(max_cnv_parts):
    # ---------------------------
    # CSVファイル(実験)情報を読み込み表示する
    # 畳み込みファイル
    file_cnv_input = foldername + code_cnv_input[i] + ".csv"  # ファイルパス名の生成 
    mx_conv = read_csvfile(file_cnv_input)
    if i == 0:    
        tensor_bend1 = tensor(mx_conv).float()
    elif i == 1:
        tensor_bend2 = tensor(mx_conv).float()
    elif i == 2:
        tensor_bend3 = tensor(mx_conv).float()
    elif i == 3:
        tensor_bend4 = tensor(mx_conv).float()
    elif i == 4:
        tensor_line1 = tensor(mx_conv).float()
    elif i == 5:
        tensor_line2 = tensor(mx_conv).float()
    elif i == 6:
        tensor_datum1 = tensor(mx_conv).float()
    elif i == 7:
        tensor_datum2 = tensor(mx_conv).float()
# ---------------------------
# 畳み込みカーネルを生成する
signal_kernels  = torch.stack([tensor_bend1, tensor_bend2, tensor_bend3, tensor_bend4, tensor_line1, tensor_line2])
tani_kernel     = tensor_datum1 + tensor_datum2

#=================================================
# SUB PROGRAM(3) : 畳み込みRTメトリックスの準備
#=================================================
# 3. 畳み込みRTメトリックス
# テンソルイメージ処理のパラメタ
cmax_tsr_row, cmax_tsr_col = 12, 36
# ストライド量
stride_tsr_row, stride_tsr_col = 4, 4
# RT法処理のサイズ
crtm_tsr_row, crtm_tsr_col = 4, 4
# RTベクトル長と信号空間長
cmax_jy_idx, len_cmx = 4*4, 6
# ---------------------------
# テンソル行列の処理(for)開始点のベクトルを生成する
row_range3, col_range3 = [], []
sum_row   = 0
while sum_row <= cmax_tsr_row-crtm_tsr_row:
    row_range3.append(sum_row)
    sum_row  += stride_tsr_row
#print(row_range3)
# -----
sum_col = 0
while sum_col <= cmax_tsr_col-crtm_tsr_col:
    col_range3.append(sum_col)
    sum_col  += stride_tsr_col
#print(col_range3)

#=================================================
# FUNCTION(1) : ファイブアイズでモジュール化した部分
#=================================================
# ファイブアイズRTテンソルを生成する
def create_feyeRT(img_tensorL, img_tensorC, img_tensorR):

    # ファイブアイズRT法によるマトリックスの生成
    num_rows, num_cols = len(row_range1), len(col_range1)
    feyeY2 = []
    # ------
    cnt_row = 0
    for row in row_range1:
        cnt_col = 0
        for col in col_range1:
            # ------
            # 単位空間の空間ベクトルを生成する
            tsr_tani_array = img_tensorC[row:row+drtm_tsr_row,col:col+drtm_tsr_col].flatten()
            #print(tsr_tani_array)
            # ------
            # 信号空間の空間マトリックスを生成する
            tsr_sigL = img_tensorL[row:row+drtm_tsr_row,col:col+drtm_tsr_col].flatten()
            tsr_sigR = img_tensorR[row:row+drtm_tsr_row,col:col+drtm_tsr_col].flatten()
            # -----
            tsr_sig_matrix = torch.stack([tsr_sigL, tsr_sigR])
            #print("----- tsr_sig_matrix -----")
            #print(tsr_sig_matrix)
            # ------
            # ファイブアイズ法RTメトリックスを計算する
            temp_feyeY2 = calc_deyeRTmet(len_dmx, dmax_jy_idx, tsr_sig_matrix, tsr_tani_array)
            feyeY2.append(temp_feyeY2)
            # ------
            # COUNT UP
            cnt_col = cnt_col + 1
        # ------
        # COUNT UP.append
        cnt_row = cnt_row + 1

    # ---------------------------
    # 結果をテキスト出力する
    mx_fY2       = np.array(feyeY2).reshape([num_rows, num_cols])
    tensor_fY2   = tensor(mx_fY2)
    #print(tensor_fY2)

    return tensor_fY2

# ------
# 畳み込みテンソルイメージを生成する
def create_feyeConv(tensor_Y1Y2):

    # 単位空間用の畳み込み処理を実施する
    tani_image = tensor([[apply_kernel(i,j, tani_kernel, tensor_Y1Y2) for j in col_range2] for i in row_range2])
    #print("----- tani_image -----")
    #print(tani_image)
    # ------------------
    # 信号空間用の畳み込み処理を実施する
    for i in range(max_cnv_parts-2):
        # ---------------------------
        # CSVファイル(実験)情報を読み込み表示する
        kernel = signal_kernels[i]
        calc_conv = tensor([[apply_kernel(i,j, kernel, tensor_Y1Y2) for j in col_range2] for i in row_range2])
        # -----
        if i == 0:    
            tsr_cvbend1 = tensor(calc_conv).float()
        elif i == 1:
            tsr_cvbend2 = tensor(calc_conv).float()
        elif i == 2:
            tsr_cvbend3 = tensor(calc_conv).float()
        elif i == 3:
            tsr_cvbend4 = tensor(calc_conv).float()
        elif i == 4:
            tsr_cvline1 = tensor(calc_conv).float()
        elif i == 5:
            tsr_cvline2 = tensor(calc_conv).float()
    # ---------------------------
    # 畳み込みテンソルイメージの生成  
    signal_images = torch.stack([tsr_cvbend1, tsr_cvbend2, tsr_cvbend3, tsr_cvbend4, tsr_cvline1, tsr_cvline2])
    # 結果の出力
    #print(signal_images.shape)     # torch.Size([6, 12, 36])
    #print("----- signal_images[0] -----")
    #print(signal_images[0])

    return tani_image, signal_images

# ------
# 畳み込みRTメトリックスを生成する
def create_CRTmetric(tani_image, signal_images):

    # RTメトリックスをCSVファイルに出力するための行列
    mx_CRTmetric = np.zeros([27,20])     # 6x3 + 2(row,col)
    # ---------------------------
    cnt_pin = 0
    cnt_row = 0
    for row in row_range3:
        cnt_col = 0
        for col in col_range3:
            # ------
            # 単位空間の空間ベクトルを生成する
            tsr_tani_array = tani_image[row:row+crtm_tsr_row,col:col+crtm_tsr_col].flatten()
            #print(tsr_tani_array)
            # ------
            # 信号空間の空間マトリックスを生成する
            temp_matrix = signal_images[0]
            tsr_sig_p0 = temp_matrix[row:row+crtm_tsr_row,col:col+crtm_tsr_col].flatten()
            temp_matrix = signal_images[1]
            tsr_sig_p1 = temp_matrix[row:row+crtm_tsr_row,col:col+crtm_tsr_col].flatten()
            temp_matrix = signal_images[2]
            tsr_sig_p2 = temp_matrix[row:row+crtm_tsr_row,col:col+crtm_tsr_col].flatten()
            temp_matrix = signal_images[3]
            tsr_sig_p3 = temp_matrix[row:row+crtm_tsr_row,col:col+crtm_tsr_col].flatten()
            temp_matrix = signal_images[4]
            tsr_sig_p4 = temp_matrix[row:row+crtm_tsr_row,col:col+crtm_tsr_col].flatten()
            temp_matrix = signal_images[5]
            tsr_sig_p5 = temp_matrix[row:row+crtm_tsr_row,col:col+crtm_tsr_col].flatten()
            # -----
            tsr_sig_matrix = torch.stack([tsr_sig_p0, tsr_sig_p1, tsr_sig_p2, tsr_sig_p3, tsr_sig_p4, tsr_sig_p5])
            #print("----- tsr_sig_matrix -----")
            #print(tsr_sig_matrix)
            # ------
            # RTメトリックスを計算する(配列長6)
            btY1_yarray, Y2_yarray, Y3_yarray = calc_soaRT(len_cmx, cmax_jy_idx, tsr_sig_matrix, tsr_tani_array)
            # ------
            # マトリックスへの入力
            temp_array = [cnt_row, cnt_col] + btY1_yarray + Y2_yarray + Y3_yarray
            mx_CRTmetric[cnt_pin] = temp_array     # 6x3 + 2(row,col)
            # ------
            # COUNT UP
            #print(cnt_row, cnt_col)
            cnt_col = cnt_col + 1
            cnt_pin = cnt_pin + 1
        # ------
        # COUNT UP
        cnt_row = cnt_row + 1
    # ---------------------------
    # 結果のテキスト出力する
    #print(mx_CRTmetric)

    return mx_CRTmetric

#=================================================
# FUNCTION(2) : ファイブアイズ畳み込みRTメトリックスを連続して実行する
#=================================================
def bind_process(img_tensorL, img_tensorC, img_tensorR, img_tensorU, img_tensorD):

    #=================================================
    # BIND PROGRAM(1) : ファイブアイズRTメトリックスを実行する
    #=================================================
    # ファイブアイズRTテンソルを生成する
    tensor_LCRY2   = create_feyeRT(img_tensorL, img_tensorC, img_tensorR)
    tensor_UCDY2   = create_feyeRT(img_tensorU, img_tensorC, img_tensorD)

    #=================================================
    # BIND PROGRAM(2) : 畳み込みメトリックスを実行する
    #=================================================
    # 畳み込みテンソルイメージを生成する
    tani_LCRimage, signal_LCRimages = create_feyeConv(tensor_LCRY2)
    tani_UCDimage, signal_UCDimages = create_feyeConv(tensor_UCDY2)

    #=================================================
    # BIND PROGRAM(3) : 畳み込みRTメトリックスを実行する
    #=================================================
    # 畳み込みRTメトリックスを生成する
    mx_CRTmetric = create_CRTmetric(tani_LCRimage, signal_LCRimages)
    #for i in range(27):
    #    print(mx_CRTmetric[i])
        
    # ---------------------------
    # 学習に使用するデータを抽出する(アドレス3-1とアドレス4-1のみ)
    arr_CRTA = mx_CRTmetric[12]
    arr_CRTB = mx_CRTmetric[13]

    return arr_CRTA, arr_CRTB

#=================================================
# MAIN PROGRAM(1) : ファイブアイズRTメトリックス
#=================================================
# マトリックスの初期化
mx_metrics = []
# 条件（5画像）ごとの繰り返し
for ipic in range(len_pics):   # len_pics

    # ---------------------------
    # LCR画像ファイルを読んで、テンソルを出力する
    img_tensorL, img_tensorC, img_tensorR, img_tensorU, img_tensorD = read_LCRpics(ipic ,mx_VRf)
    #print(img_tensorC)
    
    # ---------------------------
    # マハラノビス距離(Y1,Y2,Y3)のベクトルを生成する
    arr_CRTA, arr_CRTB = bind_process(img_tensorL, img_tensorC, img_tensorR, img_tensorU, img_tensorD)
    #print("---- NO:{} ----".format(ipic))
    #print(arr_CRTA)
    #print(arr_CRTB)
   
    # ---------------------------
    # メトリックスの積み上げ
    arr_temp = np.hstack([arr_CRTA[2:], arr_CRTB[2:], [mx_lab[0][ipic]]])
    if ipic == 0:
        mx_metrics = arr_temp
    else:
        mx_metrics = np.vstack([mx_metrics, arr_temp])

# ---------------------------
# メトリックスの出力
#print(mx_metrics)

#=================================================
# MAIN PROGRAM(2) : CSVファイルに学習結果を出力する
#=================================================
# コラムの生成
arr_columns_b = ["b{}".format(i) for i in range(18*2)]
arr_columns   = arr_columns_b + ["label"]
#print(arr_columns)
df_metrics = pd.DataFrame(mx_metrics, columns=arr_columns)
#print(df_metrics)
df_metrics['filename'] = mx_VRf[0]
print(df_metrics)
# CSV ファイル (learn_result.csv) として出力
file_csvout = foldername + "learn_result2.csv" # ファイルパス名の生成
df_metrics.to_csv(file_csvout)
print("CSVファイルの出力完了 : {}".format(file_csvout)) 

```

QEU:FOUNDER ： “このプログラムを起動すると学習データができます。かなり時間がかかるけど・・・。この次は、例によってSVM（サポートベクトルマシン）による学習です。この結果（↓）をみてみましょう。”

**(修正版データ：ライトをいじって、物体の明るさが変わりまくりだが・・・)**

![imageRL2-11-3](/2022-06-22-QEUR21_SOART9/imageRL2-11-3.jpg)

D先生 ： “修正版でこの程度の正確度までしか行かないんですか？ああ・・・低いなあ・・・、明るさをバンバン変えていることは認めるが・・・。じゃあ、Y2メトリックスを消してみますか？”

![imageRL2-11-4](/2022-06-22-QEUR21_SOART9/imageRL2-11-4.jpg)

QEU:FOUNDER ： “（Y2を消すのは）やめておこう・・・。マハラノビス距離をSVMで学習する場合には、意味なく異常値を発生させやすいY2値を消すことには意味があったと思うけど・・・。ダイレクト入力の場合にはY2は有効な情報を持っていますよ。”

D先生 ： “じゃあ、どうします？”

QEU:FOUNDER ： “**SOARTメトリックスの定義に問題があった**のだと思います。”

D先生 ： “え～っつ！！次回につづきます。”

## ～　まとめ　～

C部長 : “・・・というわけで、QEUではイケメン・バトルを推奨しております。”

![imageRL2-11-5](/2022-06-22-QEUR21_SOART9/imageRL2-11-5.jpg)

C部長 : “じゃあ、ボクはコレを行きます。”

[![MOVIE1](http://img.youtube.com/vi/XrO_5-zhDk0/0.jpg)](http://www.youtube.com/watch?v=XrO_5-zhDk0 "消費税はほんまに社会保障財源？")

C部長 : “この話（↓）がらみで・・・。”

![imageRL2-11-6](/2022-06-22-QEUR21_SOART9/imageRL2-11-6.jpg)

D先生 : “**大変な国家運営の失敗をした**んだろうね。この超大物のTWITTERにあるように・・・。”

![imageRL2-11-7](/2022-06-22-QEUR21_SOART9/imageRL2-11-7.jpg)

QEU:FOUNDER ： “他の国から（ダメダメ）言われちゃ、相当ひどいんだろうねぇ・・・（笑）。これだけ**PRODUCT_INNOVATIONが起こらない**と、まあしようがないよね。”

![imageRL2-11-8](/2022-06-22-QEUR21_SOART9/imageRL2-11-8.jpg)

QEU:FOUNDER ： “C部長が紹介してくれた美人政治家候補の経済に対する見解には、小生は同意していません。**「国債をすれば、なんとでもなる」って簡単なものじゃない**から・・・、近年の通貨安をみればわかる通り・・・。”

![imageRL2-11-9](/2022-06-22-QEUR21_SOART9/imageRL2-11-9.jpg)

C部長 : “A国では（MMTは）問題ないんでしょうね。”

QEU:FOUNDER ： “A国ぐらい、あらゆる意味で抜きんでいるとね・・・。あと、**イノベーションが起きている**し・・・。”

D先生 : “どうすればいいんでしょうか？”

QEU:FOUNDER ： “とにかく**「お腹いっぱい食べる」**ことから始めるしかないですね。”

**（イギリスの物価例）**

![imageRL2-11-10](/2022-06-22-QEUR21_SOART9/imageRL2-11-10.jpg)

D先生 : “お腹がすいたら知恵も出ないですからね・・・。”
