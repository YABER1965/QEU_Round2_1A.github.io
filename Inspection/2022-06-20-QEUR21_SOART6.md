---
title: QEUR21_SOART6:　本実験～SOARTメトリックスを使ってみる (その4) 
date: 2022-06-20
tags: ["QEUシステム", "メトリックス", "Python言語", "機械学習", "マハラノビス距離", "DX", "Blender", "SOART法", "異常判別"]
excerpt: Python言語とSOART法を使った異常判別
---

## QEUR21_SOART6:　本実験～SOARTメトリックスを使ってみる (その4) 

## ～　特徴量マップを出力する　～

D先生 ： “それでは、いよいよ特徴量マップを出力しましょう。ここが事実上のクライマックスですね。私からは特にコメントはありません。”

**（処理フロー）**

![imageRL2-8-1](/2022-06-20-QEUR21_SOART6/imageRL2-8-1.jpg)

QEU:FOUNDER ： “プログラムの操作方法の説明です。出力画像の選択は、SELECT変数に番号を代入してください。ただし、これだけじゃ動かないよ・・・。”

![imageRL2-8-2](/2022-06-20-QEUR21_SOART6/imageRL2-8-2.jpg)

QEU:FOUNDER ： “ベースの画像ファイル名を保管しているCSVファイルのうち、出力したいものに「calc」という文字を入力しておいてください。ファイブアイズなので、この記入は5画像単位になっています。”

![imageRL2-8-3](/2022-06-20-QEUR21_SOART6/imageRL2-8-3.jpg)

D先生 ： “ベース画像は数百枚あるので、まずは数枚だけ出力するように絞り込んで、さらに番号で指定するというやりかたですね。それではプログラムをドン・・・。”

```python
# ---------------------------
# ファイブアイズ畳み込みSOART法の本実験
# soaRT_inspection_featuremap.py
# CSVファイル上の5枚の画像(１つのファイブアイズ分)を読み込み、その特徴量マップを出力する
# 特徴量マップは、Y1-Y2-Y3のヒートマップ
# ---------------------------
# モジュールのインポート
import cv2
from fastai.vision.all import *
import seaborn as sns
import pandas as pd
import numpy as np
from scipy.spatial import distance
import matplotlib.pyplot as plt

# ---------------------------
# 読み込み先の指定
foldername = "./ARRAY_RTCNN/"
newsize = (900, 300)

#=================================================
# PREPARATION : csvファイルを読み込む
#=================================================
# VR画像ファイルを読み込み表示する
def read_vrImgfile(file_readcsv): 
 
    # 畳み込みファイルの読み込み
    df = pd.read_csv(file_readcsv) 
    df = df[df["calc"]=="calc"]
    dfL = df.query("camNO == 0")
    dfC = df.query("camNO == 1")
    dfR = df.query("camNO == 2")
    dfU = df.query("camNO == 3")
    dfD = df.query("camNO == 4")
    # ------------------
    # file_nameのリストを生成する
    mx_Xs = df.loc[:,"file_name":"label"].values
    arr_VRfL = dfL.loc[:,"file_name"].values
    arr_VRfC = dfC.loc[:,"file_name"].values
    arr_VRfR = dfR.loc[:,"file_name"].values
    arr_VRfU = dfU.loc[:,"file_name"].values
    arr_VRfD = dfD.loc[:,"file_name"].values
    #print("----- mx_Xs -----")
    #print(mx_Xs)
  
    return mx_Xs, arr_VRfL, arr_VRfC, arr_VRfR, arr_VRfU, arr_VRfD

# ---------------------------
# CSVファイル(実験)情報を読み込み表示する
file_VRinput = foldername + "labels_all.csv"  # ファイルパス名の生成 
mx_Xs, arr_VRfL, arr_VRfC, arr_VRfR, arr_VRfU, arr_VRfD = read_vrImgfile(file_VRinput)
num_VRfs = len(arr_VRfC)
print("num_VRfs: ",num_VRfs)
print("arr_VRfL: ",arr_VRfL)
print("arr_VRfR: ",arr_VRfR)
print("arr_VRfU: ",arr_VRfU)
print("arr_VRfD: ",arr_VRfD)

# ---------------------------
# 画像を選択する
select_number = 0

pic_nameL = arr_VRfL[select_number] + ".png"
pic_nameC = "average_pic.png"
pic_nameR = arr_VRfR[select_number] + ".png"
pic_nameU = arr_VRfU[select_number] + ".png"
pic_nameD = arr_VRfD[select_number] + ".png"
print("pic_nameL: ",pic_nameL)
print("pic_nameR: ",pic_nameR)
print("pic_nameU: ",pic_nameU)
print("pic_nameD: ",pic_nameD)

#=================================================
# READ CSV_FILE FUNCTION
#=================================================
# 畳み込みファイルを読み込み表示する
def read_csvfile(file_readcsv): 
 
    # 畳み込みファイルの読み込み
    df = pd.read_csv(file_readcsv) 
    # ------------------
    # 選択項目の読み込み
    # Xsマトリックス
    mx_Xs = df.loc[:,"col1":"col5"].values
    #print("----- mx_Xs -----")
    #print(mx_Xs)
  
    return mx_Xs

# ---------------------------
# LCR画像ファイルを読んで、テンソルを出力する
def read_LCRpics(pic_nameL, pic_nameC, pic_nameR, pic_nameU, pic_nameD): 

    # ------------------
    # 左(L)画像
    filename = foldername + pic_nameL
    imgL = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)
    img_cropL = imgL[212:856,47:1867]
    # リサイズ
    img_resizeL = cv2.resize(img_cropL , newsize)

    # ------------------
    # 中央(C)画像
    filename = foldername + pic_nameC
    imgC = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)
    img_cropC = imgC[212:856,47:1867]
    # リサイズ
    img_resizeC = cv2.resize(img_cropC , newsize)

    # ------------------
    # 右(R)画像
    filename = foldername + pic_nameR
    imgR = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)
    img_cropR = imgR[212:856,47:1867]
    # リサイズ
    img_resizeR = cv2.resize(img_cropR , newsize)
    # 画像出力
    #cv2.imshow("image(Right)", img_resizeR)
    #cv2.waitKey(0)
    #cv2.destroyAllWindows()

    # ------------------
    # 上(U)画像
    filename = foldername + pic_nameU
    imgU = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)
    img_cropU = imgU[212:856,47:1867]
    # リサイズ
    img_resizeU = cv2.resize(img_cropU , newsize)

    # ------------------
    # 下(D)画像
    filename = foldername + pic_nameD
    imgD = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)
    img_cropD = imgD[212:856,47:1867]
    # リサイズ
    img_resizeD = cv2.resize(img_cropD , newsize)

    # ------------------
    # 画像データのテンソル化(L,C,R)
    img_tensorL = tensor(img_resizeL/255)  # 信号空間(左)
    img_tensorC = tensor(img_resizeC/255)  # 単位空間（標準ベクトル）
    img_tensorR = tensor(img_resizeR/255)  # 信号空間(右)
    img_tensorU = tensor(img_resizeU/255)  # 信号空間(上)
    img_tensorD = tensor(img_resizeD/255)  # 単位空間（下）
    
    return img_tensorL, img_tensorC, img_tensorR, img_tensorU, img_tensorD

# ---------------------------
# ファイブアイズ法RTメトリックスを計算する(テンソル活用版)
def calc_deyeRTmet(len_temp, max_jy_index, tsr_sig_matrix, tsr_tani_array): 

    # ---------------------------
    # 変数の初期化
    L1_loss = torch.nn.L1Loss()
    btY1_yarray, Y2_yarray = [], []

    # 繰り返し
    for i in range(len_temp):

        y = tsr_sig_matrix[i]
        x = tsr_tani_array

        xx = torch.dot(x,x)
        xy = torch.dot(x,y)
        beta = xy/xx
        #print("i:{}, beta:{}".format(i,beta))

        mDistance   = L1_loss(y, beta*x)
        #print("i:{}, beta:{}".format(i,beta))
        #print("mDistance: ", mDistance.item())
        #print("yres:{}".format(yres))
        
        btY1_yarray.append(beta.item())
        Y2_yarray.append(mDistance.item())
            
    return round(Y2_yarray[0] - Y2_yarray[1], 5)

# ---------------------------
# 畳み込み処理を実施する
def apply_kernel(row, col, kernel, img_tensor):
    return (img_tensor[row:row+max_cnv_row,col:col+max_cnv_col] * kernel).sum()

# ---------------------------
# soaRTメトリックスを計算する(テンソル活用版)
def calc_soaRT(len_temp, max_jy_index, tsr_sig_matrix, tsr_tani_array): 

    # ---------------------------
    # 変数の初期化
    L1_loss = torch.nn.L1Loss()
    tsr_zero = torch.zeros(max_jy_index)
    btY1_yarray, Y2_yarray, Y3_yarray = [], [], []

    # 繰り返し
    for i in range(len_temp):

        y = tsr_sig_matrix[i]
        x = tsr_tani_array

        # 回転を計測
        xx = torch.dot(x,x)
        xy = torch.dot(x,y)
        beta = xy/xx
        #print("i:{}, beta:{}".format(i,beta))

        # 体積ひずみを計測
        vol_xtani = L1_loss(beta*x, tsr_zero)
        vol_ysig  = L1_loss(y, tsr_zero)
        ratio_vol = vol_ysig/vol_xtani

        # せん断ひずみを計測
        mDistance   = L1_loss(y, beta*ratio_vol*x)
        #print("mDistance: ", mDistance.item())
        #print("yres:{}".format(yres))
        
        btY1_yarray.append(beta.item())
        Y2_yarray.append(ratio_vol.item())
        Y3_yarray.append(mDistance.item())

    return btY1_yarray, Y2_yarray, Y3_yarray

#=================================================
# SUB PROGRAM(1) : ファイブアイズRTメトリックスの準備
#=================================================
# 1.ファイブアイズRTメトリックス
# 画像リサイズのパラメタ
newsize = (900, 300)
# テンソルイメージ処理のパラメタ
dmax_tsr_col, dmax_tsr_row   = newsize[0], newsize[1]  #(900, 300)
# ストライド量
stride_tsr_row, stride_tsr_col = 5, 5
# RT法処理のサイズ
drtm_tsr_row, drtm_tsr_col = 5, 5
# RTベクトル長と信号空間長
dmax_jy_idx  = 5*5
len_dmx      = 2  # 右と左

# ---------------------------
# 1.ファイブアイズRTメトリックス: テンソル行列の処理(for)開始点のベクトルを生成
row_range1, col_range1 = [], []
sum_row   = 0
while sum_row <= dmax_tsr_row-drtm_tsr_row:
    row_range1.append(sum_row)
    sum_row  += stride_tsr_row
# -----
sum_col = 0
while sum_col <= dmax_tsr_col-drtm_tsr_col:
    col_range1.append(sum_col)
    sum_col  += stride_tsr_col

#=================================================
# SUB PROGRAM(2) : 畳み込みメトリックスの準備
#=================================================
# 2.畳み込み：画像の処理(for)開始点のベクトルを生成する
# パラメタの設定
max_cnv_parts  = 8
# 画像サンプルのサイズ
max_sp_row, max_sp_col = 60, 180
# 畳み込み部品のサイズ
max_cnv_row, max_cnv_col = 5, 5
# ストライド量
stride_cnv_row, stride_cnv_col = 5, 5
# ---------------------------
# 2.畳み込み：画像の処理(for)開始点のベクトルを生成する
row_range2, col_range2 = [], []
sum_row   = 0
while sum_row <= max_sp_row-max_cnv_row:
    row_range2.append(sum_row)
    sum_row  += stride_cnv_row
# -----
sum_col = 0
while sum_col <= max_sp_col-max_cnv_col:
    col_range2.append(sum_col)
    sum_col  += stride_cnv_col
# ------------------
nam_cnv_input = "基準用CSVデータ" 
code_cnv_input = ["NA"] * 8 # CSVコードの指定
code_cnv_input[0] = "bend1_cnv" # CSVコードの指定
code_cnv_input[1] = "bend2_cnv" # CSVコードの指定
code_cnv_input[2] = "bend3_cnv" # CSVコードの指定
code_cnv_input[3] = "bend4_cnv" # CSVコードの指定
code_cnv_input[4] = "line1_cnv" # CSVコードの指定
code_cnv_input[5] = "line2_cnv" # CSVコードの指定
code_cnv_input[6] = "datum1_cnv" # CSVコードの指定
code_cnv_input[7] = "datum2_cnv" # CSVコードの指定
# ------------------
# 畳み込み処理を実施する
for i in range(max_cnv_parts):
    # ---------------------------
    # CSVファイル(実験)情報を読み込み表示する
    # 畳み込みファイル
    file_cnv_input = foldername + code_cnv_input[i] + ".csv"  # ファイルパス名の生成 
    mx_conv = read_csvfile(file_cnv_input)
    if i == 0:    
        tensor_bend1 = tensor(mx_conv).float()
    elif i == 1:
        tensor_bend2 = tensor(mx_conv).float()
    elif i == 2:
        tensor_bend3 = tensor(mx_conv).float()
    elif i == 3:
        tensor_bend4 = tensor(mx_conv).float()
    elif i == 4:
        tensor_line1 = tensor(mx_conv).float()
    elif i == 5:
        tensor_line2 = tensor(mx_conv).float()
    elif i == 6:
        tensor_datum1 = tensor(mx_conv).float()
    elif i == 7:
        tensor_datum2 = tensor(mx_conv).float()
# ---------------------------
# 畳み込みカーネルを生成する
signal_kernels  = torch.stack([tensor_bend1, tensor_bend2, tensor_bend3, tensor_bend4, tensor_line1, tensor_line2])
tani_kernel     = tensor_datum1 + tensor_datum2

#=================================================
# SUB PROGRAM(3) : 畳み込みRTメトリックスの準備
#=================================================
# 3. 畳み込みRTメトリックス
# テンソルイメージ処理のパラメタ
cmax_tsr_row, cmax_tsr_col = 12, 36
# ストライド量
stride_tsr_row, stride_tsr_col = 4, 4
# RT法処理のサイズ
crtm_tsr_row, crtm_tsr_col = 4, 4
# RTベクトル長と信号空間長
cmax_jy_idx, len_cmx  = 4*4, 6
# ---------------------------
# テンソル行列の処理(for)開始点のベクトルを生成する
row_range3, col_range3 = [], []
sum_row   = 0
while sum_row <= cmax_tsr_row-crtm_tsr_row:
    row_range3.append(sum_row)
    sum_row  += stride_tsr_row
#print(row_range3)
# -----
sum_col = 0
while sum_col <= cmax_tsr_col-crtm_tsr_col:
    col_range3.append(sum_col)
    sum_col  += stride_tsr_col
#print(col_range3)

#=================================================
# FUNCTION(1) : ファイブアイズでモジュール化した部分
#=================================================
# ファイブアイズRTテンソルを生成する
def create_feyeRT(img_tensorL, img_tensorC, img_tensorR):

    # ファイブアイズRT法によるマトリックスの生成
    num_rows, num_cols = len(row_range1), len(col_range1)
    feyeY2 = []
    # ------
    cnt_row = 0
    for row in row_range1:
        cnt_col = 0
        for col in col_range1:
            # ------
            # 単位空間の空間ベクトルを生成する
            tsr_tani_array = img_tensorC[row:row+drtm_tsr_row,col:col+drtm_tsr_col].flatten()
            #print(tsr_tani_array)
            # ------
            # 信号空間の空間マトリックスを生成する
            tsr_sigL = img_tensorL[row:row+drtm_tsr_row,col:col+drtm_tsr_col].flatten()
            tsr_sigR = img_tensorR[row:row+drtm_tsr_row,col:col+drtm_tsr_col].flatten()
            # -----
            tsr_sig_matrix = torch.stack([tsr_sigL, tsr_sigR])
            #print("----- tsr_sig_matrix -----")
            #print(tsr_sig_matrix)
            # ------
            # ファイブアイズ法RTメトリックスを計算する
            temp_feyeY2 = calc_deyeRTmet(len_dmx, dmax_jy_idx, tsr_sig_matrix, tsr_tani_array)
            feyeY2.append(temp_feyeY2)
            # ------
            # COUNT UP
            cnt_col = cnt_col + 1
        # ------
        # COUNT UP.append
        cnt_row = cnt_row + 1

    # ---------------------------
    # 結果をテキストで出力する
    mx_fY2       = np.array(feyeY2).reshape([num_rows, num_cols])
    tensor_fY2   = tensor(mx_fY2)
    #print(tensor_fY2)

    return tensor_fY2


# 畳み込みテンソルイメージを生成する
def create_feyeConv(tensor_Y1Y2):

    # 単位空間用の畳み込み処理を実施する
    tani_image = tensor([[apply_kernel(i,j, tani_kernel, tensor_Y1Y2) for j in col_range2] for i in row_range2])
    #print("----- tani_image -----")
    #print(tani_image)
    # ------------------
    # 信号空間用の畳み込み処理を実施する
    for i in range(max_cnv_parts-2):
        # ---------------------------
        # CSVファイル(実験)情報を読み込み表示する
        kernel = signal_kernels[i]
        calc_conv = tensor([[apply_kernel(i,j, kernel, tensor_Y1Y2) for j in col_range2] for i in row_range2])
        # -----
        if i == 0:    
            tsr_cvbend1 = tensor(calc_conv).float()
        elif i == 1:
            tsr_cvbend2 = tensor(calc_conv).float()
        elif i == 2:
            tsr_cvbend3 = tensor(calc_conv).float()
        elif i == 3:
            tsr_cvbend4 = tensor(calc_conv).float()
        elif i == 4:
            tsr_cvline1 = tensor(calc_conv).float()
        elif i == 5:
            tsr_cvline2 = tensor(calc_conv).float()
    # ---------------------------
    # 畳み込みテンソルイメージの生成  
    signal_images = torch.stack([tsr_cvbend1, tsr_cvbend2, tsr_cvbend3, tsr_cvbend4, tsr_cvline1, tsr_cvline2])
    # 結果の出力
    #print(signal_images.shape)     # torch.Size([6, 12, 36])
    #print("----- signal_images[0] -----")
    #print(signal_images[0])

    return tani_image, signal_images


# 畳み込みRTメトリックスを生成する
def create_CRTmetric(tani_image, signal_images):

    # RTメトリックスをCSVファイルに出力するための行列
    mx_p0Y1, mx_p1Y1, mx_p2Y1 = np.zeros([3,9]), np.zeros([3,9]), np.zeros([3,9])
    mx_p3Y1, mx_p4Y1, mx_p5Y1 = np.zeros([3,9]), np.zeros([3,9]), np.zeros([3,9])
    # ----
    mx_p0Y2, mx_p1Y2, mx_p2Y2 = np.zeros([3,9]), np.zeros([3,9]), np.zeros([3,9])
    mx_p3Y2, mx_p4Y2, mx_p5Y2 = np.zeros([3,9]), np.zeros([3,9]), np.zeros([3,9])
    # ----
    mx_p0Y3, mx_p1Y3, mx_p2Y3 = np.zeros([3,9]), np.zeros([3,9]), np.zeros([3,9])
    mx_p3Y3, mx_p4Y3, mx_p5Y3 = np.zeros([3,9]), np.zeros([3,9]), np.zeros([3,9])
    # ---------------------------
    cnt_row = 0
    for row in row_range3:
        cnt_col = 0
        for col in col_range3:
            # ------
            # 単位空間の空間ベクトルを生成する
            tsr_tani_array = tani_image[row:row+crtm_tsr_row,col:col+crtm_tsr_col].flatten()
            #print(tsr_tani_array)
            # ------
            # 信号空間の空間マトリックスを生成する
            temp_matrix = signal_images[0]
            tsr_sig_p0 = temp_matrix[row:row+crtm_tsr_row,col:col+crtm_tsr_col].flatten()
            temp_matrix = signal_images[1]
            tsr_sig_p1 = temp_matrix[row:row+crtm_tsr_row,col:col+crtm_tsr_col].flatten()
            temp_matrix = signal_images[2]
            tsr_sig_p2 = temp_matrix[row:row+crtm_tsr_row,col:col+crtm_tsr_col].flatten()
            temp_matrix = signal_images[3]
            tsr_sig_p3 = temp_matrix[row:row+crtm_tsr_row,col:col+crtm_tsr_col].flatten()
            temp_matrix = signal_images[4]
            tsr_sig_p4 = temp_matrix[row:row+crtm_tsr_row,col:col+crtm_tsr_col].flatten()
            temp_matrix = signal_images[5]
            tsr_sig_p5 = temp_matrix[row:row+crtm_tsr_row,col:col+crtm_tsr_col].flatten()
            # -----
            tsr_sig_matrix = torch.stack([tsr_sig_p0, tsr_sig_p1, tsr_sig_p2, tsr_sig_p3, tsr_sig_p4, tsr_sig_p5])
            #print("----- tsr_sig_matrix -----")
            #print(tsr_sig_matrix)
            # ------
            # RTメトリックスを計算する
            btY1_yarray, Y2_yarray, Y3_yarray = calc_soaRT(len_cmx, cmax_jy_idx, tsr_sig_matrix, tsr_tani_array)
            # ------
            # CSV出力用のマトリックスを生成する       
            mx_p0Y1[cnt_row, cnt_col] = round(btY1_yarray[0],3)
            mx_p1Y1[cnt_row, cnt_col] = round(btY1_yarray[1],3)
            mx_p2Y1[cnt_row, cnt_col] = round(btY1_yarray[2],3)
            mx_p3Y1[cnt_row, cnt_col] = round(btY1_yarray[3],3)
            mx_p4Y1[cnt_row, cnt_col] = round(btY1_yarray[4],3)
            mx_p5Y1[cnt_row, cnt_col] = round(btY1_yarray[5],3)
            # -----
            mx_p0Y2[cnt_row, cnt_col] = round(Y2_yarray[0],3)
            mx_p1Y2[cnt_row, cnt_col] = round(Y2_yarray[1],3)
            mx_p2Y2[cnt_row, cnt_col] = round(Y2_yarray[2],3)
            mx_p3Y2[cnt_row, cnt_col] = round(Y2_yarray[3],3)
            mx_p4Y2[cnt_row, cnt_col] = round(Y2_yarray[4],3)
            mx_p5Y2[cnt_row, cnt_col] = round(Y2_yarray[5],3)
            # -----
            mx_p0Y3[cnt_row, cnt_col] = round(Y3_yarray[0],3)
            mx_p1Y3[cnt_row, cnt_col] = round(Y3_yarray[1],3)
            mx_p2Y3[cnt_row, cnt_col] = round(Y3_yarray[2],3)
            mx_p3Y3[cnt_row, cnt_col] = round(Y3_yarray[3],3)
            mx_p4Y3[cnt_row, cnt_col] = round(Y3_yarray[4],3)
            mx_p5Y3[cnt_row, cnt_col] = round(Y3_yarray[5],3)
            # ------
            # COUNT UP
            #print(cnt_row, cnt_col)
            cnt_col = cnt_col + 1
        # ------
        # COUNT UP
        cnt_row = cnt_row + 1
    # ---------------------------
    # Chain化する
    mx_Y1   = [mx_p0Y1, mx_p1Y1, mx_p2Y1, mx_p3Y1, mx_p4Y1, mx_p5Y1]
    mx_Y2   = [mx_p0Y2, mx_p1Y2, mx_p2Y2, mx_p3Y2, mx_p4Y2, mx_p5Y2]
    mx_Y3   = [mx_p0Y3, mx_p1Y3, mx_p2Y3, mx_p3Y3, mx_p4Y3, mx_p5Y3]
    # 結果のテキスト出力する
    #print(mx_Y2)

    return mx_Y1, mx_Y2, mx_Y3

#=================================================
# FUNCTION(2) : ファイブアイズ畳み込みRTメトリックスを連続して実行する
#=================================================
def bind_process(img_tensorL, img_tensorC, img_tensorR, img_tensorU, img_tensorD):

    #=================================================
    # MAIN PROGRAM(1) : ファイブアイズRTメトリックスを実行する
    #=================================================
    # ファイブアイズRTテンソルを生成する
    tensor_LCRY2   = create_feyeRT(img_tensorL, img_tensorC, img_tensorR)
    tensor_UCDY2   = create_feyeRT(img_tensorU, img_tensorC, img_tensorD)

    #=================================================
    # MAIN PROGRAM(2) : 畳み込みメトリックスを実行する
    #=================================================
    # 畳み込みテンソルイメージを生成する
    tani_LCRimage, signal_LCRimages = create_feyeConv(tensor_LCRY2)
    tani_UCDimage, signal_UCDimages = create_feyeConv(tensor_UCDY2)

    #=================================================
    # MAIN PROGRAM(3) : 畳み込みRTメトリックスを実行する
    #=================================================
    # 畳み込みRTメトリックスを生成する
    mx_Y1_LCR, mx_Y2_LCR, mx_Y3_LCR = create_CRTmetric(tani_LCRimage, sig-nal_LCRimages)
    mx_Y1_UCD, mx_Y2_UCD, mx_Y3_UCD = create_CRTmetric(tani_UCDimage, sig-nal_UCDimages)

    return mx_Y1_LCR, mx_Y1_UCD, mx_Y2_LCR, mx_Y2_UCD, mx_Y3_LCR, mx_Y3_UCD

#=================================================
# MAIN PROGRAM(1) : ファイブアイズRTメトリックス
#=================================================
# LCR画像ファイルを読んで、テンソルを出力する
img_tensorL, img_tensorC, img_tensorR, img_tensorU, img_tensorD = read_LCRpics(pic_nameL, pic_nameC, pic_nameR, pic_nameU, pic_nameD)
#print(img_tensorC)
# ---------------------------
# チェーン化された感度とSN比を出力する
mx_Y1_LCR, mx_Y1_UCD, mx_Y2_LCR, mx_Y2_UCD, mx_Y3_LCR, mx_Y3_UCD = bind_process(img_tensorL, img_tensorC, img_tensorR, img_tensorU, img_tensorD)
# 結果のテキスト出力する
#print("---- NO:{} ----".format(i))
#print(mx_btY1_bindA[0])
#print(mx_Y2_LCRA[0])
# -----
# Y1類の場合
arr_p0Y1 = np.array(mx_Y1_LCR[0]).flatten()
arr_p1Y1 = np.array(mx_Y1_LCR[1]).flatten()
arr_p2Y1 = np.array(mx_Y1_LCR[2]).flatten()
arr_p3Y1 = np.array(mx_Y1_LCR[3]).flatten()
arr_p4Y1 = np.array(mx_Y1_LCR[4]).flatten()
arr_p5Y1 = np.array(mx_Y1_LCR[5]).flatten()
arr_p6Y1 = np.array(mx_Y1_UCD[0]).flatten()
arr_p7Y1 = np.array(mx_Y1_UCD[1]).flatten()
arr_p8Y1 = np.array(mx_Y1_UCD[2]).flatten()
arr_p9Y1 = np.array(mx_Y1_UCD[3]).flatten()
arr_p10Y1 = np.array(mx_Y1_UCD[4]).flatten()
arr_p11Y1 = np.array(mx_Y1_UCD[5]).flatten()
# -----
# Y2類の場合
arr_p0Y2 = np.array(mx_Y2_LCR[0]).flatten()
arr_p1Y2 = np.array(mx_Y2_LCR[1]).flatten()
arr_p2Y2 = np.array(mx_Y2_LCR[2]).flatten()
arr_p3Y2 = np.array(mx_Y2_LCR[3]).flatten()
arr_p4Y2 = np.array(mx_Y2_LCR[4]).flatten()
arr_p5Y2 = np.array(mx_Y2_LCR[5]).flatten()
arr_p6Y2 = np.array(mx_Y2_UCD[0]).flatten()
arr_p7Y2 = np.array(mx_Y2_UCD[1]).flatten()
arr_p8Y2 = np.array(mx_Y2_UCD[2]).flatten()
arr_p9Y2 = np.array(mx_Y2_UCD[3]).flatten()
arr_p10Y2 = np.array(mx_Y2_UCD[4]).flatten()
arr_p11Y2 = np.array(mx_Y2_UCD[5]).flatten()
# -----
# Y3類の場合
arr_p0Y3 = np.array(mx_Y3_LCR[0]).flatten()
arr_p1Y3 = np.array(mx_Y3_LCR[1]).flatten()
arr_p2Y3 = np.array(mx_Y3_LCR[2]).flatten()
arr_p3Y3 = np.array(mx_Y3_LCR[3]).flatten()
arr_p4Y3 = np.array(mx_Y3_LCR[4]).flatten()
arr_p5Y3 = np.array(mx_Y3_LCR[5]).flatten()
arr_p6Y3 = np.array(mx_Y3_UCD[0]).flatten()
arr_p7Y3 = np.array(mx_Y3_UCD[1]).flatten()
arr_p8Y3 = np.array(mx_Y3_UCD[2]).flatten()
arr_p9Y3 = np.array(mx_Y3_UCD[3]).flatten()
arr_p10Y3 = np.array(mx_Y3_UCD[4]).flatten()
arr_p11Y3 = np.array(mx_Y3_UCD[5]).flatten()
# ---------------------------
# データを結合して信号空間を生成する
mx_Xs_sigY1  = np.vstack([arr_p0Y1, arr_p1Y1, arr_p2Y1, arr_p3Y1, arr_p4Y1, arr_p5Y1, arr_p6Y1, arr_p7Y1, arr_p8Y1, arr_p9Y1, arr_p10Y1, arr_p11Y1])
mx_Xs_sigY2  = np.vstack([arr_p0Y2, arr_p1Y2, arr_p2Y2, arr_p3Y2, arr_p4Y2, arr_p5Y2, arr_p6Y2, arr_p7Y2, arr_p8Y2, arr_p9Y2, arr_p10Y2, arr_p11Y2])
mx_Xs_sigY3  = np.vstack([arr_p0Y3, arr_p1Y3, arr_p2Y3, arr_p3Y3, arr_p4Y3, arr_p5Y3, arr_p6Y3, arr_p7Y3, arr_p8Y3, arr_p9Y3, arr_p10Y3, arr_p11Y3])
# 結果のテキスト出力する
#print(mx_Xs_sigY2.shape)

#=================================================
# READ CSV_FILE FUNCTION
#=================================================
# ファイルを読み込み表示する(相関逆行列)
def read_invcorr(code_cnv_input): 

    # 相関逆行列ファイルの読み込み
    file_cnv_input = foldername + code_cnv_input + ".csv"  # ファイルパス名の生成 
    df = pd.read_csv(file_cnv_input)
    # 選択項目の読み込み
    # Xsマトリックス
    mx_Xs = df.loc[:,"0":"11"].values
    #print("----- mx_Xs -----")
    #print(mx_Xs)
  
    return mx_Xs
# ------------------
# ファイルを読み込み表示する(相関逆行列)
# Y1類の場合
cov_iY1A = read_invcorr("Rtm_covY1A")
cov_iY1B = read_invcorr("Rtm_covY1B")
cov_iY1C = read_invcorr("Rtm_covY1C")
# -----
# Y2類の場合
cov_iY2A = read_invcorr("Rtm_covY2A")
cov_iY2B = read_invcorr("Rtm_covY2B")
cov_iY2C = read_invcorr("Rtm_covY2C")
# -----
# Y3類の場合
cov_iY3A = read_invcorr("Rtm_covY3A")
cov_iY3B = read_invcorr("Rtm_covY3B")
cov_iY3C = read_invcorr("Rtm_covY3C")
#print(cov_iY2A)
   
# ------------------
# ファイルを読み込み表示する(単位空間の中心)
def read_taniXs(code_cnv_input): 
 
    # 単位空間の中心ファイルの読み込み
    file_cnv_input = foldername + code_cnv_input + ".csv"  # ファイルパス名の生成
    df = pd.read_csv(file_cnv_input) 
    # 選択項目の読み込み
    # Xsマトリックス
    mx_Xs = df.loc[:,"0":"11"].values
    #print("----- mx_Xs -----")
    #print(mx_Xs)
  
    return mx_Xs.flatten()
# ------------------
# ファイルを読み込み表示する(単位空間の中心)
# Y1類の場合
arr_Xs_taniY1A = read_taniXs("Rtm_aveY1A")
arr_Xs_taniY1B = read_taniXs("Rtm_aveY1B")
arr_Xs_taniY1C = read_taniXs("Rtm_aveY1C")
# -----
# Y2類の場合
arr_Xs_taniY2A = read_taniXs("Rtm_aveY2A")
arr_Xs_taniY2B = read_taniXs("Rtm_aveY2B")
arr_Xs_taniY2C = read_taniXs("Rtm_aveY2C")
# -----
# Y3類の場合
arr_Xs_taniY3A = read_taniXs("Rtm_aveY3A")
arr_Xs_taniY3B = read_taniXs("Rtm_aveY3B")
arr_Xs_taniY3C = read_taniXs("Rtm_aveY3C")
#print(arr_Xs_taniY2A)

#=================================================
# MAIN PROGRAM(3) : MAHARANOBIS　FEATURE　ENGINEERING
#=================================================
# マハラノビスマトリックスとグラフ作画準備
max_iRow_cells = 12
max_jCol_cells = 3*9

# 単位空間の分配ベクトルの定義
bun_temp = np.array([[1,1,2,2,2,2,2,3,3],[1,1,2,2,2,2,2,3,3],[1,1,2,2,2,2,2,3,3],])
arr_bun  = bun_temp.flatten()
#print(arr_bun)

# ------------------
# 2つの標本 [X_tani, X_sig] と [(mean)arr_X_tani] のマハラノビス距離を計算する
arr_dY1, arr_dY2, arr_dY3 = [], [], []
for jCol in range(max_jCol_cells):
    # -----
    if arr_bun[jCol] == 1:
        arr_Xs_taniY1 = arr_Xs_taniY1A
        arr_Xs_taniY2 = arr_Xs_taniY2A
        arr_Xs_taniY3 = arr_Xs_taniY3A
        cov_iY1 = cov_iY1A
        cov_iY2 = cov_iY2A
        cov_iY3 = cov_iY3A
    elif arr_bun[jCol] == 2:
        arr_Xs_taniY1 = arr_Xs_taniY1B
        arr_Xs_taniY2 = arr_Xs_taniY2B
        arr_Xs_taniY3 = arr_Xs_taniY3B
        cov_iY1 = cov_iY1B
        cov_iY2 = cov_iY2B
        cov_iY3 = cov_iY3B
    elif arr_bun[jCol] == 3:
        arr_Xs_taniY1 = arr_Xs_taniY1C
        arr_Xs_taniY2 = arr_Xs_taniY2C
        arr_Xs_taniY3 = arr_Xs_taniY3C
        cov_iY1 = cov_iY1C
        cov_iY2 = cov_iY2C
        cov_iY3 = cov_iY3C
    # -----
    dY1 = distance.mahalanobis(mx_Xs_sigY1[:,jCol], arr_Xs_taniY1, cov_iY1)
    dY2 = distance.mahalanobis(mx_Xs_sigY2[:,jCol], arr_Xs_taniY2, cov_iY2)
    dY3 = distance.mahalanobis(mx_Xs_sigY3[:,jCol], arr_Xs_taniY3, cov_iY3)
    arr_dY1.append(round(dY1,4))
    arr_dY2.append(round(dY2,4))
    arr_dY3.append(round(dY3,4))
    #print("jCol:{}, マハラノビス距離の計算結果:{}".format(jCol,dY2))
# ------
#print("arr_dY3: ",arr_dY3)
mx_MHdistY1 = np.reshape(np.array(arr_dY1), (3, 9))
mx_MHdistY2 = np.reshape(np.array(arr_dY2), (3, 9))
mx_MHdistY3 = np.reshape(np.array(arr_dY3), (3, 9))
#print("---- mx_MHdistY3 ----")
#print(mx_MHdistY3)

# ------------------
# マハラノビス距離のマトリックスを生成する
arr_index   = list(range(3))
arr_columns = list(range(9))
# -----
# Y1類の場合
df_mahaY1 = pd.DataFrame(mx_MHdistY1, index=arr_index, columns=arr_columns)
#print("---- df_mahaY1 ----")
#print(df_mahaY1)
# ------------------
# ヒートマップへ出力する
fig = plt.figure(figsize=(14, 9))
ax1 = fig.add_subplot(2, 2, 1)
ax1.set_title("Y1 Distance:({})".format(pic_nameL))
sns.heatmap(df_mahaY1, ax=ax1, annot=True, cbar=True, fmt='.2f')
# -----
# Y2類の場合
df_mahaY2 = pd.DataFrame(mx_MHdistY2, index=arr_index, columns=arr_columns)
#print("---- df_mahaY2 ----")
#print(df_mahaY2)
# ------------------
# ヒートマップへ出力する
ax2 = fig.add_subplot(2, 2, 2)
ax2.set_title("Y2 Distance:({})".format(pic_nameL))
sns.heatmap(df_mahaY2, ax=ax2, annot=True, cbar=True, fmt='.2f')
# -----
# Y3類の場合
df_mahaY3 = pd.DataFrame(mx_MHdistY3, index=arr_index, columns=arr_columns)
#print("---- df_mahaY3 ----")
#print(df_mahaY3)
# ------------------
# ヒートマップへ出力する
ax3 = fig.add_subplot(2, 2, 3)
ax3.set_title("Y3 Distance:({})".format(pic_nameL))
sns.heatmap(df_mahaY3, ax=ax3, annot=True, cbar=True, fmt='.2f')
# show plots
fig.tight_layout()
plt.show()

```

QEU:FOUNDER ： “あとは、ゆる～く、特徴マップを見てみましょう。あらかじめ言っておくが、**今回の実験は失敗しました**・・・（笑）。”

![imageRL2-8-4](/2022-06-20-QEUR21_SOART6/imageRL2-8-4.jpg)

D先生 ： “4つの図のうち、右上が例のY2（データ体積比）のマハラノビス距離ですね。それにしても、Y2の分布は極端じゃありません？”

QEU:FOUNDER ： “あのね・・・、右下の写真を見ればわかるが画像データ作成時に与えたシフトと回転量が大きすぎるんだ。プログラムでは「窓」を作って不要な部分をトリムをしているんだが、おそらく端部では端子の画像が削られているだろう。Y2の値はデータ体積の評価指標だから、必要以上に大きく影響を受けるんです。”

D先生 ： “（実験を）やり直すの？”

QEU:FOUNDER ： “いいじゃない。もう、**「実験をやってみた」**で・・・（笑）。本当に（自動検査機を）やってみたい人は、我々が晒したプログラムを使って自分で検証するだろうからね。我々は、「ノリ」でぶっちぎりましょう。次に正常状態のデータをもうひとつみてみましょうか。”

![imageRL2-8-5](/2022-06-20-QEUR21_SOART6/imageRL2-8-5.jpg)

D先生 ： “新RTの時と比較して、SOARTのSN比（Y3）の分布は全体的におとなしいような気がします。単位空間が３つ（左-中央-右）ありますが、中央だけなんですよね。暴れているのが・・・。端子後退の事例を見せてもらえませんか？”

**（アドレス3-1の端子後退）**

![imageRL2-8-6](/2022-06-20-QEUR21_SOART6/imageRL2-8-6.jpg)

**（アドレス4-1の端子後退）**

![imageRL2-8-7](/2022-06-20-QEUR21_SOART6/imageRL2-8-7.jpg)

QEU:FOUNDER ： “実験の失敗で物体が上下左右に大きくずれているので、メトリックスの分布も乱れています。・・・でも、その一方で「おもしろいモノ」がみられます。Y2の分布のうち、アドレス4-1の値を見てごらん？少しY2値が少し上がっているから・・・。SOART法の数理では、体積比（Y2）の基本は1.0です。これから外れれば、SOART法のSN比の値は新RT法のSN比よりも大きくなります。予備実験の結果（↓）を参考にして・・・。”

**（注意：予備実験の結果です）**

![imageRL2-8-8](/2022-06-20-QEUR21_SOART6/imageRL2-8-8.jpg)

D先生 ： “上の図で体積比（Y2）が1.0になる条件はカメラの高さ(Z-position)の値が0付近ということですね。ですから、「SOART法ではY2の導入によって不良を発見しやすくなった」といえるんですね。次に、端子傾きの不良の事例をみてみましょう。”

**（アドレス3-1の端子傾き）**

![imageRL2-8-9](/2022-06-20-QEUR21_SOART6/imageRL2-8-9.jpg)

**（アドレス4-1の端子傾き）**

![imageRL2-8-10](/2022-06-20-QEUR21_SOART6/imageRL2-8-10.jpg)

QEU:FOUNDER ： “システムがそれなりに不良を見つけていそうだが、発生部位がずれています。やっぱり、今回の実験は失敗だったんだなぁ・・・。でも、新RT法に比べて明らかに良くなったを思えるのは周辺部のY3値がいきなり大きくなるという現象が少なくなったことですね。”

D先生 ： “少しだけロジックは進歩したが・・・。この「（データの）失敗作」を使って、ひきつづき機械学習をやるんでしょう？”

QEU:FOUNDER ： “この画像データを学習させて0.85ぐらいの正確度がでればスゴイと思います（笑）。”

## ～　まとめ　～

### ・・・　ちょっと違った話をしましょう　・・・

C部長 : “そりゃそうですが・・・。他にも、もっと大規模なハラスメント（↓）があったわけで・・・。”

![imageRL2-8-11](/2022-06-20-QEUR21_SOART6/imageRL2-8-11.jpg)

QEU:FOUNDER ： “**そんなハラスメント性を含んだメディアは見なけりゃ良い**。そして、自分のやれることをがんばる。我々も初心を曲げずに継続してきたからこそ、「ここまで」来たんだから・・・。ハラスメント話はもう飽きた。他の話をしたい・・・。”

![imageRL2-8-12](/2022-06-20-QEUR21_SOART6/imageRL2-8-12.jpg)

C部長 : “またイケメン・バトル？”

QEU:FOUNDER ： “どうせ後でいやほど話をするので、一息しましょう。今回のアジェンダはコレ（↓）・・・。”

[![MOVIE1](http://img.youtube.com/vi/h4Oe1ifauPU/0.jpg)](http://www.youtube.com/watch?v=h4Oe1ifauPU "'It's Not Too Late': Harnessing the power of wind energy")

D先生 : “この前に関係した本を読んでいたらしいじゃないですか？”

![imageRL2-8-13](/2022-06-20-QEUR21_SOART6/imageRL2-8-13.jpg)

QEU:FOUNDER ： “じゃあ、その内容をちょっとだけ紹介するか・・・。”

### - Various estimates put the cost of wind energy between three and twelve cents per kilowatt-hour, de-pending on the location. This is comparable to the cost of fossil energy.(The cost of coal-generated electricity is estimated at four to eight cents per kilowatt-hour.)

### - 風力発電のコストは、場所にもよるが、1キロワット時あたり3〜12セントという試算がある。これは化石エネルギーに匹敵するコストだ（石炭火力発電のコストは1キロワット時あたり4〜8セントと推定される）。

C部長 : “A国でも風力は火力と同じコストなんですか？じゃあ、化石燃料を輸入しているJ国は風力が安くなるのは見え見えじゃないですか？うちにもほしいな・・・。”

QEU:FOUNDER ： “じゃあ、これなんかどう？**垂直式タービン**です。”

![imageRL2-8-14](/2022-06-20-QEUR21_SOART6/imageRL2-8-14.jpg)

D先生 : “海上はともかく、陸上では風向が変わりやすいから垂直式風車は便利ですね。それにしてもきれいだ・・・。”

[![MOVIE2](http://img.youtube.com/vi/9p3uvU_H53w/0.jpg)](http://www.youtube.com/watch?v=9p3uvU_H53w "Why this invention can send wind turbines to the past")

QEU:FOUNDER ： “今は**もっとすごい風力発電機が発明されている**んですよ。”

D先生 : “ダイ〇ンの扇風機みたい・・・（笑）。”

QEU:FOUNDER : “だから、**いろいろな風力発電機を集めて大阪にテーマパークをつくるといいと思う**んですよ・・・（笑）。”
