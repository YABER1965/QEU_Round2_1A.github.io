---
title: QEUR21_RL2048T31:　番外編～AAC（パターンRT法）を使ってみた
date: 2022-06-12
tags: ["QEUシステム", "メトリックス", "Python言語", "強化学習", "RT法", "ディープラーニング", "2048"]
excerpt: Python言語とディープラーニングを使ったゲーム2048の強化学習
---

## QEUR21_RL2048T31:　番外編～AAC（パターンRT法）を使ってみた

## ～　念押し・・・、これは「趣味のコーナー」です　～

D先生 ： “次は、**「パターンRT法」**でやってみるんでしょ？「畳み込みRT法」よりも、ちょっとはマシかなぁ・・・。”

QEU:FOUNDER ： “じゃあ、やってみましょう。それではプログラムをドン！！”

```python
# ----------------
# 2048ゲームの強化学習システム
# step6D : パターンRT法を動的メトリックスとして活用する
# step6D : AAC_convRT_game2048_agent.py
# step6D : AACとパターンRT法で強化学習します
# ---------------- 
# 数値計算のﾗｲﾌﾞﾗﾘ読み込み
import math
import numpy as np
import copy, random, time
import pandas as pd
import collections
from scipy.special import softmax
from IPython.display import clear_output
# -----
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
# -----
import matplotlib.pyplot as plt
#%matplotlib inline
# ---------------- 
# environment
import RT_auto_game_2048_2

# =================================================
# difinition of function
# =================================================
# イレギュラー検出関数(タイル移動後の状態とスコア)
def is_invalid_action(state, a_order):
    # ----------------
    spare = copy.deepcopy(state)  # 状態
    #print("spare",type(spare))
    reward = 0  # 当該行動による報酬
    if a_order == 0:
        for y in range(4):
            z_cnt = 0
            prev = -1
            for x in range(4):
                if spare[y, x] == 0:
                    z_cnt = z_cnt + 1
                elif spare[y, x] != prev:
                    tmp = spare[y, x]
                    spare[y, x] = 0
                    spare[y, x - z_cnt] = tmp
                    prev = tmp
                else:
                    z_cnt = z_cnt + 1
                    spare[y, x - z_cnt] *= 2
                    reward += spare[y, x - z_cnt]
                    spare[y, x] = 0
                    prev = -1
    elif a_order == 1:
        for x in range(4):
            z_cnt = 0
            prev = -1
            for y in range(4):
                if spare[y, x] == 0:
                    z_cnt = z_cnt + 1
                elif spare[y, x] != prev:
                    tmp = spare[y, x]
                    spare[y, x] = 0
                    spare[y - z_cnt, x] = tmp
                    prev = tmp
                else:
                    z_cnt = z_cnt + 1
                    spare[y - z_cnt, x] *= 2
                    reward += spare[y - z_cnt, x]
                    spare[y, x] = 0
                    prev = -1
    elif a_order == 2:
        for y in range(4):
            z_cnt = 0
            prev = -1
            for x in range(4):
                if spare[y, 3 - x] == 0:
                    z_cnt = z_cnt + 1
                elif spare[y, 3 - x] != prev:
                    tmp = spare[y, 3 - x]
                    spare[y, 3 - x] = 0
                    spare[y, 3 - x + z_cnt] = tmp
                    prev = tmp
                else:
                    z_cnt = z_cnt + 1
                    spare[y, 3 - x + z_cnt] *= 2
                    reward += spare[y, 3 - x + z_cnt]
                    spare[y, 3 - x] = 0
                    prev = -1
    elif a_order == 3:
        for x in range(4):
            z_cnt = 0
            prev = -1
            for y in range(4):
                if spare[3 - y, x] == 0:
                    z_cnt = z_cnt + 1
                elif spare[3 - y, x] != prev:
                    tmp = state[3 - y, x]
                    spare[3 - y, x] = 0
                    spare[3 - y + z_cnt, x] = tmp
                    prev = tmp
                else:
                    z_cnt = z_cnt + 1
                    spare[3 - y + z_cnt, x] *= 2
                    reward += spare[3 - y + z_cnt, x]
                    spare[3 - y, x] = 0
                    prev = -1

    if (state - spare).any() == False:
        return True, reward, spare
    else:
        return False, reward, spare

# ---------------------------
# ゲームパターンのCSVファイルを読み込み表示する
def read_gamepattern(file_readcsv): 
 
    # 画像異常検出ファイルの読み込み
    df = pd.read_csv(file_readcsv) 
    max_turn = len(df)
    #print("データ量 max_dfplay",max_dfplay)
    #print(df)
    # --------------------------------------------------
    # 選択項目の読み込み
    #　基本情報-["iplay","batch","name","score","usefile","maxturn","actmatch","perform","epsilon"]
    mx_pattern   = df.loc[:,"p0":"p15"].values
    #print("----- mx_pattern -----")
    #print(mx_pattern)
    
    return mx_pattern

# ---------------------------
# newRTメトリックスを計算する(テンソル活用版)
def calc_newRT(tsr_sig_matrix, tsr_tani_array): 

    # マンハッタン距離のインスタンスを初期化する
    L1_loss = torch.nn.L1Loss()
    # 標準ベクトル(X)と計測ベクトル(Y)
    y = tsr_sig_matrix
    x = tsr_tani_array
    # 回転度の計測(Y1)
    xx = torch.dot(x,x)
    xy = torch.dot(x,y)
    beta = xy/xx
    # ひずみ度の計測(Y2)
    mDistance   = L1_loss(y, beta*x)

    return beta.item(), mDistance.item()

# ----------------
# 移動可能な方向をまとめ、RTメトリックスを生成する
def calc_mxlog2(arr_state):    

    tsr_state = torch.tensor(arr_state).float()
    #print(tsr_state)
    # ---------------------------
    # tsr_spareを対数化する
    for jCol in range(16):
        raw_value = tsr_state[jCol]
        if raw_value > 0:
            tsr_state[jCol] = math.log2(raw_value)
        else:
            tsr_state[jCol] = 0

    return tsr_state

# ----------------
# 移動可能な方向をまとめ、RTメトリックスを生成する
def calc_RTmovables(state, tsr_tani):

    # --------------------------------------------------
    # 配列の初期化
    spare = np.array(state)
    # --------------------------------------------------
    # オリジナル操作方法 -> 数字に変更します(ENVIRONMENT)
    # 数字を上へ -> 1
    # 数字を下へ -> 3
    # 数字を左へ -> 0
    # 数字を右へ -> 2
    # --------------------------------------------------
    a_map, movables, arr_rewards = [0, 1, 2, 3], [], []
    iCnt, arr_states = 0, []
    for i_map in a_map:
        flg_invalid, val_reward, spare_next = is_invalid_action(spare, i_map)
        if flg_invalid == False:
            arr_states.append(spare_next.flatten())
            arr_rewards.append(val_reward + 4)
            movables.append(i_map)
            iCnt = iCnt + 1
    num_avail = iCnt
    
    # --------------------------------------------------
    # newRT法の生成
    # --------------------------------------------------
    # 信号空間
    tsr_signal  = torch.zeros(16).float()
    tsr_signal  = calc_mxlog2(spare_next.flatten())

    # ------------------
    # FEATURE-ENGINEERING(新RTメトリックスを計算する)
    btY1_yarray, Y2_yarray = np.zeros(4), np.zeros(4)
    for iCnt_tani in range(4):
        btY1_yarray[iCnt_tani], Y2_yarray[iCnt_tani] = calc_newRT(tsr_signal, tsr_tani[iCnt_tani])
    
    # ------------------
    # 特徴テンソルを生成する 
    tsr_features  = torch.zeros(state_size).float()     # state_size
    for iCnt in [0,1,2,3]:
        iZero = int(2 * iCnt)
        iPone = int(2 * iCnt + 1)
        tsr_features[iZero] = btY1_yarray[iCnt]
        tsr_features[iPone] = Y2_yarray[iCnt]
    # 結果の出力
    #print("----- tsr_features -----")
    #print(tsr_features)

    return num_avail, movables, arr_rewards, tsr_features

# ----------------
# 命令を発行する
def find_action(ActorCritic, tsr_tani, state, iCnt_turn, iCnt_play):

    # ---------------------------
    # 配列の初期化
    spare = np.array(state)
    # --------------------------------------------------
    # オリジナル操作方法 -> 数字に変更します(ENVIRONMENT)
    # 数字を上へ -> 1
    # 数字を下へ -> 3
    # 数字を左へ -> 0
    # 数字を右へ -> 2
    # --------------------------------------------------
    a_order, gameover, acType = 0, False, "NA"
    state_input = torch.zeros(8).float()
    movables, arr_rewards = [], []

    # --------------------------------------------------
    # 移動可能な方向をまとめ、RTメトリックスを生成する
    if iCnt_turn >= num_maxturns:
        iCnt_pattern = num_maxturns - 1
    else:
        iCnt_pattern = iCnt_turn
    num_avail, movables, arr_rewards, state_input = calc_RTmovables(spare, tsr_tani[iCnt_pattern])
    #print("状態 - state_input:",state_input)    

    # --------------------------------------------------
    # 命令の初期選択
    if (num_avail == 0):
        gameover = True
        log_prob, state_value = 0, 0
        acType = "gameover"
        print("ゲームオーバー:iCnt_turn:{0}, iCnt_play:{1}".format(iCnt_turn, iCnt_play))
    else:
        acType = "machine"
        # --------------------------------------------------
        # generate new 'x' tensor
        #state_input = torch.from_numpy(state_input).float()
        #print("state_input: ", state_input)
        # --------------------------------------------------
        action_probs, state_value = ActorCritic(state_input)
        a_order = torch.multinomial(action_probs, 1).item()  # actionを一つ選ぶ
        log_prob = torch.log(action_probs[a_order])        
        # --------------------------------------------------
        # 命令の妥当性を確認する(ボルツマン行動選択を用いる)
        val_boltz    = 10
        if (movables.count(a_order) == 0):
            acType   = "stucked"
            qs_prob  = softmax(np.array(arr_rewards) / val_boltz)
            a_order  = np.random.choice(movables, p=qs_prob)
        if iCnt_turn%20 == 0:
            print("行動タイプ:{0}, プレイ:{1}, ターン:{2}, 行動選択結果:{3}".format(acType, iCnt_play, iCnt_turn, a_order))

    return a_order, state_input, num_avail, acType, log_prob, state_value

#=================================================
# Calculation class(1) : Actor_Critic_FUNCTION
#=================================================
class ActorCriticModel(nn.Module):
    def __init__(self):
        super(ActorCriticModel, self).__init__()
        self.fc1 = nn.Linear(dim_input, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, 64)
        self.action = nn.Linear(64, 4)
        self.value = nn.Linear(64, 1)
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        action_probs = F.softmax(self.action(x), dim=-1)
        state_values = self.value(x)
        return action_probs, state_values

#=================================================
# Calculation class(2) : Agent
#=================================================
class Agent():
    # -----
    def __init__(self):

        # --------------------------------------------------
        # 記録用パラメタ類(プレイベース)の初期化
        self.arr_iCnt_play  = []  # count game play    プレイ番号
        self.arr_maxturn    = []  # max_turn   ゲームのターン数
        self.arr_csv_play   = []  # name game play    プレイファイル名のリスト
        self.arr_maxscore   = []  # rl_score game play    最終プレイスコア
        # ----------
        # 統計データ(プレイベース)の初期化
        self.loss_actors    = []  # クリティックの損失
        self.loss_critics   = []  # アクターの損失
        # ----------
        # AV値の分析用
        self.arr_num_AV     = []  # AV値のN数
        self.arr_max_AV     = []  # AV値の最大値
        self.arr_q25_AV     = []  # AV値の4分の1値
        self.arr_q75_AV     = []  # AV値の4分の3値
        # ----------
        # 盤面の分析用
        self.arr_yp256      = []  # 256スコアの数
        self.arr_yp512      = []  # 512スコアの数
        self.arr_yp1024     = []  # 1024スコアの数
        self.arr_upright    = []  # 角のスコア（右上）
        self.arr_upleft     = []  # 角のスコア（左上）
        self.arr_downright  = []  # 角のスコア（右下）
        self.arr_downleft   = []  # 角のスコア（左下）
        
        # --------------------------------------------------
        gamepanel       = RT_auto_game_2048_2.Board()
        self.game2048   = RT_auto_game_2048_2.Game(gamepanel)
        self.game2048.gamepanel.gridCell = np.array([[0, 2, 4, 0], [2, 0, 16, 0], [0, 0, 2, 0], [0, 8, 0, 4]])

        # --------------------------------------------------
        ActorCritic = ActorCriticModel()
        self.optimizer = torch.optim.Adam(ActorCritic.parameters(), learning_rate)  # 学習率の大きさは外で定義
        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=10, gamma=0.98)

        # --------------------------------------------------
        # ゲームパターンの単位空間テンソル
        self.tsr_pattern = torch.zeros([num_maxturns,max_ptn_parts,16])
        # ゲームパターン用部品(4種類)を読み込む
        for iPtn in range(max_ptn_parts):    # max_ptn_parts
            # ゲームパターンのCSVファイルを読み込み表示する
            file_ptn_input  = foldername + code_ptn_input[iPtn] + ".csv"  # ファイルパス名の生成 
            mx_pattern      = read_gamepattern(file_ptn_input)
            #print("mx_pattern: ",mx_pattern)
            for iTurn in range(num_maxturns):
                self.tsr_pattern[iTurn, iPtn] = torch.tensor(mx_pattern[iTurn])
        #print("tsr_pattern: ",self.tsr_pattern)

        # --------------------------------------------------
        # ゲーム2048をプレイする
        for iCnt_play in range(num_episodes):       # num_episodes
            # ----------
            num_maxturn, num_maxscore, critic_loss, actor_loss = self.get_episode(ActorCritic, iCnt_play)

            # ----------
            # 記録用パラメタ類(プレイベース)の追加
            self.arr_iCnt_play.append(iCnt_play)  # count game play    プレイ番号
            self.arr_maxturn.append(num_maxturn)  # max_turn   ゲームのターン数
            self.arr_maxscore.append(num_maxscore)  # rl_score game play    最終プレイスコア
            # ----------
            # 統計データ(プレイベース)の追加
            self.loss_actors.append(critic_loss.item())  # アクターの損失
            self.loss_critics.append(actor_loss.item())  # クリティックの損失
            # ----------
            # しばらくすれば表示が消えます
            if iCnt_play%40 == 0:
                time.sleep(SLEEP_TIME)
                clear_output(wait=True)

        # --------------------------------------------------
        # 学習履歴を出力する(1)
        x = list(range(num_episodes))
        # print(x) 
        
        fig = plt.figure(figsize=(12, 6))
        ax1 = fig.add_subplot(1, 2, 1)
        ax1.set_title('learning transition : tile number')
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('number')
        ax1.scatter(x, self.arr_yp256, label="256", color="red")
        ax1.scatter(x, self.arr_yp512, label="512", color="blue")
        ax1.scatter(x, self.arr_yp1024, label="1024", color="orange")
        ax1.legend(loc='best')
        # -----
        ax2 = fig.add_subplot(1, 2, 2)
        ax2.set_title('learning transition : Corner score')
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('number')
        ax2.scatter(x, self.arr_upright, label="upright", color="red")
        ax2.scatter(x, self.arr_upleft, label="upleft", color="blue")
        ax2.scatter(x, self.arr_downright, label="downright", color="orange")
        ax2.scatter(x, self.arr_downleft, label="downleft", color="green")
        ax2.legend(loc='best')
        plt.show()

        # --------------------------------------------------
        # グラフを表示する
        self.show_graph()

        # --------------------------------------------------
        # PyTorchモデルの保存
        #torch.save(ActorCritic.state_dict(), file_output_model)


    # --------------------------------------------------
    def get_episode(self, ActorCritic, iCnt_play):

        # --------------------------------------------------
        acType, reward, done, action  = "random", 0, False, 0
        state  = np.zeros([4, 4])
        action_prev, ttlscore_prev = 0, 0
        iCnt_turn , num_maxturn, num_maxscore = 0, 0, 0
        
        # --------------------------------------------------
        # 記録用パラメタ類(ターンベース)
        self.arr_icount     = []        # カウンタリスト
        self.arr_order      = []        # 指示リスト
        self.arr_avail      = []        # 使用可能な命令数リスト
        self.arr_ttlscore   = []        # ゲームスコアリスト(真数)  
        self.arr_reward     = []        # 報酬リスト(真数)  
        self.arr_acType     = []        # 指示タイプ(random,machine)
        self.arr_predAV     = []        # DL予測されたAV値

        # --------------------------------------------------
        # ゲーム・エピソードを行う
        self.reset()  # 環境をﾘｾｯﾄする
        log_probs, rewards, state_values = [], [], []
        while True:

            # --------------------------------------------------
            # 命令を選択する(1)
            state = self.game2048.gamepanel.gridCell
            action, state_input, num_avail, acType, log_prob, state_value = find_action(ActorCritic, self.tsr_pattern, state, iCnt_turn, iCnt_play)

            # --------------------------------------------------
            # 環境(ENVIRONMENT)を実行する
            state_next, ttlscore, done = self.game2048.link_keys(action)
            #print("状態 - state_input:",state_input)
            # 報酬を計算する
            reward = math.sqrt(ttlscore - ttlscore_prev)
            # タイプ種類が"stucked"では大幅なマイナスの報酬
            if acType == "stucked":
                reward = 0

            # --------------------------------------------------
            # Valueをtensorに変換する
            reward = torch.tensor([reward], device=device)

            # --------------------------------------------------
            # ゲームオーバーの場合の処理
            if done == True:  # ゲームを1回完了後に学習を行う
                returns, Gt, pw = [], 0, 0
                # print(rewards)
                
                for reward in rewards[::-1]:
                    Gt = Gt + (gamma ** pw) * reward  
                    # print(Gt)
                    pw += 1
                    returns.append(Gt)

                returns = returns[::-1]
                returns = torch.cat(returns)
                returns = (returns - returns.mean()) / (returns.std() + 1e-9)
                # print(returns)
                log_probs = torch.cat(log_probs)
                state_values = torch.cat(state_values)
                # print(returns)
                # print(log_probs)
                # print(state_values)

                advantage = returns.detach() - state_values

                critic_loss = F.smooth_l1_loss(state_values, returns.detach())
                actor_loss = (-log_probs * advantage.detach()).mean()
                loss = critic_loss + actor_loss

                # criticを更新する
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()
                #if iCnt_play%10 == 0:
                print('iCnt_play:{0}, maxturn:{1}, critic_loss:{2}, actor_loss:{3}, '.format(iCnt_play, iCnt_turn, critic_loss, actor_loss))
                # --------------------------------------------------  
                if iCnt_turn > 100:
                    self.scheduler.step()
                break
                
            else:
                # --------------------------------------------------
                # データをListに保管する
                log_probs.append(log_prob.view(-1))
                rewards.append(reward)
                state_values.append(state_value)
                # --------------------------------------------------
                # 状態マトリックスの形成
                if iCnt_turn == 0:
                    self.mx_input = [state_input.numpy()]
                else:
                    self.mx_input = np.concatenate([self.mx_input, [state_input.numpy()]], axis=0)
                # --------------------------------------------------  
                #val_predAV = torch.max(state_value)
                val_predAV = np.max(state_value.detach().numpy())
                # --------------------------------------------------    
                self.arr_icount.append(iCnt_turn)       # カウンタリスト
                self.arr_order.append(action)        # 指示リスト
                self.arr_avail.append(num_avail)     # 使用可能な命令数リスト
                self.arr_ttlscore.append(ttlscore)      # ゲームスコアリスト(真数)  
                self.arr_reward.append(reward)       # 報酬リスト(真数)  
                self.arr_acType.append(acType)          # 指示タイプ(random,machine)     
                self.arr_predAV.append(val_predAV)      # DL予測されたAV値
                # --------------------------------------------------  
                ttlscore_prev   = ttlscore
                action_prev     = action
                iCnt_turn       = iCnt_turn + 1
                
        # --------------------------------------------------  
        # AV値の分析用配列群の計算
        self.arr_num_AV.append(len(self.arr_predAV))               # AV値のN数
        self.arr_max_AV.append(np.max(self.arr_predAV))                 # AV値の最大値
        self.arr_q25_AV.append(np.percentile(self.arr_predAV, q=25))    # AV値の4分の1値
        self.arr_q75_AV.append(np.percentile(self.arr_predAV, q=75))    # AV値の4分の3値
        # --------------------------------------------------     
        board_flatten = np.array(state).flatten()
        cc = collections.Counter(board_flatten)
        self.arr_yp256.append(cc[256]) 
        self.arr_yp512.append(cc[512]) 
        self.arr_yp1024.append(cc[1024])  
        # -------------------------------------------------- 
        self.arr_upright.append(state[0][3])    # arr_upright , 角のスコア（右上）
        self.arr_upleft.append(state[0][0])    # arr_upleft , 角のスコア（左上）
        self.arr_downright.append(state[3][3])    # arr_downright , 角のスコア（右下）
        self.arr_downleft.append(state[3][0])    # arr_downleft , 角のスコア（左下）
        # -------------------------------------------------- 
        # プレイリストに引き渡す変数
        num_maxturn     = iCnt_turn
        num_maxscore    = ttlscore

        return num_maxturn, num_maxscore, critic_loss, actor_loss

    # ----------------
    # ﾘｾｯﾄする
    def reset(self):   
    
        state_init = np.array([[0, 2, 4, 0], [2, 0, 16, 0], [0, 0, 2, 0], [0, 8, 0, 4]])
    
        # --------------------------------------------------
        # パラメタの初期化(Panel)
        self.game2048.gamepanel.gridCell=state_init
        self.game2048.gamepanel.compress=False
        self.game2048.gamepanel.merge=False
        self.game2048.gamepanel.moved=False
        self.game2048.gamepanel.score=0
        # ---------------- 
        # パラメタの初期化(Game)
        self.game2048.end=False
        self.game2048.won=False

        # --------------------------------------------------
        # 記録用リストの初期化(ターンベース)
        self.arr_icount     = []        # カウンタリスト
        self.arr_order      = []        # 指示リスト
        self.arr_avail      = []        # 使用可能な命令数リスト
        self.arr_ttlscore   = []        # ゲームスコアリスト(真数)  
        self.arr_reward     = []        # 報酬リスト(真数)  
        self.arr_acType     = []        # 指示タイプ(random,machine)
        self.arr_predAV     = []        # DL予測されたAV値

    # ----------------
    # 学習結果のグラフ化
    def show_graph(self):
        # -----
        x = list(range(num_episodes))
        # print(x)

        fig = plt.figure(figsize=(12, 10))
        # -----
        ax1 = fig.add_subplot(2, 2, 1)
        ax1.set_title('learning transition : loss_actor')
        ax1.set_xlabel('episode')
        ax1.set_ylabel('loss')
        ax1.grid(True)
        ax1.plot(x, self.loss_actors)
        # -----
        ax2 = fig.add_subplot(2, 2, 2)
        ax2.set_title('learning transition : loss_critic')
        ax2.set_xlabel('episode')
        ax2.set_ylabel('loss')
        ax2.grid(True)
        ax2.plot(x, self.loss_critics)
        # -----
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_maxturn).rolling(window=12, center=True).mean()

        ax3 = fig.add_subplot(2, 2, 3)
        ax3.set_title('learning transition : maxturn')
        ax3.set_xlabel('episode')
        ax3.set_ylabel('maxturn')
        ax3.grid(True)
        ax3.plot(x, self.arr_maxturn, label="original", color="blue")
        ax3.plot(x, y_rolling, label="moving", color="red")
        ax3.legend(loc='best')
        # -----
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_maxscore).rolling(window=12, center=True).mean()

        ax4 = fig.add_subplot(2, 2, 4)
        ax4.set_title('learning transition : maxscore')
        ax4.set_xlabel('episode')
        ax4.set_ylabel('maxscore')
        ax4.grid(True)
        ax4.plot(x, self.arr_maxscore, label="original", color="blue")
        ax4.plot(x, y_rolling, label="moving", color="red")
        ax4.legend(loc='best')
        # -----
        fig.tight_layout()
        #fig.savefig("./AAC_img.png")
        plt.show()


#=================================================
# main function            
#=================================================
if __name__ == "__main__":

    # ---------------------------
    # デバイス検出
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    #print(device)

    # ---------------------------
    # DQN Parameter
    state_size  = 8
    action_size = 4
    dim_input   = state_size

    # ---------------------------
    # パラメタ設定
    num_episodes    = 20000          # エピソード数
    learning_rate   = 0.03          # 学習率
    gamma           = 0.99
    SLEEP_TIME      = 0.01          # スリープ間隔

    # ---------------------------
    # フォルダ名の指定
    foldername      = "./ARRAY_RTCNN/"  # My project folder

    # ---------------------------
    # パラメタの設定
    max_ptn_parts   = 4                     # パターンRT部品数
    num_maxturns    = 250                   # パターンRTの最大数
    
    # ---------------------------
    # 入力用：パターンRT部品のCSVファイルを定義する
    nam_ptn_input = "パターンRT部品用CSVデータ" 
    code_ptn_input = ["NA"] * 4 # CSVコードの指定
    code_ptn_input[0] = "train_pattern_pattern0-1" # CSVコードの指定
    code_ptn_input[1] = "train_pattern_pattern2-3" # CSVコードの指定
    code_ptn_input[2] = "train_pattern_pattern4-5" # CSVコードの指定
    code_ptn_input[3] = "train_pattern_pattern6-7" # CSVコードの指定
    print("---- 入力用パターンCSVファイル名 ----")
    print(code_ptn_input)
    print("----------------------")
    
    # ---------------------------
    # 出力用Pytorchモデルのファイル名
    comment_output_model = "game2048_AAC"
    code_output_model = "model_game2048_AAC.pt"
    file_output_model = foldername + code_output_model  # ファイルパス名の生成

    # ---------------------------
    # AAC学習を実行する
    Agent()

```

QEU:FOUNDER ： “そして学習結果をドン・・・。これは、なんと20000回の学習結果です。”

**(主要パフォーマンス)**

![imageRL1-31-1](/2022-06-12-QEUR21_RL2048T31/imageRL1-31-1.jpg)

**(その他のパフォーマンス)**

![imageRL1-31-2](/2022-06-12-QEUR21_RL2048T31/imageRL1-31-2.jpg)

D先生 ： “一瞬だけパフォーマンスが上がりましたが、その後はダメダメですね。なんでまた20000回も学習したんですか？”

QEU:FOUNDER ： “ちょうど半日は、他の事をやることになってたんで、今回の学習期間は長くてもいいだろうと・・・(笑)。やっぱり、AACはゲーム2048に向かないよね。”

D先生 ： “でも、畳み込みRTと比較すると少しだけ進歩していると思います。安定状態でもゲームボードの角部の値が250位まで行きます。角部のスコアのグラフを見ると、以前のDQNでは青色のプロットだけだったんですが、AACでは黄色と緑のプロットがあります。AACの柔軟性を感じます。”

QEU:FOUNDER ： “じゃあ、ついでにAACに強制学習を組み込んでやってみましょう。次回につづく・・・。

## ～　まとめ　～

C部長 : “継続は力なり、イケメン・バトルをやりましょう。”

![imageRL1-31-3](/2022-06-12-QEUR21_RL2048T31/imageRL1-31-3.jpg)

C部長 : “バトルカードの提出をお願いします。”

QEU:FOUNDER ： “ちょっと趣向と変えて・・・。”

[![MOVIE1](http://img.youtube.com/vi/fT-CV4sfdUc/0.jpg)](http://www.youtube.com/watch?v=fT-CV4sfdUc "れいわ躍進を示す政界地図分析〜世界で勢いづく左派ポピュリズム　左右対決から上下対決へ")

D先生 : “さすが某大新聞の出身・・・。よくまとまっています。”

![imageRL1-31-4](/2022-06-12-QEUR21_RL2048T31/imageRL1-31-4.jpg)

C部長 : “う～ん、わかりやすい・・・。他にも、こんな人もいます。”

[![MOVIE1](http://img.youtube.com/vi/oiZ2IoD3PN0/0.jpg)](http://www.youtube.com/watch?v=oiZ2IoD3PN0 "2022.6.10　ウガ金　ウクライナ戦争取材5ヶ月　田中龍作記者に聞く　その２")

D先生 : “う～ん・・・、この人もすごい。しかし、この人がラスボスじゃない？”

[![MOVIE1](http://img.youtube.com/vi/Y_JOoswZUts/0.jpg)](http://www.youtube.com/watch?v=Y_JOoswZUts "何もしない岸田文雄総理。安倍の嘘つき、菅の怖さとも違う。実は二人よりも危険な岸田内閣。消費税増税間違いなし、物価上昇＆賃金下落を許すのか？元朝日新聞・記者佐藤章")

QEU:FOUNDER ： “**やめた人がこのレベルなんだから、本丸に残っている人たちはすごいんだろうなぁ**・・・(棒)。”

C部長 : “たぶん、「神」新聞じゃないんでしょうか・・・。”
