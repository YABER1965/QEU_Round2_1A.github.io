---
title: QEUR21_RL2048T30:　番外編～AAC（畳み込みRT法）で強制学習法を使ってみた
date: 2022-06-10
tags: ["QEUシステム", "メトリックス", "Python言語", "強化学習", "RT法", "ディープラーニング", "2048"]
excerpt: Python言語とディープラーニングを使ったゲーム2048の強化学習
---

## QEUR21_RL2048T30:　番外編～AAC（畳み込みRT法）で強制学習法を使ってみた

## ～　これは「趣味のコーナー」です　～

D先生 ： “あれ？ゲーム2048のシリーズは終わったんじゃなかったんですか？”

QEU:FOUNDER ： “コレは番外編です。以前（すでに消去）の2048強化学習のシリーズの最後の方でAAC(Advantage Actor Critic)を使っていたので、せっかくだからそれも記録として残しておきたい。AACって、ゲーム2048にはあまり向いていないということを承知のうえです。いきなりですが、プログラムをドン！！まずはベースラインとして、畳み込みRT法を使って学習しましょう。”

```python
# ----------------
# 2048ゲームの強化学習システム
# step5A : 畳み込み(RT)を動的メトリックスとして活用する
# step5A : AAC_convRT_game2048_agent.py
# step5A : AACと畳み込みRTでデータを生成します
# ---------------- 
# 数値計算のﾗｲﾌﾞﾗﾘ読み込み
import math
import numpy as np
import copy, random, time
import pandas as pd
import collections
from scipy.special import softmax
from IPython.display import clear_output
# -----
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
# -----
import matplotlib.pyplot as plt
#%matplotlib inline
# ---------------- 
# environment
import RT_auto_game_2048_2

# =================================================
# difinition of function
# =================================================
# イレギュラー検出関数(タイル移動後の状態とスコア)
def is_invalid_action(state, a_order):
    # ----------------
    spare = copy.deepcopy(state)  # 状態
    #print("spare",type(spare))
    reward = 0  # 当該行動による報酬
    if a_order == 0:
        for y in range(4):
            z_cnt = 0
            prev = -1
            for x in range(4):
                if spare[y, x] == 0:
                    z_cnt = z_cnt + 1
                elif spare[y, x] != prev:
                    tmp = spare[y, x]
                    spare[y, x] = 0
                    spare[y, x - z_cnt] = tmp
                    prev = tmp
                else:
                    z_cnt = z_cnt + 1
                    spare[y, x - z_cnt] *= 2
                    reward += spare[y, x - z_cnt]
                    spare[y, x] = 0
                    prev = -1
    elif a_order == 1:
        for x in range(4):
            z_cnt = 0
            prev = -1
            for y in range(4):
                if spare[y, x] == 0:
                    z_cnt = z_cnt + 1
                elif spare[y, x] != prev:
                    tmp = spare[y, x]
                    spare[y, x] = 0
                    spare[y - z_cnt, x] = tmp
                    prev = tmp
                else:
                    z_cnt = z_cnt + 1
                    spare[y - z_cnt, x] *= 2
                    reward += spare[y - z_cnt, x]
                    spare[y, x] = 0
                    prev = -1
    elif a_order == 2:
        for y in range(4):
            z_cnt = 0
            prev = -1
            for x in range(4):
                if spare[y, 3 - x] == 0:
                    z_cnt = z_cnt + 1
                elif spare[y, 3 - x] != prev:
                    tmp = spare[y, 3 - x]
                    spare[y, 3 - x] = 0
                    spare[y, 3 - x + z_cnt] = tmp
                    prev = tmp
                else:
                    z_cnt = z_cnt + 1
                    spare[y, 3 - x + z_cnt] *= 2
                    reward += spare[y, 3 - x + z_cnt]
                    spare[y, 3 - x] = 0
                    prev = -1
    elif a_order == 3:
        for x in range(4):
            z_cnt = 0
            prev = -1
            for y in range(4):
                if spare[3 - y, x] == 0:
                    z_cnt = z_cnt + 1
                elif spare[3 - y, x] != prev:
                    tmp = state[3 - y, x]
                    spare[3 - y, x] = 0
                    spare[3 - y + z_cnt, x] = tmp
                    prev = tmp
                else:
                    z_cnt = z_cnt + 1
                    spare[3 - y + z_cnt, x] *= 2
                    reward += spare[3 - y + z_cnt, x]
                    spare[3 - y, x] = 0
                    prev = -1

    if (state - spare).any() == False:
        return True, reward, spare
    else:
        return False, reward, spare

# ---------------------------
# 畳み込みパターンファイルを読み込み表示する
def read_csvfile(file_readcsv, max_idx, max_col): 
 
    # 画像異常検出ファイルの読み込み
    df = pd.read_csv(file_readcsv, header=None) 
    #print(df)
    # --------------------------------------------------
    # 選択項目(Xs)の読み込み
    mx_Xs = df.iloc[0:max_idx,0:max_col].values
    #print("----- mx_Xs:{0} -----".format(file_readcsv))
    #print(mx_Xs)
  
    return mx_Xs

# ---------------------------
# 畳み込み処理を実施する
def apply_kernel(row, col, kernel, img_tensor):
    return (img_tensor[row:row+max_cnv_idx,col:col+max_cnv_col] * kernel).sum()

# ---------------------------
# newRTメトリックスを計算する(テンソル活用版)
def calc_newRT(len_temp, max_jy_index, tsr_sig_matrix, tsr_tani_array): 

    # ---------------------------
    # 変数の初期化
    L1_loss = torch.nn.L1Loss()
    btY1_yarray, Y2_yarray = [], []

    # 繰り返し
    for i in range(len_temp):

        y = tsr_sig_matrix[i]
        x = tsr_tani_array

        xx = torch.dot(x,x)
        xy = torch.dot(x,y)
        beta = xy/xx
        mDistance   = L1_loss(y, beta*x)
        #print("i:{}, beta:{}".format(i,beta))
        #print("mDistance: ", mDistance.item())
        
        btY1_yarray.append(beta.item())
        Y2_yarray.append(mDistance.item())

    return torch.tensor(btY1_yarray).float(), torch.tensor(Y2_yarray).float()

# ----------------
# 使用可能な配列を提示する
def make_metrics(state, tani_kernel, signal_kernels):
    # ---------------------------
    # テンソルの初期化
    spare = copy.deepcopy(state)
    tsr_spare = torch.tensor(spare).float()
    #print(tsr_spare)
    # ---------------------------
    # tsr_spareを対数化する
    for iRow in range(4):
        for jCol in range(4):
            raw_value = tsr_spare[iRow, jCol]
            # print("make_metrics - raw_value",raw_value)
            if raw_value > 0:
                tsr_spare[iRow, jCol] = math.log2(raw_value)
            else:
                tsr_spare[iRow, jCol] = 0

    # ---------------------------
    # feature-engineeringの計算(新RT法)
    # 単位空間用の畳み込みテンソルを生成する  
    tsr_tani_array = torch.tensor([[apply_kernel(i,j, tani_kernel, tsr_spare) for j in [0,1]] for i in [0,1]]).flatten()
    #print("----- tsr_tani_array -----")
    #print(tsr_tani_array)
    
    # ------------------
    # 信号空間用の畳み込みテンソルを生成する 
    for i in range(len_cmx):
        # ---------------------------
        # CSVファイル(実験)情報を読み込み表示する
        kernel = signal_kernels[i]
        calc_conv = torch.tensor([[apply_kernel(i,j, kernel, tsr_spare) for j in [0,1]] for i in [0,1]]).flatten()
        # -----
        if i == 0:    
            tsr_cvbend1 = calc_conv
        elif i == 1:
            tsr_cvbend2 = calc_conv
        elif i == 2:
            tsr_cvbend3 = calc_conv
        elif i == 3:
            tsr_cvbend4 = calc_conv
        elif i == 4:
            tsr_cvline1 = calc_conv
        elif i == 5:
            tsr_cvline2 = calc_conv
    tsr_sig_matrix = torch.stack([tsr_cvbend1, tsr_cvbend2, tsr_cvbend3, tsr_cvbend4, tsr_cvline1, tsr_cvline2])
    # 結果の出力
    #print(tsr_sig_matrix.shape)     # torch.Size([6, 12, 36])
    #print("----- tsr_sig_matrix[0] -----")
    #print(tsr_sig_matrix[0])
    
    # ---------------------------
    # FEATURE-ENGINEERING(新RTメトリックスを計算する)
    btY1_yarray, Y2_yarray = calc_newRT(len_cmx, cmax_jy_idx, tsr_sig_matrix, tsr_tani_array)
    #print("btY1_yarray:",btY1_yarray)
    #print("Y2_yarray:",Y2_yarray)
    # ---------------------------
    # arr_feature結果の表示
    arr_feature = np.hstack((btY1_yarray.numpy(), Y2_yarray.numpy()))

    return arr_feature

# ----------------
# 命令を発行する
def find_action(ActorCritic, tani_kernel, signal_kernels, state, iCnt_turn, iCnt_play):

    # ---------------------------
    # 配列の初期化
    spare = np.array(state)
    # --------------------------------------------------
    # オリジナル操作方法 -> 数字に変更します(ENVIRONMENT)
    # 数字を上へ -> 1
    # 数字を下へ -> 3
    # 数字を左へ -> 0
    # 数字を右へ -> 2
    # --------------------------------------------------
    a_order, gameover, flg_invalid, val_reward, rl_acType = 0, False, False, 0, "NA"
    a_map = [0, 1, 2, 3]
    state_input = np.zeros(8)
    rl_Avindex, rl_reward_next = [], []
    # --------------------------------------------------
    iCnt = 0
    for i_map in a_map:
        flg_invalid, val_reward, mx_spare_next = is_invalid_action(spare, i_map)
        if flg_invalid == False:
            rl_reward_next.append(val_reward)
            rl_Avindex.append(i_map)
            iCnt = iCnt + 1

    # --------------------------------------------------
    state_input = make_metrics(spare, tani_kernel, signal_kernels)
    #print("状態 - state_input:",state_input)    

    # --------------------------------------------------
    # 命令の初期選択
    num_avail = iCnt
    if (num_avail == 0):
        gameover = True
        log_prob, state_value = 0, 0
        rl_acType = "gameover"
        print("ゲームオーバー:iCnt_turn:{0}, iCnt_play:{1}".format(iCnt_turn, iCnt_play))
    else:
        rl_acType = "machine"
        # --------------------------------------------------
        # generate new 'x' tensor
        x_input_tensor = torch.from_numpy(state_input).float()
        # --------------------------------------------------
        action_probs, state_value = ActorCritic(x_input_tensor)
        a_order = torch.multinomial(action_probs, 1).item()  # actionを一つ選ぶ
        log_prob = torch.log(action_probs[a_order])        
        # --------------------------------------------------
        # 命令の妥当性を確認する(ボルツマン行動選択を用いる)
        val_boltz = 10
        if (rl_Avindex.count(a_order) == 0):
            rl_acType = "stucked"
            qs_prob   = softmax(np.array(rl_reward_next) / val_boltz)
            a_order   = np.random.choice(rl_Avindex, p=qs_prob)
        if iCnt_turn%20 == 0:
            print("行動タイプ:{0}, プレイ:{1}, ターン:{2}, 行動選択結果:{3}".format(rl_acType, iCnt_play, iCnt_turn, a_order))

    return a_order, state_input, num_avail, rl_acType, log_prob, state_value


#=================================================
# Calculation class(1) : Actor_Critic_FUNCTION
#=================================================
class ActorCriticModel(nn.Module):
    def __init__(self):
        super(ActorCriticModel, self).__init__()
        self.fc1 = nn.Linear(dim_input, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, 64)
        self.action = nn.Linear(64, 4)
        self.value = nn.Linear(64, 1)
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        action_probs = F.softmax(self.action(x), dim=-1)
        state_values = self.value(x)
        return action_probs, state_values


#=================================================
# Calculation class(2) : Agent
#=================================================
class Agent():
    # -----
    def __init__(self):

        # --------------------------------------------------
        # 記録用パラメタ類(プレイベース)の初期化
        self.arr_iCnt_play  = []  # count game play    プレイ番号
        self.arr_maxturn    = []  # max_turn   ゲームのターン数
        self.arr_csv_play   = []  # name game play    プレイファイル名のリスト
        self.arr_maxscore   = []  # rl_score game play    最終プレイスコア
        # ----------
        # 統計データ(プレイベース)の初期化
        self.loss_actors    = []  # クリティックの損失
        self.loss_critics   = []  # アクターの損失
        # ----------
        # AV値の分析用
        self.arr_num_AV     = []  # AV値のN数
        self.arr_max_AV     = []  # AV値の最大値
        self.arr_q25_AV     = []  # AV値の4分の1値
        self.arr_q75_AV     = []  # AV値の4分の3値
        # ----------
        # 盤面の分析用
        self.arr_yp256      = []  # 256スコアの数
        self.arr_yp512      = []  # 512スコアの数
        self.arr_yp1024     = []  # 1024スコアの数
        self.arr_upright    = []  # 角のスコア（右上）
        self.arr_upleft     = []  # 角のスコア（左上）
        self.arr_downright  = []  # 角のスコア（右下）
        self.arr_downleft   = []  # 角のスコア（左下）
        
        # --------------------------------------------------
        gamepanel       = RT_auto_game_2048_2.Board()
        self.game2048   = RT_auto_game_2048_2.Game(gamepanel)
        self.game2048.gamepanel.gridCell = np.array([[0, 2, 4, 0], [2, 0, 16, 0], [0, 0, 2, 0], [0, 8, 0, 4]])

        # --------------------------------------------------
        ActorCritic = ActorCriticModel()
        self.optimizer = torch.optim.Adam(ActorCritic.parameters(), learning_rate)  # 学習率の大きさは外で定義
        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=10, gamma=0.98)

        # --------------------------------------------------
        # 畳み込み用部品(8種類)を読み込む
        for i_cnv in range(max_cnv_parts):    # max_cnv_parts
            # -----
            # 畳み込みファイル
            file_cnv_input = foldername + code_cnv_input[i_cnv] + ".csv"  # ファイルパス名の生成 
            mx_conv = read_csvfile(file_cnv_input, max_cnv_idx, max_cnv_col)
            if i_cnv == 0:    
                tsr_bend1 = torch.tensor(mx_conv).float()
            elif i_cnv == 1:
                tsr_bend2 = torch.tensor(mx_conv).float()
            elif i_cnv == 2:
                tsr_bend3 = torch.tensor(mx_conv).float()
            elif i_cnv == 3:
                tsr_bend4 = torch.tensor(mx_conv).float()
            elif i_cnv == 4:
                tsr_line1 = torch.tensor(mx_conv).float()
            elif i_cnv == 5:
                tsr_line2 = torch.tensor(mx_conv).float()
            elif i_cnv == 6:
                tsr_datum1 = torch.tensor(mx_conv).float()
            elif i_cnv == 7:
                tsr_datum2 = torch.tensor(mx_conv).float()

        # --------------------------------------------------
        # 畳み込みカーネルを生成する
        self.signal_kernels  = torch.stack([tsr_bend1, tsr_bend2, tsr_bend3, tsr_bend4, tsr_line1, tsr_line2])
        self.tani_kernel     = tsr_datum1 + tsr_datum2

        # --------------------------------------------------
        # ゲーム2048をプレイする
        for iCnt_play in range(num_episodes):       # num_episodes
            # ----------
            num_maxturn, num_maxscore, critic_loss, actor_loss = self.get_episode(ActorCritic, iCnt_play)

            # ----------
            # 記録用パラメタ類(プレイベース)の追加
            self.arr_iCnt_play.append(iCnt_play)  # count game play    プレイ番号
            self.arr_maxturn.append(num_maxturn)  # max_turn   ゲームのターン数
            self.arr_maxscore.append(num_maxscore)  # rl_score game play    最終プレイスコア
            # ----------
            # 統計データ(プレイベース)の追加
            self.loss_actors.append(critic_loss)  # アクターの損失
            self.loss_critics.append(actor_loss)  # クリティックの損失
            # ----------
            # しばらくすれば表示が消えます
            if iCnt_play%40 == 0:
                time.sleep(SLEEP_TIME)
                clear_output(wait=True)

        # --------------------------------------------------
        # 学習履歴を出力する(1)
        x = list(range(num_episodes))
        # print(x) 
        
        fig = plt.figure(figsize=(12, 6))
        ax1 = fig.add_subplot(1, 2, 1)
        ax1.set_title('learning transition : tile number')
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('number')
        ax1.plot(x, self.arr_yp256, label="256", color="red")
        ax1.plot(x, self.arr_yp512, label="512", color="blue")
        ax1.plot(x, self.arr_yp1024, label="1024", color="orange")
        ax1.legend(loc='best')
        # -----
        ax2 = fig.add_subplot(1, 2, 2)
        ax2.set_title('learning transition : Corner score')
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('number')
        ax2.plot(x, self.arr_upright, label="upright", color="red")
        ax2.plot(x, self.arr_upleft, label="upleft", color="blue")
        ax2.plot(x, self.arr_downright, label="downright", color="orange")
        ax2.plot(x, self.arr_downleft, label="downleft", color="green")
        ax2.legend(loc='best')
        plt.show()

        # --------------------------------------------------
        # グラフを表示する
        self.show_graph()

        # --------------------------------------------------
        # PyTorchモデルの保存
        #torch.save(ActorCritic.state_dict(), file_output_model)

    # --------------------------------------------------
    def get_episode(self, ActorCritic, iCnt_play):

        # --------------------------------------------------
        rl_acType, rl_reward, rl_gameover, rl_action  = "random", 0, False, 0
        rl_state  = np.zeros([4, 4])
        rl_action_prev, rl_ttlscore_prev = 0, 0
        iCnt_turn , num_maxturn, num_maxscore = 0, 0, 0
        
        # --------------------------------------------------
        # 記録用パラメタ類(ターンベース)
        self.arr_icount     = []        # カウンタリスト
        self.arr_order      = []        # 指示リスト
        self.arr_avail      = []        # 使用可能な命令数リスト
        self.arr_ttlscore   = []        # ゲームスコアリスト(真数)  
        self.arr_reward     = []        # 報酬リスト(真数)  
        self.arr_acType     = []        # 指示タイプ(random,machine)
        self.arr_predAV     = []        # DL予測されたAV値

        # --------------------------------------------------
        # ゲーム・エピソードを行う
        self.reset()  # 環境をﾘｾｯﾄする
        log_probs, rewards, state_values = [], [], []
        while True:

            # --------------------------------------------------
            # 命令を選択する(1)
            rl_state = self.game2048.gamepanel.gridCell
            rl_action, state_input, rl_num_avail, rl_acType, log_prob, state_value = find_action(ActorCritic, self.tani_kernel, self.signal_kernels, rl_state, iCnt_turn, iCnt_play)

            # --------------------------------------------------
            # 環境(ENVIRONMENT)を実行する
            rl_state_next, rl_ttlscore, rl_gameover = self.game2048.link_keys(rl_action)
            #state_input = make_metrics(rl_state_next, self.mx_conv)
            #print("状態 - state_input:",state_input)
            # 報酬のを計算する
            rl_reward = math.sqrt(rl_ttlscore - rl_ttlscore_prev)
            # タイプ種類がerrorでは大幅なマイナスの報酬
            if rl_acType == "stucked":
                #rl_reward = -1000
                rl_reward = 0

            # --------------------------------------------------
            # Valueをtensorに変換する
            reward = torch.tensor([rl_reward], device=device)

            # --------------------------------------------------
            # ゲームオーバーの場合の処理
            if rl_gameover == True:  # ゲームを1回完了後に学習を行う
                returns, Gt, pw = [], 0, 0
                # print(rewards)
                
                for reward in rewards[::-1]:
                    Gt = Gt + (gamma ** pw) * reward  
                    # print(Gt)
                    pw += 1
                    returns.append(Gt)

                returns = returns[::-1]

                returns = torch.cat(returns)
                returns = (returns - returns.mean()) / (returns.std() + 1e-9)
                # print(returns)
                log_probs = torch.cat(log_probs)
                state_values = torch.cat(state_values)
                # print(returns)
                # print(log_probs)
                # print(state_values)

                advantage = returns.detach() - state_values

                critic_loss = F.smooth_l1_loss(state_values, returns.detach())
                actor_loss = (-log_probs * advantage.detach()).mean()
                loss = critic_loss + actor_loss

                # criticを更新する
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()
                if iCnt_play%10 == 0:
                    print('iCnt_play:{0}, maxturn:{1}, critic_loss:{2}, actor_loss:{3}, '.format(iCnt_play, iCnt_turn, critic_loss, actor_loss))
                # --------------------------------------------------  
                if iCnt_turn > 100:
                    self.scheduler.step()
                break
                
            else:
                # --------------------------------------------------
                # データをListに保管する
                log_probs.append(log_prob.view(-1))
                rewards.append(reward)
                state_values.append(state_value)
                # --------------------------------------------------
                # 状態マトリックスの形成
                if iCnt_turn == 0:
                    self.mx_input = [state_input]
                else:
                    self.mx_input = np.concatenate([self.mx_input, [state_input]], axis=0)
                # --------------------------------------------------  
                #val_predAV = torch.max(state_value)
                val_predAV = np.max(state_value.detach().numpy())
                # --------------------------------------------------    
                self.arr_icount.append(iCnt_turn)       # カウンタリスト
                self.arr_order.append(rl_action)        # 指示リスト
                self.arr_avail.append(rl_num_avail)     # 使用可能な命令数リスト
                self.arr_ttlscore.append(rl_ttlscore)      # ゲームスコアリスト(真数)  
                self.arr_reward.append(rl_reward)       # 報酬リスト(真数)  
                self.arr_acType.append(rl_acType)       # 指示タイプ(random,machine)     
                self.arr_predAV.append(val_predAV)      # DL予測されたAV値
                # --------------------------------------------------  
                rl_ttlscore_prev = rl_ttlscore
                rl_action_prev = rl_action
                iCnt_turn = iCnt_turn + 1
                
        # --------------------------------------------------  
        # AV値の分析用配列群の計算
        self.arr_num_AV.append(len(self.arr_predAV))               # AV値のN数
        self.arr_max_AV.append(np.max(self.arr_predAV))                 # AV値の最大値
        self.arr_q25_AV.append(np.percentile(self.arr_predAV, q=25))    # AV値の4分の1値
        self.arr_q75_AV.append(np.percentile(self.arr_predAV, q=75))    # AV値の4分の3値
        # --------------------------------------------------     
        board_flatten = np.array(rl_state).flatten()
        cc = collections.Counter(board_flatten)
        self.arr_yp256.append(cc[256]) 
        self.arr_yp512.append(cc[512]) 
        self.arr_yp1024.append(cc[1024])  
        # -------------------------------------------------- 
        self.arr_upright.append(rl_state[0][3])    # arr_upright , 角のスコア（右上）
        self.arr_upleft.append(rl_state[0][0])    # arr_upleft , 角のスコア（左上）
        self.arr_downright.append(rl_state[3][3])    # arr_downright , 角のスコア（右下）
        self.arr_downleft.append(rl_state[3][0])    # arr_downleft , 角のスコア（左下）
        # -------------------------------------------------- 
        # プレイリストに引き渡す変数
        num_maxturn     = iCnt_turn
        num_maxscore    = rl_ttlscore

        return num_maxturn, num_maxscore, critic_loss, actor_loss

    # ----------------
    # ﾘｾｯﾄする
    def reset(self):   
    
        rl_state_init = np.array([[0, 2, 4, 0], [2, 0, 16, 0], [0, 0, 2, 0], [0, 8, 0, 4]])
    
        # --------------------------------------------------
        # パラメタの初期化(Panel)
        self.game2048.gamepanel.gridCell=rl_state_init
        self.game2048.gamepanel.compress=False
        self.game2048.gamepanel.merge=False
        self.game2048.gamepanel.moved=False
        self.game2048.gamepanel.score=0
        # ---------------- 
        # パラメタの初期化(Game)
        self.game2048.end=False
        self.game2048.won=False

        # --------------------------------------------------
        # 記録用リストの初期化(ターンベース)
        self.arr_icount     = []        # カウンタリスト
        self.arr_order      = []        # 指示リスト
        self.arr_avail      = []        # 使用可能な命令数リスト
        self.arr_ttlscore   = []        # ゲームスコアリスト(真数)  
        self.arr_reward     = []        # 報酬リスト(真数)  
        self.arr_yzero      = []        # ゼロ・スコアの数
        self.arr_acType     = []        # 指示タイプ(random,machine)
        self.arr_predAV     = []        # DL予測されたAV値

    # ----------------
    # 学習結果のグラフ化
    def show_graph(self):
        # -----
        x = list(range(num_episodes))
        # print(x)

        fig = plt.figure(figsize=(12, 10))
        # -----
        ax1 = fig.add_subplot(2, 2, 1)
        ax1.set_title('learning transition : loss_actor')
        ax1.set_xlabel('episode')
        ax1.set_ylabel('loss')
        ax1.grid(True)
        ax1.plot(x, self.loss_actors)
        # -----
        ax2 = fig.add_subplot(2, 2, 2)
        ax2.set_title('learning transition : loss_critic')
        ax2.set_xlabel('episode')
        ax2.set_ylabel('loss')
        ax2.grid(True)
        ax2.plot(x, self.loss_critics)
        # -----
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_maxturn).rolling(window=12, center=True).mean()

        ax3 = fig.add_subplot(2, 2, 3)
        ax3.set_title('learning transition : maxturn')
        ax3.set_xlabel('episode')
        ax3.set_ylabel('maxturn')
        ax3.grid(True)
        ax3.plot(x, self.arr_maxturn, label="original", color="blue")
        ax3.plot(x, y_rolling, label="moving", color="red")
        ax3.legend(loc='best')
        # -----
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_maxscore).rolling(window=12, center=True).mean()

        ax4 = fig.add_subplot(2, 2, 4)
        ax4.set_title('learning transition : maxscore')
        ax4.set_xlabel('episode')
        ax4.set_ylabel('maxscore')
        ax4.grid(True)
        ax4.plot(x, self.arr_maxscore, label="original", color="blue")
        ax4.plot(x, y_rolling, label="moving", color="red")
        ax4.legend(loc='best')
        # -----
        fig.tight_layout()
        #fig.savefig("./AAC_img.png")
        plt.show()

#=================================================
# main function            
#=================================================
if __name__ == "__main__":

    # ---------------------------
    # デバイス検出
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    #print(device)

    # ---------------------------
    # パラメタ設定
    num_episodes    = 5000          # エピソード数
    learning_rate   = 0.03          # 学習率
    gamma           = 0.99
    SLEEP_TIME      = 0.01          # スリープ間隔

    # ---------------------------
    # パラメタの設定
    max_cnv_parts = 8          # 畳み込み部品数
    # ---------------------------
    max_sp_idx    = 4          # サンプルのidx数
    max_cnv_idx   = 3          # 畳み込み部品のidx数
    max_sp_col    = 4          # サンプルのidx数
    max_cnv_col   = 3          # 畳み込み部品のidx数
    # ---------------------------
    len_cmx       = 6
    cmax_jy_idx   = 4
    dim_input     = 12
    
    # ---------------------------
    # フォルダ名の指定
    foldername    = "./ARRAY_RTCNN/"  # My project folder

    # ---------------------------
    # 入力用：畳み込み部品のCSVファイルを定義する
    nam_cnv_input = "畳み込み部品用CSVデータ" 
    code_cnv_input = ["NA"] * 8 # CSVコードの指定
    code_cnv_input[0] = "bend1_cnv" # CSVコードの指定
    code_cnv_input[1] = "bend2_cnv" # CSVコードの指定
    code_cnv_input[2] = "bend3_cnv" # CSVコードの指定
    code_cnv_input[3] = "bend4_cnv" # CSVコードの指定
    code_cnv_input[4] = "line1_cnv" # CSVコードの指定
    code_cnv_input[5] = "line2_cnv" # CSVコードの指定
    code_cnv_input[6] = "datum1_cnv" # CSVコードの指定
    code_cnv_input[7] = "datum2_cnv" # CSVコードの指定
    print("---- 入力CSVファイル名 ----")
    print(code_cnv_input)
    print("----------------------")

    # --------------------------------------------------
    # ゲームプレイリスト用のCSVファイルを定義する
    nam_csv_playlist    = "ゲームプレイリストのCSVデータ" 
    code_csv_playlist   = "train_playlist_AAC.csv" # CSVコードの指定
    file_csv_playlist   = foldername + code_csv_playlist  # ファイルパス名の生成
    print("------------ 計測データ出力用のCSV名 -------------")   
    print("ゲームプレイリストのCSVファイル名 ：{0}".format(file_csv_playlist))

    # ---------------------------
    # 出力用Pytorchモデルのファイル名
    comment_output_model = "game2048_AAC"
    code_output_model = "model_game2048_AAC.pt"
    file_output_model = foldername + code_output_model  # ファイルパス名の生成

    # ---------------------------
    # AAC学習を実行する
    Agent()

```

QEU:FOUNDER ： “そして学習結果をドン・・・。これは5000回の学習結果です。”

**(主要パフォーマンス)**

![imageRL1-30-1](/2022-06-10-QEUR21_RL2048T30/imageRL1-30-1.jpg)

**(その他のパフォーマンス)**

![imageRL1-30-2](/2022-06-10-QEUR21_RL2048T30/imageRL1-30-2.jpg)

D先生 ： “あれ？以前、同じ実験をしたときよりも、少しだけパフォーマンスが高いです。何か変えました？”

```python

            # 報酬を計算する
            rl_reward = math.sqrt(rl_ttlscore - rl_ttlscore_prev)
            # タイプ種類がerrorでは大幅なマイナスの報酬
            if rl_acType == "stucked":
                rl_reward = 0

```

QEU:FOUNDER ： “報酬のシステムを少し変えました。（ロジックに）平方根が入ってるでしょ？RTメトリックスを状態(state)に使っている場合、います。ゲーム初期と後期で似た値になる可能性があります。すると昔の報酬はゲーム後期の報酬が著しく高くなることがあるので、ゲームの学習がうまく行かないんです。これはゲーム2048に特有の話だと思うよ。”

D先生 ： “以前の実験では**平方根をつかっていない**んですか？”

### 参考ブログ：「パターンRT法」をテクノメトリックスに使用した強化学習

QEU:FOUNDER ： “使ってないです。一連の実験の途中で条件を変えちゃうと訳が分からなくなりますからね。自分でプログラムを変えて、やってみてください。”

D先生 ： “しょうがない・・・。そうそう思いだした。アレ・・・、やってみませんか・・・？AACの性質を利用して、ゲームの初期ではむりやり理想的なゲーム進行を強制的に行わせ、あとで通常の学習に移行するという・・・。”

QEU:FOUNDER ： “D先生、いきなりの提案をありがとう。でも、説明が唐突・・・（笑）。AACという強化学習法はDQNのように「ε-Greedy」を使っていません。その代わり、上図（↑）のACTOR側ではREINFORCEと同じPOLICY GRADIENT法を使用し、CRITIC側では状態価値関数Vを計算して、ACTORにアドバイスを与えます。我々のやることは、率直、これに「黒魔術」を加えたモノなんだけどね・・・（笑）。強制的に正しい行動（ACTION）を与え、CRITICに勝ちパターンを教え込みます。”

![imageRL1-30-4](/2022-06-10-QEUR21_RL2048T30/imageRL1-30-4.jpg)

D先生 ： “CRITIC側の状態価値関数(V)は「状態（STATE）」だけしか見てませんから、ACTORに強制介入をしてもCRITICに影響を与えません。じゃあ、「強制学習」をやってみましょう。プログラムをドン！！”

QEU:FOUNDER ： “（これ以上のプログラムの紹介は）いやです・・・（笑）。これは（読者への）宿題です、自分でやって・・・。これ（↓）は6000回の学習結果です。初めの1000回は強制学習段階であり、その後の5000回で普通の強化学習をしました。”

**(主要パフォーマンス)**

![imageRL1-30-5](/2022-06-10-QEUR21_RL2048T30/imageRL1-30-5.jpg)

**(その他パフォーマンス)**

![imageRL1-30-6](/2022-06-10-QEUR21_RL2048T30/imageRL1-30-6.jpg)

D先生 ： “あらあら・・・、パフォーマンスが学習とともに「尻すぼみ」・・・(笑)。学習とともに、CRITICがベスト戦略を忘れてきたんだ・・・。最後には、普通の（強制学習なし）学習と同じパフォーマンスになっちゃった。”

QEU:FOUNDER ： “まぁ・・・、ここは逆に考えよう。この結果はいいことなんだよ。我々の考え方は間違っていなかった。あとは、**CRITICの状態価値関数Vが強制学習の経験を忘却する前に、ACTORがより良い行動を出すようにしなければならない**。今回のロジック（「畳み込みRTメトリックス」による学習）では忘却に間に合わないのです。”

D先生 ： “これでギブアップ？・・・もともと「番外編」なので、それでもいいけど・・・。”

QEU:FOUNDER ： “「パターンRTメトリックス」でやってみれば、どう・・・？強制学習との親和性は高いと思うよ・・・。”

D先生 ： “じゃあ、やってみましょうか？”

QEU:FOUNDER ： “・・・ということで、次回につづく・・・。”

## ～　まとめ　～

C部長 : “たとえ番外戦でも、QEUはイケメン・バトルを推奨しております。”

![imageRL1-30-7](/2022-06-10-QEUR21_RL2048T30/imageRL1-30-7.jpg)

C部長 : “バトルカードの提出をお願いします。もし、提出がなければボクが出しますよ・・・。”

D先生 : “もちろん、私はこの秘密兵器を出します・・・。”

[![MOVIE1](http://img.youtube.com/vi/MkCZKiJhf04/0.jpg)](http://www.youtube.com/watch?v=MkCZKiJhf04 "高円寺在住者の水道橋博士が、選挙フェスに登場 （選挙フェス 東京・高円寺駅前 20220610）")

QEU:FOUNDER ： “この人(↑)、いままで政治家をやっていなかったのが不思議だ・・・。”

C部長 : “自分の地元でやってる？とてもリラックスしていますね。”

D先生 : “しかし、昔、熱湯風呂に入っていた人とは信じられない・・・。”

[![MOVIE1](http://img.youtube.com/vi/hwfx8c0Uua0/0.jpg)](http://www.youtube.com/watch?v=hwfx8c0Uua0 "ダチョウ倶楽部以前コンプラ無視【熱湯コマーシャル】昭和60年★スーパージョッキー★THEガンバルマンの『熱湯風呂』時代★最後にすごいコメント有り　ＢＰＯってなんだ")

QEU:FOUNDER ： “あと、久々に本家のラッシャー様を見てしまった。ステキ・・・。”
