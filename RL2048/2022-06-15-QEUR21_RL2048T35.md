---
title: QEUR21_RL2048T35:　番外編～AACとREINFORCEを結合する
date: 2022-06-15
tags: ["QEUシステム", "メトリックス", "Python言語", "強化学習", "RT法", "ディープラーニング", "2048"]
excerpt: Python言語とディープラーニングを使ったゲーム2048の強化学習
---

## QEUR21_RL2048T35:　番外編～AACとREINFORCEを結合する

## ～　念押しの念押し・・・、これは「趣味のコーナー」です(ヤケ)　～

QEU:FOUNDER ： “さあて、この長い準備作業を経てやっと「やりたいこと」ができる。**AACとREINFORCEを結合させる**よ。”

![imageRL1-35-1](/2022-06-15-QEUR21_RL2048T35/imageRL1-35-1.jpg)

D先生 ： “えっ？（システムの）結合？まあ・・・、できないことはないですが・・・。”

![imageRL1-35-2](/2022-06-15-QEUR21_RL2048T35/imageRL1-35-2.jpg)

QEU:FOUNDER ： “以前なぜ我々がAACで強制学習をやっているのかについて説明をしました。**ロボットのように「ティーチング」をしたい**んです。今回も同様に強化学習で「それに相当するもの」をやってみたい・・・。”

![imageRL1-35-3](/2022-06-15-QEUR21_RL2048T35/imageRL1-35-3.jpg)

QEU:FOUNDER ： “AACは念のため8000回で再度学習させ、安定したパフォーマンスがでるようにしております。”

![imageRL1-35-4](/2022-06-15-QEUR21_RL2048T35/imageRL1-35-4.jpg)

D先生 ： “REINFORCEが、このAAC学習データ（↑）を引き継いだとすると、いきなりスゴイパフォーマンスが出るんじゃないでしょうか・・・。”

QEU:FOUNDER ： “やってみないとわからない・・・。それでは、**AACのデータをインポートできるREINFORCEプログラム**をドン！！”

```python
# ----------------
# GYM Cartpoleの強化学習システム
# step9 : REINFORCE_with_AAC_cartpole_agent.py
# step9 : REINFORCEとAACを融合させて強化学習します
# ---------------- 
import gym
import numpy as np
import time
import pandas as pd
from collections import deque
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (16, 10)

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical
torch.manual_seed(0)

import base64, io

# For visualization
from gym.wrappers.monitoring import video_recorder
from IPython.display import HTML
from IPython import display
from IPython.display import clear_output
import glob

# ------
#device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
#device
env = gym.make('CartPole-v0')
env.seed(0)
print('observation space:', env.observation_space)
print('action space:', env.action_space)

# ------
# ACモデルを定義する
class ACModel(nn.Module):
    def __init__(self, input_shape, num_actions):
        super(ACModel, self).__init__()
        self.fc1 = nn.Linear(input_shape,hidden_size)
        #self.fc2 = nn.Linear(hidden_size,hidden_size)
        self.fc_pi = nn.Linear(hidden_size,num_actions)
        self.fc_v = nn.Linear(hidden_size,1)

    def v(self, x):
        x = F.relu(self.fc1(x))
        # x = F.relu(self.fc2(x))
        v = self.fc_v(x)
        return v

    def pi(self, x, softmax_dim = 0):
        x = F.relu(self.fc1(x))
        #x = F.relu(self.fc2(x))
        x = self.fc_pi(x)
        prob = F.softmax(x, dim=softmax_dim)
        return prob

# --------
# PGモデルを定義する
class Policy(nn.Module):
    def __init__(self, input_shape=4, num_actions=2, hidden_size=64):
        super(Policy, self).__init__()
        self.fc1 = nn.Linear(input_shape, hidden_size)
        self.fc2 = nn.Linear(hidden_size, num_actions)
        
    def forward(self, state):
        x = F.relu(self.fc1(state))
        x = self.fc2(x)
        # we just consider 1 dimensional probability of action
        return F.softmax(x, dim=1)
    
    def act(self, state):
        state = torch.from_numpy(state).float().unsqueeze(0)
        probs = self.forward(state).cpu()
        model = Categorical(probs)
        action = model.sample()
        return action.item(), model.log_prob(action)

# --------
# アーキテクチャ
input_shape = 4
num_actions = 2
hidden_size = 64

# --------
# parameters
iterations  = 3000
aac_eposodes = 1000
n_rollout   = 100
gamma       = 0.99
print_interval = 10
maxlen      = 500

# --------
def reinforce(policy, optimizer, iterations=1000, n_rollout=100, gamma=0.99, print_interval=10):
    scores_deque = deque(maxlen=100)
    scores  = []
    plosses = []
    for e in range(1, iterations):
        saved_log_probs = []
        rewards = []
        state = env.reset()
        # Collect trajectory
        for t in range(n_rollout):
            if t < aac_eposodes:
                # AACのデータを入力する
                log_prob = modelAC.pi(torch.from_numpy(state).float())
                m = Categorical(log_prob)
                action = m.sample().item()
            else:
                # 本来のREINFORCEのデータを入力する
                action, log_prob = policy.act(state)

            saved_log_probs.append(log_prob)
            state, reward, done, _ = env.step(action)
            rewards.append(reward)
            if done:
                break
        # Calculate total expected reward
        scores_deque.append(sum(rewards))
        scores.append(sum(rewards))
        
        # Recalculate the total reward applying discounted factor
        discounts = [gamma ** i for i in range(len(rewards) + 1)]
        R = sum([a * b for a,b in zip(discounts, rewards)])
        
        # Calculate the loss 
        policy_loss = []
        for log_prob in saved_log_probs:
            # Note that we are using Gradient Ascent, not Descent. So we need to calculate it with negative rewards.
            policy_loss.append(-log_prob * R)
        # After that, we concatenate whole policy loss in 0th dimension
        policy_loss = torch.cat(policy_loss).sum()
        
        # Backpropagation
        optimizer.zero_grad()
        policy_loss.backward()
        optimizer.step()

        # ----------
        # しばらくすれば表示が消えます
        val_ploss = torch.mean(policy_loss).item()
        #print("ploss", val_ploss)
        plosses.append(val_ploss)
        # ----------
        # インターバル表示
        if e % print_interval == 0:
            print('Episode {}\tAverage Score: {:.2f}'.format(e, np.mean(scores_deque)))
        if np.mean(scores_deque) >= 195.0:
            print('Environment solved in {:d} episodes!\tAverage Score: {:.2f}'.format(e - 100, np.mean(scores_deque)))
            break
        # ----------
        # しばらくすれば表示が消えます
        if e % 500 == 0:
            time.sleep(0.01)
            clear_output(wait=True)
    return scores, plosses

# --------
# ACの初期化
modelAC = ACModel(input_shape, num_actions)
# --------
# 学習結果を読み出す
model_path = './model_ActorCritic.pth'
modelAC.load_state_dict(torch.load(model_path))

# --------
# PGの初期化
policy = Policy(input_shape, num_actions, hidden_size)
optimizer = optim.Adam(policy.parameters(), lr=1e-2)
scores, plosses = reinforce(policy, optimizer, iterations, n_rollout, gamma, print_interval)

# --------------------------------------------------
# 学習履歴を出力する
x = list(range(iterations-1))
# print(x) 

# --------
# 結果を表示する
fig = plt.figure(figsize=(12, 8))
y_rolling = pd.Series(scores).rolling(window=12, center=True).mean()
ax1 = fig.add_subplot(1, 2, 1)
ax1.set_title('learning transition : score')
ax1.set_ylabel('Score')
ax1.set_xlabel('Episode #')
ax1.grid(True)
ax1.plot(x, scores, label="original", color="blue")
ax1.plot(x, y_rolling, label="moving", color="red")
ax1.legend(loc='best')
# -----
y_rolling = pd.Series(plosses).rolling(window=12, center=True).mean()
ax2 = fig.add_subplot(1, 2, 2)
ax2.set_title('learning transition : policy loss')
ax2.set_ylabel('Loss')
ax2.set_xlabel('Episode #')
ax2.grid(True)
ax2.plot(x, plosses, label="original", color="blue")
ax2.plot(x, y_rolling, label="moving", color="red")
ax2.legend(loc='best')
# -----
fig.tight_layout()
#fig.savefig("./REINFORCE_img.png")

```

QEU:FOUNDER ： “そして学習結果をドン・・・。D先生、大当たり～。”

**(注意：パフォーマンスの最高は100です)**

![imageRL1-35-5](/2022-06-15-QEUR21_RL2048T35/imageRL1-35-5.jpg)

D先生 ： “うわぁ～、これは見たこともない、すごい「学習直線」・・・。これなら**学習はオフライン（現場以外）で、「高価でパワーのあるコンピュータ」を使ってAACで学習し、オンライン（現場に載せるとき）には「安価で省電力のRaspberryPi（ワンボードPC）」を使ってもらくちんに動くREINFORCEにする**ことも可能ですね。”

QEU:FOUNDER ： “まだやっていないのだが、「AACからREINFORCEへの転移学習」ってできないのかなあ・・・。できれば、もっと簡単になります。”

![imageRL1-35-6](/2022-06-15-QEUR21_RL2048T35/imageRL1-35-6.jpg)

D先生 ： “それができればさらに作業が簡単になりますね。強化学習でロボットを作れないという皆さん・・・。それは思い込みにすぎません。”

QEU:FOUNDER ： “単に我々がテクノメトリックスをうまく使いこなせていないだけです。あ～あ、やっと2048シリーズが終わった・・・。次のシリーズに行こ・・。”

## ～　まとめ　～

C部長 : “さあ選挙も近いぞ！イケメン・バトルをやりましょう。”

![imageRL1-35-7](/2022-06-15-QEUR21_RL2048T35/imageRL1-35-7.jpg)

C部長 : “自分でいきます。ドン、ちょっと地味だけど・・・。”

[![MOVIE1](http://img.youtube.com/vi/jJDGqxs5Htg/0.jpg)](http://www.youtube.com/watch?v=jJDGqxs5Htg "【白熱憲法講義】ウィシュボンさんと学ぶ自民党改憲草案 Vol.2 国防軍")

QEU:FOUNDER ： “そうなんだよねぇ・・・。弁護士って地味なんだよね。派手好きなD先生、いいプロモーションはありませんか？”

[![MOVIE2](http://img.youtube.com/vi/SmwvTQaS6Pw/0.jpg)](http://www.youtube.com/watch?v=SmwvTQaS6Pw "【放送事故】　橋下徹 VS 山本太郎 ～生放送でいきなり喧嘩ｗｗｗ～")

D先生 : “大人気の元弁護士もいます。私のイケメン観察の長い経験によると・・・。やっぱり、大衆の**「見慣れ」**がクリティカルだよね・・・。”

![imageRL1-35-8](/2022-06-15-QEUR21_RL2048T35/imageRL1-35-8.jpg)

C部長 : “つまり、**「お笑いに走れ」**と・・・？”

![imageRL1-35-9](/2022-06-15-QEUR21_RL2048T35/imageRL1-35-9.jpg)

D先生 : “**偉大な大先輩がいる**じゃないですか・・・。”

[![MOVIE3](http://img.youtube.com/vi/khOToJhnywE/0.jpg)](http://www.youtube.com/watch?v=khOToJhnywE "小池百合子、リボンの騎士コスプレで登場！ 『池袋ハロウィンコスプレフェス2016』")

QEU:FOUNDER ： “現代ではコスプレは自分のアイデンティティの表示ですから、Tシャツみたいに・・・。でも、あそこまでやれる人はそうはいないですよ。”

![imageRL1-35-10](/2022-06-15-QEUR21_RL2048T35/imageRL1-35-10.jpg)

C部長 : “すばらしい・・・。”
