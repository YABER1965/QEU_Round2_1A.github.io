---
title: QEUR21_RL2048T32:ã€€ç•ªå¤–ç·¨ï½AACï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³RTæ³•ï¼‰ã§å¼·åˆ¶å­¦ç¿’ã‚’ä½¿ã£ã¦ã¿ãŸ
date: 2022-06-13
tags: ["QEUã‚·ã‚¹ãƒ†ãƒ ", "ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚¹", "Pythonè¨€èª", "å¼·åŒ–å­¦ç¿’", "RTæ³•", "ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°", "2048"]
excerpt: Pythonè¨€èªã¨ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ä½¿ã£ãŸã‚²ãƒ¼ãƒ 2048ã®å¼·åŒ–å­¦ç¿’
---

## QEUR21_RL2048T32:ã€€ç•ªå¤–ç·¨ï½AACï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³RTæ³•ï¼‰ã§å¼·åˆ¶å­¦ç¿’ã‚’ä½¿ã£ã¦ã¿ãŸ

## ï½ã€€å¿µæŠ¼ã—ã®å¿µæŠ¼ã—ãƒ»ãƒ»ãƒ»ã€ã“ã‚Œã¯ã€Œè¶£å‘³ã®ã‚³ãƒ¼ãƒŠãƒ¼ã€ã§ã™ã€€ï½

QEU:FOUNDER ï¼š â€œã¡ã‚‡ã£ã¨ã€å…ˆã«ã€Œèƒ½æ›¸ãã€ã‚’ã‚„ã‚‹ã‚ˆãƒ»ãƒ»ãƒ»ã€‚â€

Då…ˆç”Ÿ ï¼š â€œä»Šå›ã®ã‚·ãƒªãƒ¼ã‚ºã¯FOUNDERã®è¶£å‘³ã®ã‚³ãƒ¼ãƒŠãƒ¼ãªã®ã§ã€ã©ã†ãã”è‡ªç”±ã«ãƒ»ãƒ»ãƒ»ï¼ˆç¬‘ï¼‰ã€‚â€

![imageRL1-32-1](/2022-06-13-QEUR21_RL2048T32/imageRL1-32-1.jpg)

QEU:FOUNDER ï¼š â€œãªãœæˆ‘ã€…ãŒAACã§å¼·åˆ¶å­¦ç¿’ã‚’ã‚„ã£ã¦ã„ã‚‹ã®ã‹ã«ã¤ã„ã¦ã€è©±ã‚’ã—ã¦ãŠããŸã„ã‚“ã§ã™ã€‚æ™®é€šã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å‹ãƒ­ãƒœãƒƒãƒˆã£ã¦ã€ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãŒã€Œãƒ†ã‚£ãƒ¼ãƒãƒ³ã‚°ã€ã‚’ã—ã¾ã™ã‚ˆã­ãƒ»ãƒ»ãƒ»ã€‚å¼·åˆ¶å­¦ç¿’ã‚’é€šã˜ã¦ã€å¼·åŒ–å­¦ç¿’ã§ã€Œãã‚Œã«ç›¸å½“ã™ã‚‹ã‚‚ã®ã€ã‚’ã‚„ã£ã¦ã¿ãŸã„ãƒ»ãƒ»ãƒ»ã€‚â€

Då…ˆç”Ÿ ï¼š â€œå¼·åŒ–å­¦ç¿’ã«ãŠã„ã¦ã€ãƒ­ãƒœãƒƒãƒˆãŒã€Œæ­£ã—ã„æ‰‹é †ã‚’ã™ãã«è¦šãˆã¦ãã‚Œã‚‹ã€ã®ã¯å¤¢ã§ã™ã‚ˆã­ã€‚å¼·åŒ–å­¦ç¿’ã¯å¾®èª¿æ•´ç”¨ã¨ã—ã¦å­¦ç¿’ã—ã¦ãã‚Œã‚‹ã¨ã„ã†ãƒ»ãƒ»ãƒ»ã€‚â€

QEU:FOUNDER ï¼š â€œãã‚“ãªå¤¢ã‚’è¤‡é›‘ãªã‚²ãƒ¼ãƒ 2048ã§ã§ãã‚‹ã®ã‹ãªããƒ»ãƒ»ãƒ»ã€‚ãã‚Œã§ã¯ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’ãƒ‰ãƒ³ï¼ï¼â€

```python
# ----------------
# 2048ã‚²ãƒ¼ãƒ ã®å¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ 
# step7 : ãƒ‘ã‚¿ãƒ¼ãƒ³RTæ³•ã‚’å‹•çš„ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚¹ã¨ã—ã¦æ´»ç”¨ã™ã‚‹
# step7 : AAC_convRT_game2048_forced_agent.py
# step7 : AACã¨ãƒ‘ã‚¿ãƒ¼ãƒ³RTæ³•ã§å¼·åˆ¶å­¦ç¿’å¾Œã‚‚å¼·åŒ–å­¦ç¿’ã—ã¾ã™
# ---------------- 
# æ•°å€¤è¨ˆç®—ã®ï¾—ï½²ï¾Œï¾ï¾—ï¾˜èª­ã¿è¾¼ã¿
import math
import numpy as np
import copy, random, time
import pandas as pd
import collections
from scipy.special import softmax
from IPython.display import clear_output
# -----
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
# -----
import matplotlib.pyplot as plt
#%matplotlib inline
# ---------------- 
# environment
import RT_auto_game_2048_2

# =================================================
# difinition of function
# =================================================
# ã‚¤ãƒ¬ã‚®ãƒ¥ãƒ©ãƒ¼æ¤œå‡ºé–¢æ•°(ã‚¿ã‚¤ãƒ«ç§»å‹•å¾Œã®çŠ¶æ…‹ã¨ã‚¹ã‚³ã‚¢)
def is_invalid_action(state, a_order):
    # ----------------
    spare = copy.deepcopy(state)  # çŠ¶æ…‹
    #print("spare",type(spare))
    reward = 0  # å½“è©²è¡Œå‹•ã«ã‚ˆã‚‹å ±é…¬
    if a_order == 0:
        for y in range(4):
            z_cnt = 0
            prev = -1
            for x in range(4):
                if spare[y, x] == 0:
                    z_cnt = z_cnt + 1
                elif spare[y, x] != prev:
                    tmp = spare[y, x]
                    spare[y, x] = 0
                    spare[y, x - z_cnt] = tmp
                    prev = tmp
                else:
                    z_cnt = z_cnt + 1
                    spare[y, x - z_cnt] *= 2
                    reward += spare[y, x - z_cnt]
                    spare[y, x] = 0
                    prev = -1
    elif a_order == 1:
        for x in range(4):
            z_cnt = 0
            prev = -1
            for y in range(4):
                if spare[y, x] == 0:
                    z_cnt = z_cnt + 1
                elif spare[y, x] != prev:
                    tmp = spare[y, x]
                    spare[y, x] = 0
                    spare[y - z_cnt, x] = tmp
                    prev = tmp
                else:
                    z_cnt = z_cnt + 1
                    spare[y - z_cnt, x] *= 2
                    reward += spare[y - z_cnt, x]
                    spare[y, x] = 0
                    prev = -1
    elif a_order == 2:
        for y in range(4):
            z_cnt = 0
            prev = -1
            for x in range(4):
                if spare[y, 3 - x] == 0:
                    z_cnt = z_cnt + 1
                elif spare[y, 3 - x] != prev:
                    tmp = spare[y, 3 - x]
                    spare[y, 3 - x] = 0
                    spare[y, 3 - x + z_cnt] = tmp
                    prev = tmp
                else:
                    z_cnt = z_cnt + 1
                    spare[y, 3 - x + z_cnt] *= 2
                    reward += spare[y, 3 - x + z_cnt]
                    spare[y, 3 - x] = 0
                    prev = -1
    elif a_order == 3:
        for x in range(4):
            z_cnt = 0
            prev = -1
            for y in range(4):
                if spare[3 - y, x] == 0:
                    z_cnt = z_cnt + 1
                elif spare[3 - y, x] != prev:
                    tmp = state[3 - y, x]
                    spare[3 - y, x] = 0
                    spare[3 - y + z_cnt, x] = tmp
                    prev = tmp
                else:
                    z_cnt = z_cnt + 1
                    spare[3 - y + z_cnt, x] *= 2
                    reward += spare[3 - y + z_cnt, x]
                    spare[3 - y, x] = 0
                    prev = -1

    if (state - spare).any() == False:
        return True, reward, spare
    else:
        return False, reward, spare

# ---------------------------
# ã‚²ãƒ¼ãƒ ãƒ‘ã‚¿ãƒ¼ãƒ³ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿è¡¨ç¤ºã™ã‚‹
def read_gamepattern(file_readcsv): 
 
    # ç”»åƒç•°å¸¸æ¤œå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
    df = pd.read_csv(file_readcsv) 
    max_turn = len(df)
    #print("ãƒ‡ãƒ¼ã‚¿é‡ max_dfplay",max_dfplay)
    #print(df)
    # --------------------------------------------------
    # é¸æŠé …ç›®ã®èª­ã¿è¾¼ã¿
    #ã€€åŸºæœ¬æƒ…å ±-["iplay","batch","name","score","usefile","maxturn","actmatch","perform","epsilon"]
    mx_pattern   = df.loc[:,"p0":"p15"].values
    #print("----- mx_pattern -----")
    #print(mx_pattern)
    
    return mx_pattern

# ---------------------------
# newRTãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—ã™ã‚‹(ãƒ†ãƒ³ã‚½ãƒ«æ´»ç”¨ç‰ˆ)
def calc_newRT(tsr_sig_matrix, tsr_tani_array): 

    # ãƒãƒ³ãƒãƒƒã‚¿ãƒ³è·é›¢ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’åˆæœŸåŒ–ã™ã‚‹
    L1_loss = torch.nn.L1Loss()
    # æ¨™æº–ãƒ™ã‚¯ãƒˆãƒ«(X)ã¨è¨ˆæ¸¬ãƒ™ã‚¯ãƒˆãƒ«(Y)
    y = tsr_sig_matrix
    x = tsr_tani_array
    # å›è»¢åº¦ã®è¨ˆæ¸¬(Y1)
    xx = torch.dot(x,x)
    xy = torch.dot(x,y)
    beta = xy/xx
    # ã²ãšã¿åº¦ã®è¨ˆæ¸¬(Y2)
    mDistance   = L1_loss(y, beta*x)

    return beta.item(), mDistance.item()

# ----------------
# ç§»å‹•å¯èƒ½ãªæ–¹å‘ã‚’ã¾ã¨ã‚ã€RTãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚¹ã‚’ç”Ÿæˆã™ã‚‹
def calc_mxlog2(arr_state):    

    tsr_state = torch.tensor(arr_state).float()
    #print(tsr_state)
    # ---------------------------
    # tsr_spareã‚’å¯¾æ•°åŒ–ã™ã‚‹
    for jCol in range(16):
        raw_value = tsr_state[jCol]
        if raw_value > 0:
            tsr_state[jCol] = math.log2(raw_value)
        else:
            tsr_state[jCol] = 0

    return tsr_state

# ----------------
# ç§»å‹•å¯èƒ½ãªæ–¹å‘ã‚’ã¾ã¨ã‚ã€RTãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚¹ã‚’ç”Ÿæˆã™ã‚‹
def calc_RTmovables(state, tsr_tani):

    # --------------------------------------------------
    # é…åˆ—ã®åˆæœŸåŒ–
    spare = np.array(state)
    # --------------------------------------------------
    # ã‚ªãƒªã‚¸ãƒŠãƒ«æ“ä½œæ–¹æ³• -> æ•°å­—ã«å¤‰æ›´ã—ã¾ã™(ENVIRONMENT)
    # æ•°å­—ã‚’ä¸Šã¸ -> 1
    # æ•°å­—ã‚’ä¸‹ã¸ -> 3
    # æ•°å­—ã‚’å·¦ã¸ -> 0
    # æ•°å­—ã‚’å³ã¸ -> 2
    # --------------------------------------------------
    a_map, movables, arr_rewards = [0, 1, 2, 3], [], []
    iCnt, arr_states = 0, []
    for i_map in a_map:
        flg_invalid, val_reward, spare_next = is_invalid_action(spare, i_map)
        if flg_invalid == False:
            arr_states.append(spare_next.flatten())
            arr_rewards.append(val_reward + 4)
            movables.append(i_map)
            iCnt = iCnt + 1
    num_avail = iCnt
    
    # --------------------------------------------------
    # newRTæ³•ã®ç”Ÿæˆ
    # --------------------------------------------------
    # ä¿¡å·ç©ºé–“
    tsr_signal  = torch.zeros(16).float()
    tsr_signal  = calc_mxlog2(spare_next.flatten())

    # ------------------
    # FEATURE-ENGINEERING(æ–°RTãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—ã™ã‚‹)
    btY1_yarray, Y2_yarray = np.zeros(4), np.zeros(4)
    for iCnt_tani in range(4):
        btY1_yarray[iCnt_tani], Y2_yarray[iCnt_tani] = calc_newRT(tsr_signal, tsr_tani[iCnt_tani])
    
    # ------------------
    # ç‰¹å¾´ãƒ†ãƒ³ã‚½ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ 
    tsr_features  = torch.zeros(state_size).float()     # state_size
    for iCnt in [0,1,2,3]:
        iZero = int(2 * iCnt)
        iPone = int(2 * iCnt + 1)
        tsr_features[iZero] = btY1_yarray[iCnt]
        tsr_features[iPone] = Y2_yarray[iCnt]
    # çµæœã®å‡ºåŠ›
    #print("----- tsr_features -----")
    #print(tsr_features)

    return num_avail, movables, arr_rewards, tsr_features

# ----------------
# å‘½ä»¤ã‚’ç™ºè¡Œã™ã‚‹
def find_action(ActorCritic, tsr_tani, state, iCnt_turn, iCnt_play):

    # ---------------------------
    # é…åˆ—ã®åˆæœŸåŒ–
    spare = np.array(state)
    # --------------------------------------------------
    # ã‚ªãƒªã‚¸ãƒŠãƒ«æ“ä½œæ–¹æ³• -> æ•°å­—ã«å¤‰æ›´ã—ã¾ã™(ENVIRONMENT)
    # æ•°å­—ã‚’ä¸Šã¸ -> 1
    # æ•°å­—ã‚’ä¸‹ã¸ -> 3
    # æ•°å­—ã‚’å·¦ã¸ -> 0
    # æ•°å­—ã‚’å³ã¸ -> 2
    # --------------------------------------------------
    a_order, gameover, acType = 0, False, "NA"
    state_input = torch.zeros(8).float()
    movables, arr_rewards = [], []

    # --------------------------------------------------
    # ç§»å‹•å¯èƒ½ãªæ–¹å‘ã‚’ã¾ã¨ã‚ã€RTãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚¹ã‚’ç”Ÿæˆã™ã‚‹
    if iCnt_turn >= num_maxturns:
        iCnt_pattern = num_maxturns - 1
    else:
        iCnt_pattern = iCnt_turn
    num_avail, movables, arr_rewards, state_input = calc_RTmovables(spare, tsr_tani[iCnt_pattern])
    #print("çŠ¶æ…‹ - state_input:",state_input)    

    # --------------------------------------------------
    # å‘½ä»¤ã®åˆæœŸé¸æŠ
    if (num_avail == 0):
        gameover = True
        log_prob, state_value = 0, 0
        acType = "gameover"
        print("ã‚²ãƒ¼ãƒ ã‚ªãƒ¼ãƒãƒ¼:iCnt_turn:{0}, iCnt_play:{1}".format(iCnt_turn, iCnt_play))
    else:
        acType = "machine"

        # --------------------------------------------------
        # generate new 'x' tensor
        #state_input = torch.from_numpy(state_input).float()
        #print("state_input: ", state_input)
        # --------------------------------------------------
        action_probs, state_value = ActorCritic(state_input)
        a_order = torch.multinomial(action_probs, 1).item()  # actionã‚’ä¸€ã¤é¸ã¶
        log_prob = torch.log(action_probs[a_order])   

        # --------------------------------------------------
        # å­¦ç¿’åˆæœŸã®å¼·åˆ¶å­¦ç¿’
        if iCnt_play < max_play_alter:
            acType   = "fixed"
            i_alter = int(iCnt_play%8)
            a_order = mx_alter[i_alter, iCnt_turn]

        # --------------------------------------------------
        # å‘½ä»¤ã®å¦¥å½“æ€§ã‚’ç¢ºèªã™ã‚‹(ãƒœãƒ«ãƒ„ãƒãƒ³è¡Œå‹•é¸æŠã‚’ç”¨ã„ã‚‹)
        val_boltz    = 10
        if (movables.count(a_order) == 0):
            acType   = "stucked"
            qs_prob  = softmax(np.array(arr_rewards) / val_boltz)
            a_order  = np.random.choice(movables, p=qs_prob)
        if iCnt_turn%20 == 0:
            print("è¡Œå‹•ã‚¿ã‚¤ãƒ—:{0}, ãƒ—ãƒ¬ã‚¤:{1}, ã‚¿ãƒ¼ãƒ³:{2}, è¡Œå‹•é¸æŠçµæœ:{3}".format(acType, iCnt_play, iCnt_turn, a_order))

    return a_order, state_input, num_avail, acType, log_prob, state_value

#=================================================
# Calculation class(1) : Actor_Critic_FUNCTION
#=================================================
class ActorCriticModel(nn.Module):
    def __init__(self):
        super(ActorCriticModel, self).__init__()
        self.fc1 = nn.Linear(dim_input, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, 64)
        self.action = nn.Linear(64, 4)
        self.value = nn.Linear(64, 1)
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        action_probs = F.softmax(self.action(x), dim=-1)
        state_values = self.value(x)
        return action_probs, state_values

#=================================================
# Calculation class(2) : Agent
#=================================================
class Agent():
    # -----
    def __init__(self):

        # --------------------------------------------------
        # è¨˜éŒ²ç”¨ãƒ‘ãƒ©ãƒ¡ã‚¿é¡(ãƒ—ãƒ¬ã‚¤ãƒ™ãƒ¼ã‚¹)ã®åˆæœŸåŒ–
        self.arr_iCnt_play  = []  # count game play    ãƒ—ãƒ¬ã‚¤ç•ªå·
        self.arr_maxturn    = []  # max_turn   ã‚²ãƒ¼ãƒ ã®ã‚¿ãƒ¼ãƒ³æ•°
        self.arr_csv_play   = []  # name game play    ãƒ—ãƒ¬ã‚¤ãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒªã‚¹ãƒˆ
        self.arr_maxscore   = []  # rl_score game play    æœ€çµ‚ãƒ—ãƒ¬ã‚¤ã‚¹ã‚³ã‚¢
        # ----------
        # çµ±è¨ˆãƒ‡ãƒ¼ã‚¿(ãƒ—ãƒ¬ã‚¤ãƒ™ãƒ¼ã‚¹)ã®åˆæœŸåŒ–
        self.loss_actors    = []  # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ã®æå¤±
        self.loss_critics   = []  # ã‚¢ã‚¯ã‚¿ãƒ¼ã®æå¤±
        # ----------
        # AVå€¤ã®åˆ†æç”¨
        self.arr_num_AV     = []  # AVå€¤ã®Næ•°
        self.arr_max_AV     = []  # AVå€¤ã®æœ€å¤§å€¤
        self.arr_q25_AV     = []  # AVå€¤ã®4åˆ†ã®1å€¤
        self.arr_q75_AV     = []  # AVå€¤ã®4åˆ†ã®3å€¤
        # ----------
        # ç›¤é¢ã®åˆ†æç”¨
        self.arr_yp256      = []  # 256ã‚¹ã‚³ã‚¢ã®æ•°
        self.arr_yp512      = []  # 512ã‚¹ã‚³ã‚¢ã®æ•°
        self.arr_yp1024     = []  # 1024ã‚¹ã‚³ã‚¢ã®æ•°
        self.arr_upright    = []  # è§’ã®ã‚¹ã‚³ã‚¢ï¼ˆå³ä¸Šï¼‰
        self.arr_upleft     = []  # è§’ã®ã‚¹ã‚³ã‚¢ï¼ˆå·¦ä¸Šï¼‰
        self.arr_downright  = []  # è§’ã®ã‚¹ã‚³ã‚¢ï¼ˆå³ä¸‹ï¼‰
        self.arr_downleft   = []  # è§’ã®ã‚¹ã‚³ã‚¢ï¼ˆå·¦ä¸‹ï¼‰
        
        # --------------------------------------------------
        gamepanel       = RT_auto_game_2048_2.Board()
        self.game2048   = RT_auto_game_2048_2.Game(gamepanel)
        self.game2048.gamepanel.gridCell = np.array([[0, 2, 4, 0], [2, 0, 16, 0], [0, 0, 2, 0], [0, 8, 0, 4]])

        # --------------------------------------------------
        ActorCritic = ActorCriticModel()
        self.optimizer = torch.optim.Adam(ActorCritic.parameters(), learning_rate)  # å­¦ç¿’ç‡ã®å¤§ãã•ã¯å¤–ã§å®šç¾©
        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=10, gamma=0.98)

        # --------------------------------------------------
        # ã‚²ãƒ¼ãƒ ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å˜ä½ç©ºé–“ãƒ†ãƒ³ã‚½ãƒ«
        self.tsr_pattern = torch.zeros([num_maxturns,max_ptn_parts,16])
        # ã‚²ãƒ¼ãƒ ãƒ‘ã‚¿ãƒ¼ãƒ³ç”¨éƒ¨å“(4ç¨®é¡)ã‚’èª­ã¿è¾¼ã‚€
        for iPtn in range(max_ptn_parts):    # max_ptn_parts
            # ã‚²ãƒ¼ãƒ ãƒ‘ã‚¿ãƒ¼ãƒ³ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿è¡¨ç¤ºã™ã‚‹
            file_ptn_input  = foldername + code_ptn_input[iPtn] + ".csv"  # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹åã®ç”Ÿæˆ 
            mx_pattern      = read_gamepattern(file_ptn_input)
            #print("mx_pattern: ",mx_pattern)
            for iTurn in range(num_maxturns):
                self.tsr_pattern[iTurn, iPtn] = torch.tensor(mx_pattern[iTurn])
        #print("tsr_pattern: ",self.tsr_pattern)

        # --------------------------------------------------
        # ã‚²ãƒ¼ãƒ 2048ã‚’ãƒ—ãƒ¬ã‚¤ã™ã‚‹
        for iCnt_play in range(num_episodes):       # num_episodes
            # ----------
            num_maxturn, num_maxscore, critic_loss, actor_loss = self.get_episode(ActorCritic, iCnt_play)

            # ----------
            # è¨˜éŒ²ç”¨ãƒ‘ãƒ©ãƒ¡ã‚¿é¡(ãƒ—ãƒ¬ã‚¤ãƒ™ãƒ¼ã‚¹)ã®è¿½åŠ 
            self.arr_iCnt_play.append(iCnt_play)  # count game play    ãƒ—ãƒ¬ã‚¤ç•ªå·
            self.arr_maxturn.append(num_maxturn)  # max_turn   ã‚²ãƒ¼ãƒ ã®ã‚¿ãƒ¼ãƒ³æ•°
            self.arr_maxscore.append(num_maxscore)  # rl_score game play    æœ€çµ‚ãƒ—ãƒ¬ã‚¤ã‚¹ã‚³ã‚¢
            # ----------
            # çµ±è¨ˆãƒ‡ãƒ¼ã‚¿(ãƒ—ãƒ¬ã‚¤ãƒ™ãƒ¼ã‚¹)ã®è¿½åŠ 
            self.loss_actors.append(critic_loss.item())  # ã‚¢ã‚¯ã‚¿ãƒ¼ã®æå¤±
            self.loss_critics.append(actor_loss.item())  # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ã®æå¤±
            # ----------
            # ã—ã°ã‚‰ãã™ã‚Œã°è¡¨ç¤ºãŒæ¶ˆãˆã¾ã™
            if iCnt_play%40 == 0:
                time.sleep(SLEEP_TIME)
                clear_output(wait=True)

        # --------------------------------------------------
        # å­¦ç¿’å±¥æ­´ã‚’å‡ºåŠ›ã™ã‚‹(1)
        x = list(range(num_episodes))
        # print(x) 
        
        fig = plt.figure(figsize=(12, 6))
        ax1 = fig.add_subplot(1, 2, 1)
        ax1.set_title('learning transition : tile number')
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('number')
        ax1.scatter(x, self.arr_yp256, label="256", color="red")
        ax1.scatter(x, self.arr_yp512, label="512", color="blue")
        ax1.scatter(x, self.arr_yp1024, label="1024", color="orange")
        ax1.legend(loc='best')
        # -----
        ax2 = fig.add_subplot(1, 2, 2)
        ax2.set_title('learning transition : Corner score')
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('number')
        ax2.scatter(x, self.arr_upright, label="upright", color="red")
        ax2.scatter(x, self.arr_upleft, label="upleft", color="blue")
        ax2.scatter(x, self.arr_downright, label="downright", color="orange")
        ax2.scatter(x, self.arr_downleft, label="downleft", color="green")
        ax2.legend(loc='best')
        plt.show()

        # --------------------------------------------------
        # ã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤ºã™ã‚‹
        self.show_graph()

        # --------------------------------------------------
        # PyTorchãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
        #torch.save(ActorCritic.state_dict(), file_output_model)


    # --------------------------------------------------
    def get_episode(self, ActorCritic, iCnt_play):

        # --------------------------------------------------
        acType, reward, done, action  = "random", 0, False, 0
        state  = np.zeros([4, 4])
        action_prev, ttlscore_prev = 0, 0
        iCnt_turn , num_maxturn, num_maxscore = 0, 0, 0
        
        # --------------------------------------------------
        # è¨˜éŒ²ç”¨ãƒ‘ãƒ©ãƒ¡ã‚¿é¡(ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹)
        self.arr_icount     = []        # ã‚«ã‚¦ãƒ³ã‚¿ãƒªã‚¹ãƒˆ
        self.arr_order      = []        # æŒ‡ç¤ºãƒªã‚¹ãƒˆ
        self.arr_avail      = []        # ä½¿ç”¨å¯èƒ½ãªå‘½ä»¤æ•°ãƒªã‚¹ãƒˆ
        self.arr_ttlscore   = []        # ã‚²ãƒ¼ãƒ ã‚¹ã‚³ã‚¢ãƒªã‚¹ãƒˆ(çœŸæ•°)  
        self.arr_reward     = []        # å ±é…¬ãƒªã‚¹ãƒˆ(çœŸæ•°)  
        self.arr_acType     = []        # æŒ‡ç¤ºã‚¿ã‚¤ãƒ—(random,machine)
        self.arr_predAV     = []        # DLäºˆæ¸¬ã•ã‚ŒãŸAVå€¤

        # --------------------------------------------------
        # ã‚²ãƒ¼ãƒ ãƒ»ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’è¡Œã†
        self.reset()  # ç’°å¢ƒã‚’ï¾˜ï½¾ï½¯ï¾„ã™ã‚‹
        log_probs, rewards, state_values = [], [], []
        while True:

            # --------------------------------------------------
            # å‘½ä»¤ã‚’é¸æŠã™ã‚‹(1)
            state = self.game2048.gamepanel.gridCell
            action, state_input, num_avail, acType, log_prob, state_value = find_action(ActorCritic, self.tsr_pattern, state, iCnt_turn, iCnt_play)

            # --------------------------------------------------
            # ç’°å¢ƒ(ENVIRONMENT)ã‚’å®Ÿè¡Œã™ã‚‹
            state_next, ttlscore, done = self.game2048.link_keys(action)
            #print("çŠ¶æ…‹ - state_input:",state_input)
            # å ±é…¬ã‚’è¨ˆç®—ã™ã‚‹
            reward = math.sqrt(ttlscore - ttlscore_prev)
            # ã‚¿ã‚¤ãƒ—ç¨®é¡ãŒ"stucked"ã§ã¯å¤§å¹…ãªãƒã‚¤ãƒŠã‚¹ã®å ±é…¬
            if acType == "stucked":
                reward = 0

            # --------------------------------------------------
            # Valueã‚’tensorã«å¤‰æ›ã™ã‚‹
            reward = torch.tensor([reward], device=device)

            # --------------------------------------------------
            # ã‚²ãƒ¼ãƒ ã‚ªãƒ¼ãƒãƒ¼ã®å ´åˆã®å‡¦ç†
            if done == True:  # ã‚²ãƒ¼ãƒ ã‚’1å›å®Œäº†å¾Œã«å­¦ç¿’ã‚’è¡Œã†
                returns, Gt, pw = [], 0, 0
                # print(rewards)
                
                for reward in rewards[::-1]:
                    Gt = Gt + (gamma ** pw) * reward  
                    # print(Gt)
                    pw += 1
                    returns.append(Gt)

                returns = returns[::-1]
                returns = torch.cat(returns)
                returns = (returns - returns.mean()) / (returns.std() + 1e-9)
                # print(returns)
                log_probs = torch.cat(log_probs)
                state_values = torch.cat(state_values)
                # print(returns)
                # print(log_probs)
                # print(state_values)

                advantage = returns.detach() - state_values

                critic_loss = F.smooth_l1_loss(state_values, returns.detach())
                actor_loss = (-log_probs * advantage.detach()).mean()

                # ---------------------------
                # å¼·åˆ¶å­¦ç¿’æ™‚ã¯ã‚¢ã‚¯ã‚¿ãƒ¼æå¤±ã®å½±éŸ¿ã‚’å°ã•ãã™ã‚‹
                if iCnt_play < max_play_alter:
                    loss = critic_loss + 0.01*actor_loss
                else:
                    loss = critic_loss + actor_loss

                # criticã‚’æ›´æ–°ã™ã‚‹
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()
                #if iCnt_play%10 == 0:
                print('iCnt_play:{0}, maxturn:{1}, critic_loss:{2}, actor_loss:{3}, '.format(iCnt_play, iCnt_turn, critic_loss, actor_loss))
                # --------------------------------------------------  
                if iCnt_turn > 100:
                    self.scheduler.step()
                break
                
            else:
                # --------------------------------------------------
                # ãƒ‡ãƒ¼ã‚¿ã‚’Listã«ä¿ç®¡ã™ã‚‹
                log_probs.append(log_prob.view(-1))
                rewards.append(reward)
                state_values.append(state_value)
                # --------------------------------------------------
                # çŠ¶æ…‹ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã®å½¢æˆ
                if iCnt_turn == 0:
                    self.mx_input = [state_input.numpy()]
                else:
                    self.mx_input = np.concatenate([self.mx_input, [state_input.numpy()]], axis=0)
                # --------------------------------------------------  
                #val_predAV = torch.max(state_value)
                val_predAV = np.max(state_value.detach().numpy())
                # --------------------------------------------------    
                self.arr_icount.append(iCnt_turn)       # ã‚«ã‚¦ãƒ³ã‚¿ãƒªã‚¹ãƒˆ
                self.arr_order.append(action)        # æŒ‡ç¤ºãƒªã‚¹ãƒˆ
                self.arr_avail.append(num_avail)     # ä½¿ç”¨å¯èƒ½ãªå‘½ä»¤æ•°ãƒªã‚¹ãƒˆ
                self.arr_ttlscore.append(ttlscore)      # ã‚²ãƒ¼ãƒ ã‚¹ã‚³ã‚¢ãƒªã‚¹ãƒˆ(çœŸæ•°)  
                self.arr_reward.append(reward)       # å ±é…¬ãƒªã‚¹ãƒˆ(çœŸæ•°)  
                self.arr_acType.append(acType)          # æŒ‡ç¤ºã‚¿ã‚¤ãƒ—(random,machine)     
                self.arr_predAV.append(val_predAV)      # DLäºˆæ¸¬ã•ã‚ŒãŸAVå€¤
                # --------------------------------------------------  
                ttlscore_prev   = ttlscore
                action_prev     = action
                iCnt_turn       = iCnt_turn + 1
                
        # --------------------------------------------------  
        # AVå€¤ã®åˆ†æç”¨é…åˆ—ç¾¤ã®è¨ˆç®—
        self.arr_num_AV.append(len(self.arr_predAV))               # AVå€¤ã®Næ•°
        self.arr_max_AV.append(np.max(self.arr_predAV))                 # AVå€¤ã®æœ€å¤§å€¤
        self.arr_q25_AV.append(np.percentile(self.arr_predAV, q=25))    # AVå€¤ã®4åˆ†ã®1å€¤
        self.arr_q75_AV.append(np.percentile(self.arr_predAV, q=75))    # AVå€¤ã®4åˆ†ã®3å€¤
        # --------------------------------------------------     
        board_flatten = np.array(state).flatten()
        cc = collections.Counter(board_flatten)
        self.arr_yp256.append(cc[256]) 
        self.arr_yp512.append(cc[512]) 
        self.arr_yp1024.append(cc[1024])  
        # -------------------------------------------------- 
        self.arr_upright.append(state[0][3])    # arr_upright , è§’ã®ã‚¹ã‚³ã‚¢ï¼ˆå³ä¸Šï¼‰
        self.arr_upleft.append(state[0][0])    # arr_upleft , è§’ã®ã‚¹ã‚³ã‚¢ï¼ˆå·¦ä¸Šï¼‰
        self.arr_downright.append(state[3][3])    # arr_downright , è§’ã®ã‚¹ã‚³ã‚¢ï¼ˆå³ä¸‹ï¼‰
        self.arr_downleft.append(state[3][0])    # arr_downleft , è§’ã®ã‚¹ã‚³ã‚¢ï¼ˆå·¦ä¸‹ï¼‰
        # -------------------------------------------------- 
        # ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã«å¼•ãæ¸¡ã™å¤‰æ•°
        num_maxturn     = iCnt_turn
        num_maxscore    = ttlscore

        return num_maxturn, num_maxscore, critic_loss, actor_loss

    # ----------------
    # ï¾˜ï½¾ï½¯ï¾„ã™ã‚‹
    def reset(self):   
    
        state_init = np.array([[0, 2, 4, 0], [2, 0, 16, 0], [0, 0, 2, 0], [0, 8, 0, 4]])
    
        # --------------------------------------------------
        # ãƒ‘ãƒ©ãƒ¡ã‚¿ã®åˆæœŸåŒ–(Panel)
        self.game2048.gamepanel.gridCell=state_init
        self.game2048.gamepanel.compress=False
        self.game2048.gamepanel.merge=False
        self.game2048.gamepanel.moved=False
        self.game2048.gamepanel.score=0
        # ---------------- 
        # ãƒ‘ãƒ©ãƒ¡ã‚¿ã®åˆæœŸåŒ–(Game)
        self.game2048.end=False
        self.game2048.won=False

        # --------------------------------------------------
        # è¨˜éŒ²ç”¨ãƒªã‚¹ãƒˆã®åˆæœŸåŒ–(ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹)
        self.arr_icount     = []        # ã‚«ã‚¦ãƒ³ã‚¿ãƒªã‚¹ãƒˆ
        self.arr_order      = []        # æŒ‡ç¤ºãƒªã‚¹ãƒˆ
        self.arr_avail      = []        # ä½¿ç”¨å¯èƒ½ãªå‘½ä»¤æ•°ãƒªã‚¹ãƒˆ
        self.arr_ttlscore   = []        # ã‚²ãƒ¼ãƒ ã‚¹ã‚³ã‚¢ãƒªã‚¹ãƒˆ(çœŸæ•°)  
        self.arr_reward     = []        # å ±é…¬ãƒªã‚¹ãƒˆ(çœŸæ•°)  
        self.arr_acType     = []        # æŒ‡ç¤ºã‚¿ã‚¤ãƒ—(random,machine)
        self.arr_predAV     = []        # DLäºˆæ¸¬ã•ã‚ŒãŸAVå€¤

    # ----------------
    # å­¦ç¿’çµæœã®ã‚°ãƒ©ãƒ•åŒ–
    def show_graph(self):
        # -----
        x = list(range(num_episodes))
        # print(x)

        fig = plt.figure(figsize=(12, 10))
        # -----
        ax1 = fig.add_subplot(2, 2, 1)
        ax1.set_title('learning transition : loss_actor')
        ax1.set_xlabel('episode')
        ax1.set_ylabel('loss')
        ax1.grid(True)
        ax1.plot(x, self.loss_actors)
        # -----
        ax2 = fig.add_subplot(2, 2, 2)
        ax2.set_title('learning transition : loss_critic')
        ax2.set_xlabel('episode')
        ax2.set_ylabel('loss')
        ax2.grid(True)
        ax2.plot(x, self.loss_critics)
        # -----
        # ç§»å‹•å¹³å‡ã‚’ä»˜ã‘ã‚‹
        y_rolling = pd.Series(self.arr_maxturn).rolling(window=12, center=True).mean()

        ax3 = fig.add_subplot(2, 2, 3)
        ax3.set_title('learning transition : maxturn')
        ax3.set_xlabel('episode')
        ax3.set_ylabel('maxturn')
        ax3.grid(True)
        ax3.plot(x, self.arr_maxturn, label="original", color="blue")
        ax3.plot(x, y_rolling, label="moving", color="red")
        ax3.legend(loc='best')
        # -----
        # ç§»å‹•å¹³å‡ã‚’ä»˜ã‘ã‚‹
        y_rolling = pd.Series(self.arr_maxscore).rolling(window=12, center=True).mean()

        ax4 = fig.add_subplot(2, 2, 4)
        ax4.set_title('learning transition : maxscore')
        ax4.set_xlabel('episode')
        ax4.set_ylabel('maxscore')
        ax4.grid(True)
        ax4.plot(x, self.arr_maxscore, label="original", color="blue")
        ax4.plot(x, y_rolling, label="moving", color="red")
        ax4.legend(loc='best')
        # -----
        fig.tight_layout()
        #fig.savefig("./AAC_img.png")
        plt.show()


#=================================================
# main function            
#=================================================
if __name__ == "__main__":

    # ---------------------------
    # ãƒ‡ãƒã‚¤ã‚¹æ¤œå‡º
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    #print(device)

    # ---------------------------
    # DQN Parameter
    state_size  = 8
    action_size = 4
    dim_input   = state_size

    # ---------------------------
    # ãƒ‘ãƒ©ãƒ¡ã‚¿è¨­å®š
    num_episodes    = 2000          # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°
    learning_rate   = 0.03          # å­¦ç¿’ç‡
    gamma           = 0.99
    SLEEP_TIME      = 0.01          # ã‚¹ãƒªãƒ¼ãƒ—é–“éš”

    # ---------------------------
    # ãƒ•ã‚©ãƒ«ãƒ€åã®æŒ‡å®š
    foldername      = "./ARRAY_RTCNN/"  # My project folder

    # ---------------------------
    # å­¦ç¿’åˆæœŸã®å¼·åˆ¶å­¦ç¿’
    max_play_alter = 1000

    # ---------------------------
    # å¼·åˆ¶å­¦ç¿’ç”¨ã®ç¹°ã‚Šè¿”ã—ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹
    # å‘½ä»¤0ï¼ˆå·¦ï¼‰ã¨å‘½ä»¤1ï¼ˆä¸Šï¼‰ã€€â†’ã€€ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå·¦ä¸Šã«åã‚‹
    # å‘½ä»¤0ï¼ˆå·¦ï¼‰ã¨å‘½ä»¤3ï¼ˆä¸‹ï¼‰ã€€â†’ã€€ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå·¦ä¸‹ã«åã‚‹
    # å‘½ä»¤2ï¼ˆå³ï¼‰ã¨å‘½ä»¤1ï¼ˆä¸Šï¼‰ã€€â†’ã€€ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå³ä¸Šã«åã‚‹
    # å‘½ä»¤2ï¼ˆå³ï¼‰ã¨å‘½ä»¤3ï¼ˆä¸‹ï¼‰ã€€â†’ã€€ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå³ä¸‹ã«åã‚‹
    mx_alter = np.zeros([8, 1000])
    for i in range(1000):
        if i%2 == 0:
            mx_alter[0, i] = 0
            mx_alter[1, i] = 1
            mx_alter[2, i] = 0
            mx_alter[3, i] = 3
            mx_alter[4, i] = 2
            mx_alter[5, i] = 1
            mx_alter[6, i] = 2
            mx_alter[7, i] = 3    
        else:
            mx_alter[0, i] = 1
            mx_alter[1, i] = 0
            mx_alter[2, i] = 3
            mx_alter[3, i] = 0
            mx_alter[4, i] = 1
            mx_alter[5, i] = 2
            mx_alter[6, i] = 3
            mx_alter[7, i] = 2
    #print(mx_alter)

    # ---------------------------
    # ãƒ‘ãƒ©ãƒ¡ã‚¿ã®è¨­å®š
    max_ptn_parts   = 4                     # ãƒ‘ã‚¿ãƒ¼ãƒ³RTéƒ¨å“æ•°
    num_maxturns    = 250                   # ãƒ‘ã‚¿ãƒ¼ãƒ³RTã®æœ€å¤§æ•°
    
    # ---------------------------
    # å…¥åŠ›ç”¨ï¼šãƒ‘ã‚¿ãƒ¼ãƒ³RTéƒ¨å“ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å®šç¾©ã™ã‚‹
    nam_ptn_input = "ãƒ‘ã‚¿ãƒ¼ãƒ³RTéƒ¨å“ç”¨CSVãƒ‡ãƒ¼ã‚¿" 
    code_ptn_input = ["NA"] * 4 # CSVã‚³ãƒ¼ãƒ‰ã®æŒ‡å®š
    code_ptn_input[0] = "train_pattern_pattern0-1" # CSVã‚³ãƒ¼ãƒ‰ã®æŒ‡å®š
    code_ptn_input[1] = "train_pattern_pattern2-3" # CSVã‚³ãƒ¼ãƒ‰ã®æŒ‡å®š
    code_ptn_input[2] = "train_pattern_pattern4-5" # CSVã‚³ãƒ¼ãƒ‰ã®æŒ‡å®š
    code_ptn_input[3] = "train_pattern_pattern6-7" # CSVã‚³ãƒ¼ãƒ‰ã®æŒ‡å®š
    print("---- å…¥åŠ›ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³CSVãƒ•ã‚¡ã‚¤ãƒ«å ----")
    print(code_ptn_input)
    print("----------------------")
    
    # ---------------------------
    # å‡ºåŠ›ç”¨Pytorchãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ«å
    comment_output_model = "game2048_AAC"
    code_output_model = "model_game2048_AAC.pt"
    file_output_model = foldername + code_output_model  # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹åã®ç”Ÿæˆ

    # ---------------------------
    # AACå­¦ç¿’ã‚’å®Ÿè¡Œã™ã‚‹
    Agent()

```

QEU:FOUNDER ï¼š â€œãã—ã¦å­¦ç¿’çµæœã‚’ãƒ‰ãƒ³ãƒ»ãƒ»ãƒ»ã€‚è‡ªç™½ã™ã‚‹ã‘ã©ã€å¤±æ•—ã§ã—ãŸãƒ»ãƒ»ãƒ»ã€‚â€

**(ä¸»è¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹)**

![imageRL1-32-2](/2022-06-13-QEUR21_RL2048T32/imageRL1-32-2.jpg)

**(ãã®ä»–ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹)**

![imageRL1-32-3](/2022-06-13-QEUR21_RL2048T32/imageRL1-32-3.jpg)

Då…ˆç”Ÿ ï¼š â€œæ€ã„ã£ãã‚Šã€CRITICSã®çŠ¶æ…‹ä¾¡å€¤é–¢æ•°ï¼ˆVï¼‰ãŒå¿˜å´ã•ã‚Œã¦ã„ã¾ã™ã­ï¼ˆç¬‘ï¼‰ã€‚æ®‹å¿µã§ã™ãŒã€ã“ã‚Œã¯äºˆè¦‹ã—ã¦ã„ãŸã‚“ã§ã€é–‹ç™ºã¯ã“ã“ã¾ã§ã«ã—ã¾ã—ã‚‡ã†ã€‚â€

QEU:FOUNDER ï¼š â€œã‚‚ã£ã¨ã„ã„ã€Œé»’é­”è¡“ã€ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ã‚ã¨ã€æˆ‘ã€…ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã«ãƒã‚°ã‚‚ã‚ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã—ãƒ»ãƒ»ãƒ»ã€‚â€

Då…ˆç”Ÿ ï¼š â€œèˆˆå‘³ãŒã‚ã‚‹ã²ã¨ã¯ã€æˆ‘ã€…ã®æˆ¦ç•¥ã‚’ãƒ’ãƒ³ãƒˆã«è‡ªåˆ†ã§ã¤ã¥ãã‚’ã‚„ã£ã¦ã¿ã¦ãã ã•ã„ã€‚æœ¬éŸ³ã‚’è¨€ãˆã°ã€çŠ¶æ…‹ä¾¡å€¤é–¢æ•°ã‚’æ›´æ–°ã•ã›ãªã‘ã‚Œã°ã„ã„ã‚“ã§ã™ã‚ˆãƒ»ãƒ»ãƒ»ã€‚â€

QEU:FOUNDER ï¼š â€œã“ã‚ŒãŒçŸ¥è­˜å…±æœ‰ã®ã‚ã‚‹ã¹ãå§¿ã§ã™ã­ãƒ»ãƒ»ãƒ»ï¼ˆç¬‘ï¼‰ã€‚ã‚„ã£ã¨ã€2048ã®ã‚·ãƒªãƒ¼ã‚ºãŒçµ‚ã‚ã‚Šã¾ã—ãŸã€‚ãƒã‚¤ãƒ³ã‚¹ã‚¤ãƒ¼ãƒ‘ãƒ¼ã«ã„ãã¾ã—ã‚‡ã†ã€‚â€

## ï½ã€€ã¾ã¨ã‚ã€€ï½

Céƒ¨é•· : â€œã¾ã ã¾ã ç¶™ç¶šã¯åŠ›ãªã‚Šã€ã‚¤ã‚±ãƒ¡ãƒ³ãƒ»ãƒãƒˆãƒ«ã‚’ã‚„ã‚Šã¾ã—ã‚‡ã†ã€‚â€

![imageRL1-32-4](/2022-06-13-QEUR21_RL2048T32/imageRL1-32-4.jpg)

Céƒ¨é•· : â€œè‡ªåˆ†ã§ã„ãã¾ã™ã€‚ãƒ‰ãƒ³ãƒ»ãƒ»ãƒ»ã€‚â€

[![MOVIE1](http://img.youtube.com/vi/AyCpS7jucPI/0.jpg)](http://www.youtube.com/watch?v=AyCpS7jucPI "ã€ğŸŠå¤§é˜ªãƒœãƒ©ãƒ³ãƒ†ã‚£ã‚¢ã‚»ãƒ³ã‚¿ãƒ¼ã‚ªãƒ¼ãƒ—ãƒ³è¨˜å¿µè¡—é ­æ¼”èª¬ä¼šLIVEğŸŠã€‘#ã‚„ã¯ãŸæ„› #ã‚Œã„ã‚æ–°é¸çµ„ æ¬¡æœŸå‚é™¢é¸ å¤§é˜ªåºœé¸æŒ™åŒº å…¬èªå€™è£œäºˆå®šè€…")

QEU:FOUNDER ï¼š â€œï¼ˆæˆé•·ãŒï¼‰æ­¢ã¾ã£ã¨ã‚‹ã‚„ã‚“ã‘ãƒ»ãƒ»ãƒ»ã€‚â€

![imageRL1-32-5](/2022-06-13-QEUR21_RL2048T32/imageRL1-32-5.jpg)

Då…ˆç”Ÿ : â€œæ¬¡ãªã‚‹å¤§çˆ†ç™ºã®ãŸã‚ã«ã€å†…ãªã‚‹ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’è²¯ã‚ã¦ã„ã‚‹ã‚“ã§ã™ãƒ»ãƒ»ãƒ»(å°å£°)ã€‚â€

Céƒ¨é•· : â€œãªã‚“ã§ã€ã“ã†ãªã£ãŸã‚“ã ã‚ã†ãƒ»ãƒ»ãƒ»ã€‚â€

Då…ˆç”Ÿ : â€œ**ã€Œå¼·ã¿ã€ã®å•é¡Œ**ã‹ã‚¡ãƒ»ãƒ»ãƒ»ã€‚äººå£ã¯é–¢è¥¿ã¨ã¯å¤‰ã‚ã‚‰ãªã„ãŒã€ã‚‚ã£ã¨çµŒæ¸ˆãŒå…ƒæ°—ãªTWã¨ã„ã†æ‰€ãŒã‚ã‚‹ã§ã—ã‚‡ï¼Ÿâ€

[![MOVIE1](http://img.youtube.com/vi/3AV6ntFN8Lc/0.jpg)](http://www.youtube.com/watch?v=3AV6ntFN8Lc "ä¸‰æ˜Ÿã€è¯ç‚ºé›£è¿½è¶•ï¼ ASMLæ›å°ç©é›»æ”»1å¥ˆç±³è£½ç¨‹æˆæ™¶ç‰‡æ¥­æ•‘æ˜Ÿï¼ï¼Ÿ-é»ƒä¸–è°ã€57çˆ†æ–°èã€‘")

QEU:FOUNDER ï¼š â€œãã‚ƒãï¼ï¼çµŒæ¸ˆè§£èª¬ãŒã‚¹ãƒ†ã‚­ã«å¼·æ°—ã ã‚ˆã­ã€‚Rå›½ã¯ãƒ˜ãƒªã‚¦ãƒ ã‚’Aå›½ã«ã¯å£²ã‚‰ãªã„ãŒã€TWã«ã¯å£²ã£ã¦ãã‚Œã‚‹ã ã‚ã†ã€‚æˆ‘ã€…ãŒå½¼ã‚‰ã«åŠå°ä½“ã‚’ä¾›çµ¦ã—ã¦ã„ã‚‹ã‚“ã ã‹ã‚‰ãƒ»ãƒ»ãƒ»ã€‚ãã†ã„ãˆã°ã€TWã¯æ•µæ€§å›½å®¶ãƒªã‚¹ãƒˆã«å…¥ã£ã¦ã„ãªã‹ã£ãŸã£ã‘ãƒ»ãƒ»ãƒ»ã€‚â€

![imageRL1-32-6](/2022-06-13-QEUR21_RL2048T32/imageRL1-32-6.jpg)

QEU:FOUNDER ï¼š â€œãƒªã‚¹ãƒˆã«ã€Œå¥‘ä¸¹ã€ã£ã¦æ›¸ã„ã¦ã‚ã‚‹ãƒ»ãƒ»ãƒ»(ç¬‘)ã€‚ã‚„ã£ã±ã‚Šæˆ¦ç•¥ç‰©è³‡ã®ç”Ÿç”£ä¸»å°æ¨©ã‚’æ¡ã£ã¦ã„ã‚‹ã®ã¯å¼·ã„ã‚ˆã­ãƒ»ãƒ»ãƒ»ã€‚â€

Då…ˆç”Ÿ : â€œæ˜”ã¯ã€é–¢è¥¿ã£ã¦åŠå°ä½“ãŒå¼·ã‹ã£ãŸã®ã«ãƒ»ãƒ»ãƒ»ã€‚ãªã‚“ã§2000å¹´ä»£åˆã‚ã«ã‚„ã‚ã¡ã‚ƒã£ãŸã‚“ã ã‚ã†ãƒ»ãƒ»ãƒ»ã€‚â€

![imageRL1-32-7](/2022-06-13-QEUR21_RL2048T32/imageRL1-32-7.jpg)

QEU:FOUNDER : â€œ1990å¹´ä»£æœ«ã€æˆ‘ã‚‰ãŒã‚¨ã‚ºã•ã‚“ã®æœ¬ï¼ˆKAWAXIï¼‰ã«æ›¸ã„ã¦ã‚ã£ãŸã‚ˆã€‚Jå›½ã¯å·¥æ¥­ç”Ÿç”£ã‚’æµ·å¤–ã«ã‚·ãƒ•ãƒˆã•ã›ã€ã‚¤ãƒ³ãƒã‚¦ãƒ³ãƒ‰ã‚’å¼·åŒ–ã™ã¹ãã ã£ã¦ãƒ»ãƒ»ãƒ»ã€‚ãã®é€šã‚Šã«ã—ãŸã‚“ã˜ã‚ƒãªã„ã®ï¼Ÿâ€
