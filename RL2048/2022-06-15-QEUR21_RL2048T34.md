---
title: QEUR21_RL2048T34:　番外編～REINFORCEでCARTPOLE
date: 2022-06-15
tags: ["QEUシステム", "メトリックス", "Python言語", "強化学習", "RT法", "ディープラーニング", "2048"]
excerpt: Python言語とディープラーニングを使ったゲーム2048の強化学習
---

## QEUR21_RL2048T34:　番外編～REINFORCEでCARTPOLE

## ～　念押しの念押し・・・、これは「趣味のコーナー」です（しつこく言うぞ・・・）　～

QEU:FOUNDER ： “つぎはREINFORCEをやります。CARTPOLEで・・・。”

![imageRL1-34-1](/2022-06-15-QEUR21_RL2048T34/imageRL1-34-1.jpg)

D先生 ： “今回のシリーズはFOUNDERの趣味のコーナーなので、どうぞご自由に・・・（笑）。”

![imageRL1-34-2](/2022-06-15-QEUR21_RL2048T34/imageRL1-34-2.jpg)

QEU:FOUNDER ： “REINFORCEって**「最強の強化学習手法」**なんだよね。これを言ったのは、この本(↑)の著者でGxxgle社のロボティックス担当です。動作が軽くて、制約が少なく柔軟です。AACは安定しやすくいい方法ではあるのですが、価値関数を使っている分だけ制約があります。”

D先生 ： “そりやぁ・・・、REINFORCEは**「この状況(STATE)では、このように行動(ACTION)する」**というのをダイレクトに学習するんだから・・・。命令が多い場合には、価値関数を使うと価値関数がとてつもなく複雑になっていきます。”

QEU:FOUNDER ： “その一方で、REINFORCEは学習が収束しにくい。だから、そのままでは簡単な問題しか解けないんだよね・・・。”

D先生 ： “**「そのままでは」**ね・・・。テクノメトリックスを有効に使って、複雑な関数を単純にしたいですよね。”

QEU:FOUNDER ： “もう一つの有効な方法があります。その前に**REINFORCEの特性**を把握しておきたい。プログラムをドン！！”

```python
# ----------------
# GYM Cartpoleの強化学習システム
# step8 : REINFORCE_cartpole_agent.py
# step8 : REINFORCEで強化学習します
# ---------------- 
import gym
import numpy as np
import time
import pandas as pd
from collections import deque
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (16, 10)

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical
torch.manual_seed(0)

import base64, io

# For visualization
from gym.wrappers.monitoring import video_recorder
from IPython.display import HTML
from IPython import display
from IPython.display import clear_output
import glob

# ------
env = gym.make('CartPole-v0')
env.seed(0)
print('observation space:', env.observation_space)
print('action space:', env.action_space)

# --------
# PGモデルを定義する
class Policy(nn.Module):
    def __init__(self, state_size=4, action_size=2, hidden_size=64):
        super(Policy, self).__init__()
        self.fc1 = nn.Linear(state_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, action_size)
        
    def forward(self, state):
        x = F.relu(self.fc1(state))
        x = self.fc2(x)
        # we just consider 1 dimensional probability of action
        return F.softmax(x, dim=1)
    
    def act(self, state):
        state = torch.from_numpy(state).float().unsqueeze(0)
        probs = self.forward(state).cpu()
        model = Categorical(probs)
        action = model.sample()
        return action.item(), model.log_prob(action)

# --------
# parameters
state_size  = 4
action_size = 2
hidden_size = 64

# --------
# parameters
iterations  = 10000
n_rollout   = 100
gamma       = 0.99
print_interval = 10
maxlen      = 500

# --------
def reinforce(policy, optimizer, iterations=1000, n_rollout=100, gamma=0.99, print_interval=10):
    scores_deque = deque(maxlen=100)
    scores  = []
    plosses = []
    for e in range(1, iterations):
        saved_log_probs = []
        rewards = []
        state = env.reset()
        # Collect trajectory
        for t in range(n_rollout):
            # Sample the action from current policy
            action, log_prob = policy.act(state)
            saved_log_probs.append(log_prob)
            state, reward, done, _ = env.step(action)
            rewards.append(reward)
            if done:
                break
        # Calculate total expected reward
        scores_deque.append(sum(rewards))
        scores.append(sum(rewards))
        
        # Recalculate the total reward applying discounted factor
        discounts = [gamma ** i for i in range(len(rewards) + 1)]
        R = sum([a * b for a,b in zip(discounts, rewards)])
        
        # Calculate the loss 
        policy_loss = []
        for log_prob in saved_log_probs:
            # Note that we are using Gradient Ascent, not Descent. So we need to calculate it with negative rewards.
            policy_loss.append(-log_prob * R)
        # After that, we concatenate whole policy loss in 0th dimension
        policy_loss = torch.cat(policy_loss).sum()
        
        # Backpropagation
        optimizer.zero_grad()
        policy_loss.backward()
        optimizer.step()

        # ----------
        # しばらくすれば表示が消えます
        val_ploss = torch.mean(policy_loss).item()
        #print("ploss", val_ploss)
        plosses.append(val_ploss)
        # ----------
        # インターバル表示
        if e % print_interval == 0:
            print('Episode {}\tAverage Score: {:.2f}'.format(e, np.mean(scores_deque)))
        if np.mean(scores_deque) >= 195.0:
            print('Environment solved in {:d} episodes!\tAverage Score: {:.2f}'.format(e - 100, np.mean(scores_deque)))
            break
        # ----------
        # しばらくすれば表示が消えます
        if e % 500 == 0:
            time.sleep(0.01)
            clear_output(wait=True)
    return scores, plosses

# --------
policy = Policy(state_size, action_size, hidden_size)
optimizer = optim.Adam(policy.parameters(), lr=1e-2)
scores, plosses = reinforce(policy, optimizer, iterations, n_rollout, gamma, print_interval)

# --------------------------------------------------
# 学習履歴を出力する
x = list(range(iterations-1))
# print(x) 

# --------
# 結果を表示する
fig = plt.figure(figsize=(12, 8))
y_rolling = pd.Series(scores).rolling(window=12, center=True).mean()
ax1 = fig.add_subplot(1, 2, 1)
ax1.set_title('learning transition : score')
ax1.set_ylabel('Score')
ax1.set_xlabel('Episode #')
ax1.grid(True)
ax1.plot(x, scores, label="original", color="blue")
ax1.plot(x, y_rolling, label="moving", color="red")
ax1.legend(loc='best')
# -----
y_rolling = pd.Series(plosses).rolling(window=12, center=True).mean()
ax2 = fig.add_subplot(1, 2, 2)
ax2.set_title('learning transition : policy loss')
ax2.set_ylabel('Loss')
ax2.set_xlabel('Episode #')
ax2.grid(True)
ax2.plot(x, plosses, label="original", color="blue")
ax2.plot(x, y_rolling, label="moving", color="red")
ax2.legend(loc='best')
# -----
fig.tight_layout()
#fig.savefig("./REINFORCE_img.png")
plt.show()

```

QEU:FOUNDER ： “そして学習結果をドン・・・。”

**(関数定義が32の場合のスコアと損失推移)**

![imageRL1-34-3](/2022-06-15-QEUR21_RL2048T34/imageRL1-34-3.jpg)

D先生 ： “**「関数定義が32」**って、なんでしたっけ・・・。”

```python
# PGモデルを定義する
class Policy(nn.Module):
    def __init__(self, state_size=4, action_size=2, hidden_size=64):
        super(Policy, self).__init__()
        self.fc1 = nn.Linear(state_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, action_size)
        
    def forward(self, state):
        x = F.relu(self.fc1(state))
        x = self.fc2(x)
        # we just consider 1 dimensional probability of action
        return F.softmax(x, dim=1)

```

QEU:FOUNDER ： “ディープラーニングの関数定義で隠れ層サイズ（hidden_size）が32というわけです。このノード数でいえば、接続数は32 x 32 = 1024本になります。それを思えば、10000回ゲームの学習も、この接続本数としてはかならずしも十分とは言えないですね。それを思えば、当然に「hidden_sizeが64ではどうなの？」いう疑問もでてくるわけで・・・。”

**(関数定義が64の場合のスコアと損失推移)**

![imageRL1-34-4](/2022-06-15-QEUR21_RL2048T34/imageRL1-34-4.jpg)

D先生 ： “hidden_sizeが32と64を見比べてみるとなんか矛盾していますね。隠れ層が64はあまりパフォーマンスがよくないが、損失が低くなるんですね。”

QEU:FOUNDER ： “「当てはめ損失が低いのでパフォーマンスが良い」とは言えないわけです。もちろん、隠れ層64の場合でもあと数万回学習すれば同じレベルになれるかも・・・（笑）。でもね、AAC(Advantage Actor Critic)では、64でも3000回程度の学習でもかなり安定したパフォーマンスがでてきます。”

**（AACで学習してみた）**

![imageRL1-34-5](/2022-06-15-QEUR21_RL2048T34/imageRL1-34-5.jpg)

D先生 ： “**AACでは隠れ層を64に大きくさせても、パフォーマンスが良い関数を生成することができる**んですね。”

QEU:FOUNDER ： “だから、**AACとREINFORCEの良いところを融合させたい。**”

## ～　まとめ　～

QEU:FOUNDER ： “見てみて・・・。面白い本を見つけました。”

![imageRL1-34-6](/2022-06-15-QEUR21_RL2048T34/imageRL1-34-6.jpg)

C部長 : “なんか面白そうですね・・・。”

Farms of massive wind turbines, often framed by majestic mountains or with their bases lapped by ocean waves, have become one of the most recognizable images of alternative energy. This book co-vers the history of wind power, today's advanced turbines and wind farms, and the advantages and disadvantages of this fast-growing energy technology. It even presents a substantive but easy-to-understand explanation of how and where winds form. It's a great resource for the study of science and technology, as well as those curious about the possible solutions to the problems arising from the global consumption of fossil fuels.
**風力発電のイメージは、山々や海に囲まれた巨大な風車群に象徴されるように、代替エネルギーとして広く認知されています。本書では、風力発電の歴史、現在の風力発電機や風力発電所、メリット・デメリットなどを解説しており、風力発電が急成長していることを実感できます。さらに、風はどこでどのように発生するのか、本質的な部分までわかりやすく解説しています。科学技術の研究者だけでなく、世界的な化石燃料の消費によって引き起こされる問題の解決策に興味がある方にもお勧めの一冊です。**

QEU:FOUNDER ： “実は薄い本だよ・・・。でもきれいで子供向けの絵本みたい・・・。そういえば、こんな風車を見たことある？”

![imageRL1-34-7](/2022-06-15-QEUR21_RL2048T34/imageRL1-34-7.jpg)

D先生 : “ほう・・・、垂直式の風車ってあるんですね・・・。”

QEU:FOUNDER ： “面白いでしょ？技術的なデータも書いてありました。風車の理論効率って59％です。現在の技術では47％までいけるらしい・・・。”

D先生 : “すごいなあ・・・。利点と欠点ってなにかな。利点は化石燃料を使用しない、欠点は・・・。”

QEU:FOUNDER ： “この本によれば、騒音、美観、鳥さんを〇すなど・・・。・・・でも、**アドバンテージが圧倒的に大きい**よね。・・・じゃなければ、導入しないって・・・。DENMXRKなんかすごいし・・・。”

[![MOVIE1](http://img.youtube.com/vi/UwkVL8A-8t4/0.jpg)](http://www.youtube.com/watch?v=UwkVL8A-8t4 "Denmark's $34BN Energy Islands Could Solve Europe's Power Problem")

D先生 : “こんな夢のような島がなぜJ国にもないのかな・・・。そうだ！！ここに作りましょう！！”

[![MOVIE2](http://img.youtube.com/vi/ZIPEru_n0FI/0.jpg)](http://www.youtube.com/watch?v=ZIPEru_n0FI "「負の遺産」で進む巨大工事　大阪・関西万博の会場「夢洲」 かつては干潟…地盤改良も進む")

QEU:FOUNDER : “とても良いアイデアです（笑）。”
