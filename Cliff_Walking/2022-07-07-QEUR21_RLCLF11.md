---
title: QEUR21_RLCLF11 – 「崖歩き(Cliff_Walking)」でAACを使う (その1)
date: 2022-07-07
tags: ["QEUシステム", "メトリックス", "Python言語", "強化学習", "Cliff_Walking", "ディープラーニング"]
excerpt: Python言語によるCliff_Walkingの強化学習
---

## QEUR21_RLCLF11 – 「崖歩き(Cliff_Walking)」でAACを使う (その1)

## ～　REINFORCEは、もうや～めた！！　～

QEU:FOUNDER ： “さて・・・、次はAACをやってみますか？”

![imageRL3-11-1](/2022-07-07-QEUR22_RLCLF11/imageRL3-11-1.jpg)

D先生 : “あれ？REINFORCEでやらないんですか？”

QEU:FOUNDER ： “もう・・・。収束が悪く、あまりにも扱いづらいので、もうやめヤメ・・・（笑）。AACはシステムは重いが、とりあえずは安定していますからね。”

D先生 : “そりゃまあ、思いっきり安定しますよね・・・。”

QEU:FOUNDER ： “では、プログラムをドン・・・。”

```python
# ----------------
# Cliff WalkingゲームのAAC学習システム
# step11 : より高度のシステム構築
# step11 : ディープラーニングでプレイデータをAACで学習させる
# step11 : AAC_cliffwalking_Simu(original).py
# ---------------- 
# 数値計算のﾗｲﾌﾞﾗﾘ読み込み
#import plotting
# -----
import math
import numpy as np
import random
import itertools
from collections import namedtuple, defaultdict
# -----
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
# -----
import matplotlib.pyplot as plt
import seaborn as sns
#%matplotlib inline

# =================================================
# difinition of function
# =================================================
# 状態の表記を変更(oneHOTへ)
def get_screen(state):
    y_state = torch.Tensor([[state]]).long()
    y_onehot = torch.FloatTensor(1, 48) 
    y_onehot.zero_() # すべて0で埋める
    y_onehot.scatter_(1, y_state, 1) # one-hotを返す
    return y_onehot

# 状態の表記を変更(2桁番号からi,jへ)
def calc_XYcoord(number_st):

    i = int(number_st/COLS + 0.01)
    j = int(number_st - i*COLS + 0.01)

    return int(i), int(j)

# 解析の最後に最適ルートを表示する
def showRoute(states):
    board = np.zeros([4, 12])
    # add cliff marked as -1
    board[3, 1:11] = -1
    for i in range(0, ROWS):
        print('-------------------------------------------------')
        out = '| '
        for j in range(0, COLS):
            token = '0'
            if board[i, j] == -1:
                token = '*'
            if (i, j) in states:
                token = 'R'
            if (i, j) == G:
                token = 'G'
            out += token + ' | '
        print(out)
    print('-------------------------------------------------')

#=================================================
# Game Environment class
#=================================================
# Cliff_Walkingゲームを動かす（環境）
class Cliff:

    def __init__(self):
        self.end = False
        self.pos = S
        self.board = np.zeros([4, 12])
        # add cliff marked as -1
        self.board[3, 1:11] = -1

    def nxtPosition(self, action):
        if action == 0:
            nxtPos = (self.pos[0] - 1, self.pos[1])
        elif action == 2:
            nxtPos = (self.pos[0] + 1, self.pos[1])
        elif action == 3:
            nxtPos = (self.pos[0], self.pos[1] - 1)
        else:
            nxtPos = (self.pos[0], self.pos[1] + 1)
        # check legitimacy
        if nxtPos[0] >= 0 and nxtPos[0] <= 3:
            if nxtPos[1] >= 0 and nxtPos[1] <= 11:
                self.pos = nxtPos
        # -----
        if self.pos == G:
            self.end = True
            #print("Game ends reaching goal")
        if self.board[self.pos] == -1:
            #self.end = True
            self.pos = S
            #print("Game ends falling off cliff and return to Start")
        # -----
        state = 12*self.pos[0] + self.pos[1]

        return self.pos, state, self.end

    def giveReward(self):
        # give reward
        val_reward = -1
        if self.pos == G:
            val_reward = -1
        elif self.board[self.pos] == 0:
            val_reward = -1
        elif self.board[self.pos] == -1:
            val_reward = -100

        return val_reward

#=================================================
# Calculation class(1) : Actor_Critic
#=================================================
class ActorCriticModel(nn.Module):
    def __init__(self):
        super(ActorCriticModel, self).__init__()
        self.fc1 = nn.Linear(48, 64)
        self.fc2 = nn.Linear(64, 32)
        self.action = nn.Linear(32, 4)
        self.value = nn.Linear(32, 1)
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        action_probs = F.softmax(self.action(x), dim=-1)
        state_values = self.value(x)
        return action_probs, state_values

#=================================================
# Calculation class(2) : Agent
#=================================================
class Agent():
    # -----
    def __init__(self):

        # ----------
        self.cliff = Cliff()
        ActorCritic = ActorCriticModel()
        stats = self.trainIters(ActorCritic)

        # グラフを表示する
        self.show_graph(stats)
        
        # ルートを計算する
        self.test_action(ActorCritic)

        # A値マップを計算する
        self.test_value(ActorCritic)

        # ---------------------------
        # モデルの保存
        #torch.save(ActorCritic.state_dict(), file_output_model)

    # -----
    def trainIters(self, ActorCritic):
        optimizer = torch.optim.Adam(ActorCritic.parameters(), learning_rate)  # 学習率の大きさは外で定義
        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.98)

        # reward、length、lossの変化を記録する
        # リストの初期化
        self.arr_episode        = np.zeros(num_episodes + 1)
        self.episode_lengths    = np.zeros(num_episodes + 1)
        self.episode_rewards    = np.zeros(num_episodes + 1)
        self.loss_actors        = np.zeros(num_episodes + 1)       
        self.loss_critics       = np.zeros(num_episodes + 1)

        for i_episode in range(1, num_episodes + 1):
        
            # ゲーム・エピソードを行う
            self.cliff.pos, state = self.reset()  # 環境をﾘｾｯﾄする
            state = get_screen(state)  # stateを为one-hotのtensorに変換する
            # -----
            log_probs = []
            rewards = []
            state_values = []
            for t in itertools.count():
                action_probs, state_value = ActorCritic(state.squeeze(0))  # stateにおける各種actionの確率

                action = torch.multinomial(action_probs, 1).item()  # actionを一つ選ぶ
                log_prob = torch.log(action_probs[action])
                # next position
                self.cliff.pos, next_state, done = self.cliff.nxtPosition(action)
                reward = self.cliff.giveReward()       

                # 統計データを計算する(reward, length)
                self.episode_rewards[i_episode] += reward  # 累計報酬を計算する

                # Valueをtensorに変換する
                reward = torch.tensor([reward], device=device)
                next_state_tensor = get_screen(next_state)

                # データをListに保管する
                log_probs.append(log_prob.view(-1))
                rewards.append(reward)
                state_values.append(state_value)

                # 状態を更新する
                state = next_state_tensor

                if done:  # ゲームを1回完了後に学習を行う
                
                    returns, Gt, pw = [], 0, 0
                    # print(rewards)
                    for reward in rewards[::-1]:
                        # Gt += (gamma ** pw) * reward
                        Gt = Gt + (gamma ** pw) * reward  
                        # print(Gt)
                        pw += 1
                        returns.append(Gt)

                    returns = returns[::-1]

                    returns = torch.cat(returns)
                    returns = (returns - returns.mean()) / (returns.std() + 1e-9)
                    # print(returns)
                    log_probs = torch.cat(log_probs)
                    state_values = torch.cat(state_values)
                    # print(returns)
                    # print(log_probs)
                    # print(state_values)

                    advantage = returns.detach() - state_values

                    critic_loss = F.smooth_l1_loss(state_values, returns.detach())
                    actor_loss = (-log_probs * advantage.detach()).mean()
                    loss = critic_loss + actor_loss

                    # 統計データを計算する(loss_actor, loss_critic)
                    self.arr_episode[i_episode]     = i_episode  # ゲーム番号
                    self.loss_actors[i_episode]     = critic_loss  # クリティックの損失
                    self.loss_critics[i_episode]    = actor_loss  # アクターの損失
                    self.episode_lengths[i_episode] = t  # 繰り返し回数を入力する
                    
                    # criticを更新する
                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()
                    if i_episode%10 == 0:
                        print('Episode: {}, total steps: {}'.format(i_episode, t))

                    if t > 20:
                        scheduler.step()

                    break

    # ----------------
    # ﾘｾｯﾄする
    def reset(self):   
        # ----------
        self.cliff = Cliff()
        return S, 36

    # ----------------
    # 学習結果のグラフ化
    def show_graph(self, stats):

        fig = plt.figure(figsize=(14, 12))
        # -----
        ax1 = fig.add_subplot(2, 2, 1)
        ax1.set_title('learning transition : loss_actor')
        ax1.set_xlabel('episode')
        ax1.set_ylabel('loss')
        ax1.grid(True)
        ax1.plot(self.arr_episode, self.loss_actors)
        # -----
        ax2 = fig.add_subplot(2, 2, 2)
        ax2.set_title('learning transition : loss_critic')
        ax2.set_xlabel('episode')
        ax2.set_ylabel('loss')
        ax2.grid(True)
        ax2.plot(self.arr_episode, self.loss_critics)
        # -----
        ax3 = fig.add_subplot(2, 2, 3)
        ax3.set_title('learning transition : length')
        ax3.set_xlabel('episode')
        ax3.set_ylabel('length')
        ax3.grid(True)
        ax3.plot(self.arr_episode, self.episode_lengths)
        # -----
        ax4 = fig.add_subplot(2, 2, 4)
        ax4.set_title('learning transition : reward')
        ax4.set_xlabel('episode')
        ax4.set_ylabel('reward')
        ax4.grid(True)
        ax4.plot(self.arr_episode, self.episode_rewards)
        # -----
        fig.tight_layout()
        #fig.savefig("./AAC_img.png")
        plt.show()


    # ----------------
    # 結果の検証(1) ACTION
    def test_action(self, ActorCritic):
        # ----------
        self.cliff.pos, state = self.reset()  # 環境をﾘｾｯﾄする

        # ----------
        states = []
        iStep = 0
        while 1:
            # 一回のゲームを始める
            get_state = get_screen(state)  # state値をone-hotのtensorに変換する
            action_probs, state_value = ActorCritic(get_state)  # stateからactionの確率に変換する
            action = torch.multinomial(action_probs, 1).item()  # actionを１つ選び出す
            a_row, a_col = calc_XYcoord(state)
            states.append((a_row, a_col))
            self.cliff.pos, next_state, done = self.cliff.nxtPosition(action)
            state = next_state
            print("iStep:{0}, action:{1}, state:{2}".format(iStep, action, state))
            # ----------
            if done == True or iStep > 100:
                break
            else:
                iStep = iStep + 1
        # ----------
        showRoute(states)

    # ----------------
    # 結果の検証(2) VALUE
    def test_value(self, ActorCritic):

        # ----------
        # 4x12の初期化
        value_data = np.zeros((4, 12))
        for state in range(48):
            # 一回のゲームを始める
            get_state = get_screen(state)  # state値をone-hotのtensorに変換する
            action_probs, state_value = ActorCritic(get_state)  # stateからstate_valueに変換する
            a_row, a_col = calc_XYcoord(state)
            value_data[a_row, a_col] = state_value
        # -----
        # ヒートマップを出力
        sns.heatmap(value_data)
        plt.show()


#=================================================
# main function            
#=================================================
if __name__ == "__main__":

    # ---------------------------
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    #print(device)

    # ---------------------------
    num_episodes    = 3000
    learning_rate   = 0.03
    gamma   = 0.98
    ROWS    = 4
    COLS    = 12
    S   = (3, 0)
    G   = (3, 11)

    # ---------------------------
    # フォルダ名の指定
    foldername = "./"  # My project folder

    # ---------------------------
    # 出力用Pytorchモデルのファイル名
    comment_output_model = "cliffwalking_AAC"
    code_output_model = "model_cliffwalking_AAC.pt"
    file_output_model = foldername + code_output_model  # ファイルパス名の生成

    # ---------------------------
    # 学習の実行
    Agent()

```

QEU:FOUNDER ： “じゃあ、実行結果を見てみましょう。”

**（学習曲線）**

![imageRL3-11-2](/2022-07-07-QEUR22_RLCLF11/imageRL3-11-2.jpg)

**（状態価値V関数の分布）**

![imageRL3-11-3](/2022-07-07-QEUR22_RLCLF11/imageRL3-11-3.jpg)

D先生 : “REINFORCEでうんざりしていたのであれば、AACは異次元の世界でしょう（笑）？”

QEU:FOUNDER ： “もう・・・、超異次元・・・。ここまで安定するのであれば、関数のストラクチャを変えたり、メトリックスを追加しても不安定にならないでしょう。”

D先生 : “ここまで学習収束が安定であれば、この環境でのメトリックスの追加は意味なくありません？”

QEU:FOUNDER ： “もともと、Cliff_Walkingの次はメイズ（2次元）だと思っていたので検証用としてシステムだけを作っておきましょう。。”

## ～　まとめ　～

### ・・・　ぶつぶつブツ・・・　・・・

QEU:FOUNDER ： “本当にあの党は、ブツブツ・・・。”

[![MOVIE1](http://img.youtube.com/vi/pKxf_tqpYjI/0.jpg)](http://www.youtube.com/watch?v=pKxf_tqpYjI "親は子どものお手伝いさん。いつもお手伝いありがとう。とても嬉しいです〜と子どもに言われて親は一人前。あるべき政治の未来について語ります。福島瑞穂社民党党首")

D先生 : “ま～だ、「あのこと」を根にもっているんですか？**彼ら（社〇党）には彼らの「立場」があるだろうし・・・。**”

QEU:FOUNDER ： “その「立場」がJ国をつぶしたんだよ！・・・おっと、過去形はだめだな・・・（笑）。これからデータをお見せましょう。なぜ小生がそう考えるのかが分かるから・・・。さて、最初に直近の状況デス・・・。”

![imageRL3-11-4](/2022-07-07-QEUR22_RLCLF11/imageRL3-11-4.jpg)

D先生 : “とうとうイケメンが「なんとかQ効果」でブレイクしましたね。これで当落ギリギリという・・・。選挙は恐ろしいね・・・。”

QEU:FOUNDER ： “次に政党別のトレンドね。ちょっと、意外に思うかもしれないけど・・・。”

![imageRL3-11-5](/2022-07-07-QEUR22_RLCLF11/imageRL3-11-5.jpg)

D先生 : “エッ？そうなの？立〇民主党とかいう政党は・・・。”

QEU:FOUNDER ： “**異様に極端な都市型政党なんです。**なんと維〇よりも都市に偏っています。これじゃあ、とてもじゃないけど国政など任せられないよ。**今の大問題は都市じゃなく、地方なんです。**次を見てみるよ・・・。”

![imageRL3-11-6](/2022-07-07-QEUR22_RLCLF11/imageRL3-11-6.jpg)

D先生 : “立〇（民主党）って東北で強いと思っていたんですが、実はそれほどでもないんですね。それに引き換え、**社〇党って地方全般から支持を受けている**んです。”

QEU:FOUNDER ： “もう消滅の危機とかいいながら、じつはあの党って**ポテンシャルが高い**んです。その力に気づかず、上の**頭が固い**モノだから、どんどん縮小スパイラルに入っているんです。”

D先生 : “なるほどねぇ～。実際のところ、立〇なんて、なくても全然困らない政党です。それに引き換え、**社〇党はこのまま消滅してもらうと困る**。”
