---
title: QEUR21_RLCLF0 – 「崖歩き(Cliff_Walking)」をさらっと復習しましょう(その１)
date: 2022-07-02
tags: ["QEUシステム", "メトリックス", "Python言語", "強化学習", "Cliff_Walking", "ディープラーニング"]
excerpt: Python言語によるCliff_Walkingの強化学習
---

## QEUR21_RLCLF0 – 「崖歩き(Cliff_Walking)」をさらっと復習しましょう(その１)

## ～　一回休みの「初めの一歩」　～

QEU:FOUNDER ; “さあて、今シリーズでは息抜きとして「崖（cliff）」について話そうと思います。コレ（↓）のこと・・・。”

![imageRL3-1-1](/2022-07-02-QEUR21_RMCLF0/imageRL3-1-1.jpg)

D先生 : “なんだ…、「Cliff-Walking」という簡単な強化学習の事例じゃないですか・・・。我々が今までやってきた、2048ゲームやマインスイーパーなんかよりはるかに簡単な例題ですよね。いまさらやる必要があるんですか？”

QEU:FOUNDER ： “一度やったテーマなので新しいモノは原則ないです。しいていえば、「できるだけ簡単にやる」がテーマです。ちなみに、Cliff-Walkingを環境に選んでいたのは、この機械（↓）が念頭にあるから・・・。”

![imageRL3-1-2](/2022-07-02-QEUR21_RMCLF0/imageRL3-1-2.jpg)

QEU:FOUNDER ： “つまり・・・、このマップ（↓）を見れば納得いかない(笑)？”

![imageRL3-1-3](/2022-07-02-QEUR21_RMCLF0/imageRL3-1-3.jpg)

D先生 : “なるほど・・・。アーク溶接ロボットの場合には、この図のCLIFFが金属になります。溶接棒の先端はCLIFFから外れてもいけないし、くっついてもいけないです。”

QEU:FOUNDER ： “つまり、システムはこんなイメージ（↓）です・・・。”

![imageRL3-1-4](/2022-07-02-QEUR21_RMCLF0/imageRL3-1-4.jpg)

QEU:FOUNDER ： “強化学習の立場でみると、こんな感じにならない(笑)？”

![imageRL3-1-5](/2022-07-02-QEUR21_RMCLF0/imageRL3-1-5.jpg)

QEU:FOUNDER ： “今回はpolicy-basedの手法を使います。この本（↓）はgoogle社のロボット開発エンジニアが書いた本であるが、おもしろい特徴があります。一般のテキストはまず最初にValue-basedの手法を学ぶが、この本ではPolicy-basedの手法を最初に紹介しています。”

![imageRL3-1-6](/2022-07-02-QEUR21_RMCLF0/imageRL3-1-6.jpg)

D先生 : “Policy-basedの手法には、それ特有の長所があるんでしょうね。まあ、せっかくですから最初に**「Cliff Walkingという環境」を理解する**ことから始めましょう。プログラムをドン！！”

```python
# ----------
# Cliff Walkingプレイグラウンドを操作してみた
#   state:
#	   各セルにおいて上下左右の方向に進むことができる。ただし、CLIFF領域にはいるとスタート(S点)に戻る
#	action: 以下の4種類の動作がある
#	0, UP
#	1, RIGHT
#	2, DOWN
#	3, LEFT
#	reward: 
#      セルが進むごとにreward=-1点, clifにハマり、S点に戻るときに-100点;
# ----------
from gym.envs.toy_text import CliffWalkingEnv
import gym
import numpy as np

#=================================================
# difinition of function
#=================================================
# ルート表示用の関数
def showRoute(states):
    board = np.zeros([4, 12])
    # add cliff marked as -1
    board[3, 1:11] = -1
    for i in range(0, ROWS):
        print('-------------------------------------------------')
        out = '| '
        for j in range(0, COLS):
            token = '0'
            if board[i, j] == -1:
                token = '*'
            if (i, j) in states:
                token = 'R'
            if (i, j) == S:
                token = 'S'
            if (i, j) == G:
                token = 'G'
            out += token + ' | '
        print(out)
    print('-------------------------------------------------') 

# -----
# 状態の表記を変更(2桁番号からi,jへ)
def calc_address(number_st):

    i = int(number_st/COLS)
    j = number_st - i*COLS

    return int(i),int(j)

#=================================================
# Calculation class(1)
#=================================================
# 崖（環境）
class Cliff:

    def __init__(self):
        self.end = False
        self.pos = S
        self.board = np.zeros([4, 12])
        # add cliff marked as -1
        self.board[3, 1:11] = -1

    def nxtPosition(self, action):
        if action == "up":
            nxtPos = (self.pos[0] - 1, self.pos[1])
        elif action == "down":
            nxtPos = (self.pos[0] + 1, self.pos[1])
        elif action == "left":
            nxtPos = (self.pos[0], self.pos[1] - 1)
        else:
            nxtPos = (self.pos[0], self.pos[1] + 1)
        # check legitimacy
        if nxtPos[0] >= 0 and nxtPos[0] <= 3:
            if nxtPos[1] >= 0 and nxtPos[1] <= 11:
                self.pos = nxtPos

        if self.pos == G:
            self.end = True
            print("Game ends reaching goal")
        if self.board[self.pos] == -1:
            self.end = True
            print("Game ends falling off cliff")

        return self.pos

    def giveReward(self):
        # give reward
        if self.pos == G:
            return -1
        if self.board[self.pos] == 0:
            return -1
        return -100

#=================================================
# Calculation class(2)
#=================================================
# 人（エージェント）
class Agent:
    def __init__(self):
        #self.cliff = Cliff()
        #self.actions = ["up", "left", "right", "down"]
        self.states = []  # record position and action of each episode
        self.pos = S

        # ----------
        # 環境のﾘｾｯﾄ
        print("環境のﾘｾｯﾄ:",env.reset())
        env.render()
        #36
        #o  o  o  o  o  o  o  o  o  o  o  o
        #o  o  o  o  o  o  o  o  o  o  o  o
        #o  o  o  o  o  o  o  o  o  o  o  o
        #x  C  C  C  C  C  C  C  C  C  C  T

        # ----------
        self.play()

        # ----------
        #self.reset()

    def reset(self):
        # ----------
        self.states = []
        self.cliff = Cliff()
        self.pos = S
        # 環境のﾘｾｯﾄ
        print("環境のﾘｾｯﾄ:",env.reset())
        env.render()
        #36
        #o  o  o  o  o  o  o  o  o  o  o  o
        #o  o  o  o  o  o  o  o  o  o  o  o
        #o  o  o  o  o  o  o  o  o  o  o  o
        #x  C  C  C  C  C  C  C  C  C  C  T

    def play(self):
        # ----------
        #	0, UP
        #	1, RIGHT
        #	2, DOWN
        #	3, LEFT
        # ----------
        iStep = 0
        for action in action_inputs:
            # 環境(env)を動かす
            if action == 0:
                # ----------
                out_param = env.step(0)
                #print("iStep:{0},上に移動する:{1}".format(iStep,out_param)) # 上に移動する
                #env.render()
                #(24, -1, False, {'prob': 1.0})
                #o  o  o  o  o  o  o  o  o  o  o  o
                #o  o  o  o  o  o  o  o  o  o  o  o
                #x  o  o  o  o  o  o  o  o  o  o  o
                #o  C  C  C  C  C  C  C  C  C  C  T

            elif action == 1:
                # ----
                out_param = env.step(1)
                #print("iStep:{0},右に移動する:{1}".format(iStep,out_param)) # 右に移動する
                #env.render()

            elif action == 2:
                # ----
                out_param = env.step(2)
                #print("iStep:{0},下に移動する:{1}".format(iStep,out_param)) # 下に移動する
                #env.render()

            elif action == 3:
                # ----
                out_param = env.step(3)
                #print("iStep:{0},左に移動する:{1}".format(iStep,out_param)) # 左に移動する
                #env.render()
                #(25, -1, False, {'prob': 1.0})

            # ----------
            curr_state = calc_address(out_param[0])
            cur_reward = out_param[1]
            print("iStep:{0}, action:{1}, curr_state:{2}, cur_reward:{3}".format(iStep,action,curr_state,cur_reward))
            self.states.append(curr_state)
            iStep = iStep + 1

        # ----------
        showRoute(self.states)
                    
#=================================================
# main function            
#=================================================
if __name__ == "__main__":

    # ----------
    ROWS = 4
    COLS = 12
    S = (3, 0)
    G = (3, 11)

    # ----------
    # 環境の導入
    env = CliffWalkingEnv()

    # ----------
    # 行動連鎖のインプット
    action_inputs = [0,0,1,1,1]

    # ----------
    # エージェントを実行する
    Agent()

```

D先生： “「action_inputs」というリストの要素をいじれば、コマの動きが変わり、結果としてマップの状態が変わってきます。”

![imageRL3-1-7](/2022-07-02-QEUR21_RMCLF0/imageRL3-1-7.jpg)

QEU:FOUNDER ： “いきなりpolicy gradientに行くのも面白くないので、最初はDQN with Experience Replayにしましょう。”

## ～　まとめ　～

C部長 : “ちょっと「潮目」が変わったのか？それでもイケイケのイケメンバトルです。”

![imageRL3-1-8](/2022-07-02-QEUR21_RMCLF0/imageRL3-1-8.jpg)

C部長 : “では、ぼくから・・・。イッヒ・リーベ・ドイチェ・・・。”

### Gastrede von Uiko Hasegawa, Ko-Vorsitzende der japanischen Grünen, auf der Bundesdelegiertenkonferenz in Hannover, 18. November 2012

### 2012年11月18日、ハノーバーで開催された連邦代表者会議での日本緑の党共同代表・長谷川ういこ氏によるゲストスピーチ。

[![MOVIE1](http://img.youtube.com/vi/8GswwVA4f7w/0.jpg)](http://www.youtube.com/watch?v=8GswwVA4f7w "Gastrede von Uiko Hasegawa auf dem Grünen-Parteitag, 18. November 2012")

QEU:FOUNDER ： “こうしてみると、この人のキャリアは長いね。”

![imageRL3-1-9](/2022-07-02-QEUR21_RMCLF0/imageRL3-1-9.jpg)

C部長 : “G国にはちゃんと、この党に関するwikiがあるんだね。ちなみに、J国のwikiは様子が違うようだ。興味があれば自分で調べてみてください・・・。”

D先生： “残念ながら、このG国のwikiの情報も2012年で止まっているしね・・・。”

QEU:FOUNDER ： “今後、動きがでてくるのではないでしょうか・・・。”
