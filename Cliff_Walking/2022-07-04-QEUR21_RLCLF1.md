---
title: QEUR21_RLCLF1 – 「崖歩き(Cliff_Walking)」をさらっと復習しましょう(その2)
date: 2022-07-04
tags: ["QEUシステム", "メトリックス", "Python言語", "強化学習", "Cliff_Walking", "ディープラーニング"]
excerpt: Python言語によるCliff_Walkingの強化学習
---

## QEUR21_RLCLF1 – 「崖歩き(Cliff_Walking)」をさらっと復習しましょう(その2)

## ～　これでも立派に「ベースライン」　～

D先生 : “さて、いきなり**DQN with Experience Replay(DQN-ER)**をやることになるらしいですが・・・。QEUのプロジェクトも昔よりも少しだけレベルが上がったとはいえ、このテーマでDQN-ERを使うとは「日本刀で大根を切る」感覚は否めません。”

QEU:FOUNDER ： “いいじゃないの、別に・・・。プログラムをドン・・・。”

```python
# ----------------
# CLIFF WALKING SIMULATIONの強化学習システム
# step1 : 素うどんの強化学習モデルです
# step1 : dqn_wER_cwSimu(agent_original).py
# これがベースラインになります。
# ---------------- 
# 数値計算のﾗｲﾌﾞﾗﾘ読み込み
from gym.envs.toy_text import CliffWalkingEnv
import math
import numpy as np
import copy, random, time
import pandas as pd
from collections import deque, Counter
from scipy.special import softmax
from IPython.display import clear_output
# ----------------
import torch
import torch.nn.functional
import torch.utils.data
# ---------------- 
import matplotlib.pyplot as plt
#%matplotlib inline

# =================================================
# difinition of function
# =================================================
# 状態の表記を変更(oneHOTへ)
def calc_address(number_state):

    state = np.zeros(48)
    state[number_state] = 1.0

    return state

# ----------------
# 状態の表記を変更(2桁番号からi,jへ)
def calc_XYcoord(number_state):

    i = number_state/COLS
    j = number_state - i*COLS

    return int(i+0.1), int(j+0.1)

# ----------------
# 実行可能な命令を選択する
def calc_availaction(number_state):

    # ----------
    # 移動命令
    #	0, UP
    #	1, RIGHT
    #	2, DOWN
    #	3, LEFT
    # ----------
    # 初期値
    arr_avail = [0,1,2,3]
    
    # 二次元アドレス化
    row, col = calc_XYcoord(number_state)

    if row == 0:
        arr_avail = [1,2,3]
    elif col == 0:
        arr_avail = [0,1,2]
    elif col == 11:
        arr_avail = [0,2,3]
    elif row == 3 and col == 0:
        arr_avail = [0]
        
    return arr_avail

#=================================================
# Deep Learning Model class            
#=================================================
# PyTorchのDLの定義
class Net(torch.nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = torch.nn.Linear(dim_input, 128)
        self.fc1.weight.data.normal_(0, 0.1)
        self.fc2 = torch.nn.Linear(128, 128)
        self.fc2.weight.data.normal_(0, 0.1)
        self.fc3 = torch.nn.Linear(128, dim_output)
        self.fc3.weight.data.normal_(0, 0.1)
        
    def forward(self, x):
        x = torch.nn.functional.relu(self.fc1(x))
        x = torch.nn.functional.relu(self.fc2(x))
        x = self.fc3(x)
        return x

#=================================================
# Calculation class(2) : DQN_Solver:          
#=================================================
# Solving the minesweeper in Deep Q-learning
class DQN_Solver:

    def __init__(self, state_size, action_size):
        
        # --------------------------------------------------
        # ハイパーパラメタ
        self.state_size  = state_size
        self.action_size = action_size
        self.memory  = deque(maxlen=MEMORY_CAPACITY)
        self.gamma   = GAMMA
        self.epsilon = EPSILON
        self.e_decay = EPDECAY
        self.e_min   = EPMIN
        self.learning_rate = LRATE

        # --------------------------------------------------
        # crate instance for input
        self.eval_net, self.target_net = Net(), Net()                           # 利用Net创建两个神经网络: 评估网络和目标网络
        self.learn_step_counter = 0    
        # --------------------------------------------------
        # Save and load the model via state_dict
        #self.eval_net.load_state_dict(torch.load(file_input_model))
        # --------------------------------------------------
        # set validaiton mode
        self.eval_net.eval()
        # --------------------------------------------------
        # set training parameters
        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=self.learning_rate)
        self.criterion = torch.nn.MSELoss()

    # ----------------
    # PyTorchモデルの保存  
    #def save_models(self):
        #torch.save(self.eval_net.state_dict(), file_output_model)

    # ----------------
    # メモリを蓄積する    
    def remember_memory(self, state, action, reward, state_next, number_state_next, done):
        self.memory.append((state, action, reward, state_next, number_state_next, done))

    # ----------------
    # 命令を選択する
    def choose_action(self, state, number_state, iCnt_turn):

        # ----------------
        # 移動命令
        #	0, UP
        #	1, RIGHT
        #	2, DOWN
        #	3, LEFT
        # ----------------
        Qvalue = -0.0001

        # ----------------
        # 実行可能な命令を選択する
        arr_avail = calc_availaction(number_state)

        # ----------------------------------------
        # 最適命令の選択
        if self.epsilon <= random.random():
            # DQNによる選択
            acType  = "machine"
            iCnt = 0
            for action in arr_avail:
                # [array]-action を生成します。
                #print("state:",state)
                #print("a:",a)
                temp_s_a = np.hstack([state, [action]])
                if iCnt == 0:
                    mx_input = np.array([temp_s_a])  # 状態(S)行動(A)マトリックス
                else:
                    mx_input = np.append(mx_input, np.array([temp_s_a]), axis=0)  # 状態(S)行動(A)マトリックス
                iCnt = iCnt + 1
            # print("----- mx_input -----")
            # print(mx_input)
            # --------------------------------------------------
            # generate new 'x'
            x_input_tensor = torch.from_numpy(mx_input).float()
            # predict 'y'
            with torch.no_grad():
                y_pred_tensor = self.eval_net(x_input_tensor)
            # convert tensor to numpy
            y_pred      = y_pred_tensor.data.numpy().flatten()
            Qvalue      = np.max(y_pred)
            temp_order  = np.argmax(y_pred)
            a_order     = arr_avail[temp_order]
        else: 
            # 乱数による選択
            acType  = "random"
            a_order = np.random.choice(arr_avail)

        return a_order, acType, Qvalue

    # ----------------
    # (REPLAY EXPERIENCE)学習する
    def replay_experience(self, batch_size):

        # 目标网络参数更新
        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:                  # 一开始触发，然后每100步触发
            self.target_net.load_state_dict(self.eval_net.state_dict())         # 将评估网络的参数赋给目标网络
        self.learn_step_counter += 1   
        # --------------------------------------------------
        batch_size = min(batch_size, len(self.memory))
        minibatch  = random.sample(self.memory, batch_size)
        X, Y = np.array([]), np.array([])
        # --------------------------------------------------
        for ibat in range(batch_size):
            state, action, reward, state_next, number_state_next, done = minibatch[ibat]
            # [array]-action を結合します。
            state_action_curr = np.hstack([state, [action]])
            #print("state_action_curr",state_action_curr)
            # ----------------
            if done:
                target_f = reward
            else:

                # 実行可能な命令を選択する
                arr_avail = calc_availaction(number_state_next)
            
                # ----------------
                mx_input = []  # 状態(state)と行動(action)マトリックス
                iCnt = 0
                for imov in arr_avail:
                    # [array]-action を結合します。
                    temp_s_a = np.hstack([state_next, [imov]])
                    if iCnt == 0:
                        mx_input = [temp_s_a]  # 状態(S)行動(A)マトリックス
                    else:
                        mx_input = np.concatenate([mx_input, [temp_s_a]], axis=0)  # 状態(S)行動(A)マトリックス
                    iCnt = iCnt + 1
                # ----------------
                mx_input = np.array(mx_input)
                #print("--- mx_input ---")
                #print(mx_input)
                # --------------------------------------------------
                # generate new 'x'
                x_input_tensor = torch.from_numpy(mx_input).float()
                # predict 'y'
                with torch.no_grad():
                    y_pred_tensor = self.target_net(x_input_tensor)
                # convert tensor to numpy
                y_pred = y_pred_tensor.data.numpy().flatten()
                np_n_r_max = np.amax(y_pred)
                target_f = reward + self.gamma * np_n_r_max
            # ----------------
            if ibat == 0:
                X = np.array([state_action_curr])  # 状態(S)行動(A)マトリックス
            else:
                X = np.append(X, np.array([state_action_curr]), axis=0)  # 状態(S)行動(A)マトリックス
            Y = np.append(Y,target_f)

        # --------------------------------------------------
        # TRAINING
        # convert numpy array to tensor
        state_action_next = torch.from_numpy(X).float()
        q_target = torch.from_numpy(Y.reshape(-1, 1)).float()
        #print("state_action_next:",state_action_next)
        #print("q_target:",q_target)
        
        # --- building model ---
        q_eval = self.eval_net(state_action_next)

        # calculate loss
        loss = self.criterion(q_eval, q_target)

        # update weights
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        # Show progress
        #print('learn done -- [epsilon: {0}, loss: {1}]'.format(self.epsilon, loss))
        
        if self.epsilon > self.e_min:
            self.epsilon *= self.e_decay

        return round(self.epsilon,5) , round(loss.data.item(),5)


#=================================================
# Calculation class(3) : Agent
#=================================================
class Agent():
    # -----
    def __init__(self):

        # --------------------------------------------------
        # イプシロンの設定
        self.epsilon    = EPSILON     # ε-Greedy

        # ---------------------------
        # 記録用パラメタ類(プレイベース)
        self.arr_iplay     = []  # count game play    プレイ番号
        self.arr_maxturn   = []  # turn game play    ターン数
        self.arr_maxscore  = []  # rl_score game play    報酬の総和
        self.arr_victory   = []  # victory    勝利したか
        self.arr_loss      = []  # DQN-Experience Replay学習
        self.arr_epsilon   = []  # ε-Greedy  
        
        # ---------------------------
        # Q値の分析用(プレイベース)
        self.arr_numQV     = []  # QV値のN数
        self.arr_maxQV     = []  # QV値の最大値
        self.arr_q25QV     = []  # QV値の4分の1値
        self.arr_q75QV     = []  # QV値の4分の3値
        
        #print("starting simulation")
        # --------------------------------------------------
        # エピソードを実行する
        for iCnt_play in range(num_episodes):       # num_episodes
        
            # ゲームする
            maxturn, maxscore, flag_goal = self.get_episode(iCnt_play)

            # ----------
            # DQN-Experience Replay学習  
            val_epsilon, val_loss = dql_solver.replay_experience(BATCH_SIZE)
            # 結果の出力
            print("iCnt_play:{0}, maxturn:{1}, maxscore:{2}, flag_goal:{3}, epsilon:{4}, loss:{5}".format(iCnt_play, maxturn, maxscore, flag_goal, val_epsilon, val_loss))

            # ----------
            # 記録用パラメタ類(プレイベース)の追加
            self.arr_iplay.append(iCnt_play)        # count game play    プレイ番号
            self.arr_maxturn.append(maxturn)        # max_turn   ゲームのターン数
            self.arr_maxscore.append(maxscore)      # rl_score game play    最終プレイスコア
            self.arr_victory.append(flag_goal)    # victory    勝利したか
            self.arr_loss.append(val_loss)          # DQN-Experience Replay学習
            self.arr_epsilon.append(val_epsilon)    # イプシロン       
            # ----------
            # Q値の保管(プレイベース)
            self.arr_numQV.append(maxturn)     # QV値のN数
            self.arr_maxQV.append(np.max(self.arr_predQV))                 # QV値の最大値
            self.arr_q25QV.append(np.percentile(self.arr_predQV, q=25))    # QV値の4分の1値
            self.arr_q75QV.append(np.percentile(self.arr_predQV, q=75))    # QV値の4分の3値
            # ----------
            # しばらくすれば表示が消えます
            if iCnt_play%50 == 0:
                time.sleep(SLEEP_TIME)
                clear_output(wait=True)

        # --------------------------------------------------
        # 学習履歴を出力する
        fig = plt.figure(figsize=(14, 6))
        ax1 = fig.add_subplot(1, 2, 1)
        ax1.set_title('learning transition : epsilon')
        ax1.plot(self.arr_iplay, self.arr_epsilon, label="epsilon", color="blue")
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('epsilon')
        ax1.legend(loc='best')
        # -----
        ax2 = fig.add_subplot(1, 2, 2)
        ax2.set_title('learning transition : Learning loss')
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('Loss amount')
        ax2.plot(self.arr_iplay, self.arr_loss, label="loss", color="blue")
        ax2.legend(loc='best')
        plt.show()

        # --------------------------------------------------
        # グラフを表示する
        self.show_graph()
        
    # --------------------------------------------------
    # エピソードを運用する
    def get_episode(self, iCnt_play):

        # ---------------------------
        # 機械学習用のパラメタ
        a_row, a_col, Qvalue = -1, -1, 0
        reward, done, flag_goal = 0, False, False
        number_state, number_state_next, maxturn, maxscore = 0, 0, 0, 0
        state, state_next = np.zeros(48), np.zeros(48)
        
        # ---------------------------
        # 記録用パラメタ類(ターンベース)
        self.arr_iturn     = []    # ターン・カウンタリスト
        self.orders_row    = []    # 指示リスト(row)
        self.orders_col    = []    # 指示リスト(col)
        self.arr_orders    = []    # 指示リスト(アドレス)
        self.arr_acType    = []    # 指示のタイプ
        self.arr_scores    = []    # ゲームスコアリスト
        self.arr_dones     = []    # ゲームオーバーフラグ
        self.mx_state      = []    # 状態マトリックス
        self.arr_predQV    = []    # Q値のリスト

        # ---------------------------
        # ゲームをリセットする
        number_state = env.reset()
        # 状態(state)を数字->ONEHOT(48)に変更する
        state = calc_address(number_state)

        # ---------------------------
        # ゲームをプレイする
        iCnt_turn = 0
        while True:

            # 命令(a_order)と状態(state)を作成する
            a_order, acType, Qvalue = dql_solver.choose_action(state, number_state, iCnt_turn)

            # ---------------------------
            # ゲームをもう一歩進める
            number_state_next, reward, done, _ = env.step(a_order)
            state_next = calc_address(number_state_next)

            # ---------------------------
            # 学習の進捗を表示する（一次元アドレスを二次元アドレスに変換する）
            a_row, a_col = calc_XYcoord(number_state) 
            #print("iCnt_play:{}, iCnt_turn:{}, a_row:{}, a_col:{}, a_order:{}, reward:{}".format(iCnt_play, iCnt_turn, a_row, a_col, a_order, reward))

            # ---------------------------
            # 記録用リストを追記する
            self.arr_iturn.append(iCnt_turn)   # ターン・カウンタリスト
            self.orders_row.append(a_row)       # 指示リスト(ROW)
            self.orders_col.append(a_col)       # 指示リスト(COL)
            self.arr_orders.append(a_order)     # 指示リスト(アドレス)
            self.arr_acType.append(acType)      # 指示のタイプ
            self.arr_scores.append(reward)      # ゲームスコアリスト   
            self.arr_dones.append(done)     # ゲームオーバーフラグ
            if iCnt_turn == 0:
                self.mx_state       = [state]            # 状態マトリックス(Current)  
                self.mx_state_next  = [state_next]       # 状態マトリックス(Next) 
            else:
                self.mx_state = np.concatenate([self.mx_state, [state]], axis=0)       # 状態マトリックス(Current)   
                self.mx_state_next = np.concatenate([self.mx_state_next, [state_next]], axis=0) # 状態マトリックス(Next) 
            self.arr_predQV.append(Qvalue)      # Q値のリスト
            # ----------------
            # Experience Replay配列に保管する
            dql_solver.remember_memory(state, a_order, reward, state_next, number_state_next, done)
            # ----------------
            # 実行継続するか分岐
            if iCnt_play < 100 and (done == True or iCnt_turn > 20000):
                # Game over
                break # (game over rip)
            elif iCnt_play >= 100 and (done == True or iCnt_turn > 20000 or reward < -90):
                # Game over
                break # (game over rip)
            else:
                # count of game turn
                number_state    = number_state_next
                state           = state_next
                iCnt_turn       = iCnt_turn + 1

        # --------------------------------------------------
        # ゲームが完了しているのか
        flag_goal   = done
        # プレイデータの引継ぎ
        maxturn, maxscore = iCnt_turn, np.sum(self.arr_scores)

        return maxturn, maxscore, flag_goal

    # ----------------
    # 学習結果のグラフ化
    def show_graph(self):

        fig2 = plt.figure(figsize=(14, 12))
        # -----
        ax1 = fig2.add_subplot(2, 2, 1)
        ax1.set_title('DQN parameter transition: QValue')
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('QValue')
        ax1.plot(self.arr_iplay, self.arr_maxQV, label="max_QV", color="blue")
        ax1.plot(self.arr_iplay, self.arr_q25QV, label="q25_QV", color="red")
        ax1.plot(self.arr_iplay, self.arr_q75QV, label="q75_QV", color="green")
        ax1.grid(True)
        ax1.legend(loc='best')
        # -----
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_maxturn).rolling(window=12, center=True).mean()
        ax3 = fig2.add_subplot(2, 2, 2)
        ax3.set_title('learning transition : maxturn')
        ax3.set_xlabel('#episode')
        ax3.set_ylabel('maxturn')
        ax3.grid(True)
        ax3.plot(self.arr_iplay, self.arr_maxturn, label="original", color="blue")
        ax3.plot(self.arr_iplay, y_rolling, label="moving", color="red")
        ax3.legend(loc='best')
        # -----
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_maxscore).rolling(window=12, center=True).mean()
        ax4 = fig2.add_subplot(2, 2, 3)
        ax4.set_title('learning transition : maxscore')
        ax4.set_xlabel('#episode')
        ax4.set_ylabel('maxscore')
        ax4.grid(True)
        ax4.plot(self.arr_iplay, self.arr_maxscore, label="original", color="blue")
        ax4.plot(self.arr_iplay, y_rolling, label="moving", color="red")
        ax4.legend(loc='best')
        # -----
        fig2.tight_layout()
        #fig.savefig("./AAC_img.png")
        plt.show()


#=================================================
# main function            
#=================================================
if __name__ == "__main__":

    # ---------------------------
    # ゲームの初期化
    ROWS = 4
    COLS = 12
    S = (3, 0)
    G = (3, 11)

    # ---------------------------
    # ゲームの初期化
    env = CliffWalkingEnv()

    # ---------------------------
    # パラメタ設定の初期化
    state_size     = 48
    action_size    = 4
    dim_input      = 48 + 1
    dim_output     = 1

    # ---------------------------
    # ハイパーパラメタ
    BATCH_SIZE = 128*4                      # サンプルサイズ
    LRATE   = 0.005                         # 学習率
    EPSILON = 0.99                          # greedy policy
    EPDECAY = 0.999
    EPMIN   = 0.01
    GAMMA   = 0.999                         # reward discount
    TARGET_REPLACE_ITER = 100               # 目标网络更新频率
    MEMORY_CAPACITY = 128*8*4               # メモリ容量
    # ---------------------------
    SLEEP_TIME      = 0.01
    num_episodes    = 5000                  # 繰り返し回数

    # ---------------------------
    # フォルダ名の指定
    #foldername = "./ML_csvout/"  # My project folder

    # ---------------------------
    # dql_solverのインスタンス化
    dql_solver  = DQN_Solver(state_size, action_size)

    # ---------------------------
    # 出力用:Pytorchモデルのファイル名
    #comment_output_model = "initial"
    #code_output_model = "model_cliffwalk_DQNER_{0}.pt".format(comment_output_model)  # モデルの指定
    #file_output_model = foldername + code_output_model  # ファイルパス名の生成

    # ---------------------------
    # DQN-ERで学習する
    Agent()

```

QEU:FOUNDER ： “学習の結果はこんな感じです。Experience_Replayの場合、eval_netとtarget_netを分けて更新頻度を分けたほうが収束がよくなるようですね。ちなみに、環境は「初めの100件だけは崖から落ちても自動復帰するモード」を使用しています。そうしたほうがゲームオーバーが出やすいので初期値がよくなります。”

**（学習曲線）**

![imageRL3-2-1](/2022-07-04-QEUR21_RMCLF1/imageRL3-2-1.jpg)

**（パフォーマンス推移）**

![imageRL3-2-2](/2022-07-04-QEUR21_RMCLF1/imageRL3-2-2.jpg)

D先生 : “よしよし、これで落ち着いた・・・。これで、**DQN-ERは強力な方法である**ことを再認識しました。エピソードが3000回あたりから一気に学習が落ち着いたことがQ値の変化から見て取れます。”

QEU:FOUNDER ： “次は、このスキームをベースにして遊んでみます。いつも言っていますが、小生は強化学習そのものに興味はありません。テクノメトリックスを適用する対象が、たまたまに強化学習であっただけであり・・・。”

## ～　まとめ　～

QEU:FOUNDER ： “今日はイケメンバトルの能書きはなし、コレよこれ・・・。”

[![MOVIE1](http://img.youtube.com/vi/nooj7UXToKE/0.jpg)](http://www.youtube.com/watch?v=nooj7UXToKE "メロリンキュー31年目の解禁!?の裏側!!【16歳のメロリンが47歳の山本太郎を応援】未来の自分がんばれ!!れいわ新選組 山本太郎 街頭演説")

D先生（60代の設定です） : “昔、テレビで見ました。”

QEU:FOUNDER（60代の設定です） ： “まさか、これを「お蔵出し」するとは思わなかった。”

[![MOVIE2](http://img.youtube.com/vi/yaU-b6nKkVQ/0.jpg)](http://www.youtube.com/watch?v=yaU-b6nKkVQ "山本太郎　ダンス甲子園　1991年　メロリンQーーーーーー！！")

D先生: “まだガウンを着ています。”

QEU:FOUNDER： “いつかな？Xデーは・・・。”
