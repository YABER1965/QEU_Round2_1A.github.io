---
title: QEUR21_RLCLF3 – 「崖歩き(Cliff_Walking)」をさらっと復習しましょう(その4)
date: 2022-07-06
tags: ["QEUシステム", "メトリックス", "Python言語", "強化学習", "Cliff_Walking", "ディープラーニング"]
excerpt: Python言語によるCliff_Walkingの強化学習
---

## QEUR21_RLCLF3 – 「崖歩き(Cliff_Walking)」をさらっと復習しましょう(その4)

## ～　少しだけでもアップすればよい　～

### ・・・　すでに前回で相当に成功しているからネ　・・・

QEU:FOUNDER ： “それでは、**マンハッタン距離をメトリックスとして導入しましょう**。これも、以前（メイズ）でやったことですが・・・。”

![imageRL3-4-1](/2022-07-06-QEUR21_RMCLF3/imageRL3-4-1.jpg)

QEU:FOUNDER ： “トータル距離で履歴ベクトルを生成し、理想的な経路をたどるベクトルと実際の経路のベクトルの比較を行います。これによって、ニューラルネットへのインプット情報はさらに大きくなりました。”

- **当初　：　ゲーム盤の大きさ（48）**
- **現在　：　ゲーム盤の大きさ（48）　+　マンハッタン距離(1)　+　S距離(1)　+　G距離(1)**

D先生 : “なんか、無駄だなぁ・・・。”

QEU:FOUNDER ： “おっと、その話は後で！！それではプログラムをドン！！”

```python
# ----------------
# CLIFF WALKING SIMULATIONの強化学習システム
# step3 : 「チーズちくわ(SG距離にマンハッタン)」を加えたうどん（強化学習モデル）です
# step3 : dqn_wER_cwSimu(agent_distance).py
# ---------------- 
# 数値計算のﾗｲﾌﾞﾗﾘ読み込み
from gym.envs.toy_text import CliffWalkingEnv
import math
import numpy as np
import copy, random, time
import pandas as pd
from collections import deque, Counter
from scipy.special import softmax
from IPython.display import clear_output
# ----------------
import torch
import torch.nn.functional
import torch.utils.data
# ---------------- 
import matplotlib.pyplot as plt
#%matplotlib inline

# =================================================
# difinition of function
# =================================================
# 状態の表記を変更(oneHOTへ)
def calc_address(number_state):

    state = np.zeros(48)
    state[number_state] = 1.0

    return state

# ----------------
# 状態の表記を変更(2桁番号からi,jへ)
def calc_XYcoord(number_state):

    i = number_state/COLS
    j = number_state - i*COLS

    return int(i+0.1), int(j+0.1)

# ----------------
# 実行可能な命令を選択する
def calc_availaction(number_state):

    # ----------
    # 移動命令
    #	0, UP
    #	1, RIGHT
    #	2, DOWN
    #	3, LEFT
    # ----------
    # 初期値
    arr_avail = [0,1,2,3]
    
    # 二次元アドレス化
    row, col = calc_XYcoord(number_state)

    if row == 0:
        arr_avail = [1,2,3]
    elif col == 0:
        arr_avail = [0,1,2]
    elif col == 11:
        arr_avail = [0,2,3]
    elif row == 3 and col == 0:
        arr_avail = [0]
        
    return arr_avail

#=================================================
# Deep Learning Model class            
#=================================================
# PyTorchのDLの定義
class Net(torch.nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = torch.nn.Linear(dim_input, 128)
        self.fc1.weight.data.normal_(0, 0.1)
        self.fc2 = torch.nn.Linear(128, 128)
        self.fc2.weight.data.normal_(0, 0.1)
        self.fc3 = torch.nn.Linear(128, dim_output)
        self.fc3.weight.data.normal_(0, 0.1)
        
    def forward(self, x):
        x = torch.nn.functional.relu(self.fc1(x))
        x = torch.nn.functional.relu(self.fc2(x))
        x = self.fc3(x)
        return x

#=================================================
# Calculation class(2) : DQN_Solver:          
#=================================================
# Solving the minesweeper in Deep Q-learning
class DQN_Solver:

    def __init__(self, state_size, action_size):
        
        # --------------------------------------------------
        # ハイパーパラメタ
        self.state_size  = state_size
        self.action_size = action_size
        self.memory  = deque(maxlen=MEMORY_CAPACITY)
        self.gamma   = GAMMA
        self.epsilon = EPSILON
        self.e_decay = EPDECAY
        self.e_min   = EPMIN
        self.learning_rate = LRATE

        # --------------------------------------------------
        # crate instance for input
        self.eval_net, self.target_net = Net(), Net()                           # 利用Net创建两个神经网络: 评估网络和目标网络
        self.learn_step_counter = 0    
        # --------------------------------------------------
        # Save and load the model via state_dict
        #self.eval_net.load_state_dict(torch.load(file_input_model))
        # --------------------------------------------------
        # set validaiton mode
        self.eval_net.eval()
        # --------------------------------------------------
        # set training parameters
        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=self.learning_rate)
        self.criterion = torch.nn.MSELoss()

    # ----------------
    # PyTorchモデルの保存  
    #def save_models(self):
        #torch.save(self.eval_net.state_dict(), file_output_model)

    # ----------------
    # メモリを蓄積する    
    def remember_memory(self, state, action, reward, state_next, number_state_next, done):
        self.memory.append((state, action, reward, state_next, number_state_next, done))

    # ----------------
    # 命令を選択する
    def choose_action(self, state, number_state, iCnt_turn):

        # ----------------
        # 移動命令
        #	0, UP
        #	1, RIGHT
        #	2, DOWN
        #	3, LEFT
        # ----------------
        Qvalue = -0.0001

        # ----------------
        # 実行可能な命令を選択する
        arr_avail = calc_availaction(number_state)

        # ----------------------------------------
        # 最適命令の選択
        if self.epsilon <= random.random():
            # DQNによる選択
            acType  = "machine"
            iCnt = 0
            for action in arr_avail:
                # [array]-action を生成します。
                #print("state:",state)
                #print("a:",a)
                temp_s_a = np.hstack([state, [action]])
                if iCnt == 0:
                    mx_input = np.array([temp_s_a])  # 状態(S)行動(A)マトリックス
                else:
                    mx_input = np.append(mx_input, np.array([temp_s_a]), axis=0)  # 状態(S)行動(A)マトリックス
                iCnt = iCnt + 1
            # print("----- mx_input -----")
            # print(mx_input)
            # --------------------------------------------------
            # generate new 'x'
            x_input_tensor = torch.from_numpy(mx_input).float()
            # predict 'y'
            with torch.no_grad():
                y_pred_tensor = self.eval_net(x_input_tensor)
            # convert tensor to numpy
            y_pred      = y_pred_tensor.data.numpy().flatten()
            Qvalue      = np.max(y_pred)
            temp_order  = np.argmax(y_pred)
            a_order     = arr_avail[temp_order]
        else: 
            # 乱数による選択
            acType  = "random"
            a_order = np.random.choice(arr_avail)

        return a_order, acType, Qvalue


    # ----------------
    # (REPLAY EXPERIENCE)学習する
    def replay_experience(self, batch_size):

        # 目标网络参数更新
        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:                  # 一开始触发，然后每100步触发
            self.target_net.load_state_dict(self.eval_net.state_dict())         # 将评估网络的参数赋给目标网络
        self.learn_step_counter += 1   
        # --------------------------------------------------
        batch_size = min(batch_size, len(self.memory))
        minibatch  = random.sample(self.memory, batch_size)
        X, Y = np.array([]), np.array([])
        # --------------------------------------------------
        for ibat in range(batch_size):
            state, action, reward, state_next, number_state_next, done = minibatch[ibat]
            # [array]-action を結合します。
            state_action_curr = np.hstack([state, [action]])
            #print("state_action_curr",state_action_curr)
            # ----------------
            if done:
                target_f = reward
            else:

                # 実行可能な命令を選択する
                arr_avail = calc_availaction(number_state_next)
            
                # ----------------
                mx_input = []  # 状態(state)と行動(action)マトリックス
                iCnt = 0
                for imov in arr_avail:
                    # [array]-action を結合します。
                    temp_s_a = np.hstack([state_next, [imov]])
                    if iCnt == 0:
                        mx_input = [temp_s_a]  # 状態(S)行動(A)マトリックス
                    else:
                        mx_input = np.concatenate([mx_input, [temp_s_a]], axis=0)  # 状態(S)行動(A)マトリックス
                    iCnt = iCnt + 1
                # ----------------
                mx_input = np.array(mx_input)
                #print("--- mx_input ---")
                #print(mx_input)
                # --------------------------------------------------
                # generate new 'x'
                x_input_tensor = torch.from_numpy(mx_input).float()
                # predict 'y'
                with torch.no_grad():
                    y_pred_tensor = self.target_net(x_input_tensor)
                # convert tensor to numpy
                y_pred = y_pred_tensor.data.numpy().flatten()
                np_n_r_max = np.amax(y_pred)
                target_f = reward + self.gamma * np_n_r_max
            # ----------------
            if ibat == 0:
                X = np.array([state_action_curr])  # 状態(S)行動(A)マトリックス
            else:
                X = np.append(X, np.array([state_action_curr]), axis=0)  # 状態(S)行動(A)マトリックス
            Y = np.append(Y,target_f)

        # --------------------------------------------------
        # TRAINING
        # convert numpy array to tensor
        state_action_next = torch.from_numpy(X).float()
        q_target = torch.from_numpy(Y.reshape(-1, 1)).float()
        #print("state_action_next:",state_action_next)
        #print("q_target:",q_target)
        
        # --- building model ---
        q_eval = self.eval_net(state_action_next)

        # calculate loss
        loss = self.criterion(q_eval, q_target)

        # update weights
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        # Show progress
        #print('learn done -- [epsilon: {0}, loss: {1}]'.format(self.epsilon, loss))
        
        if self.epsilon > self.e_min:
            self.epsilon *= self.e_decay

        return round(self.epsilon,5) , round(loss.data.item(),5)


#=================================================
# Calculation class(3) : Agent
#=================================================
class Agent():
    # -----
    def __init__(self):

        # --------------------------------------------------
        # イプシロンの設定
        self.epsilon    = EPSILON     # ε-Greedy

        # ---------------------------
        # 記録用パラメタ類(プレイベース)
        self.arr_iplay     = []  # count game play    プレイ番号
        self.arr_maxturn   = []  # turn game play    ターン数
        self.arr_maxscore  = []  # rl_score game play    報酬の総和
        self.arr_victory   = []  # victory    勝利したか
        self.arr_loss      = []  # DQN-Experience Replay学習
        self.arr_epsilon   = []  # ε-Greedy        
        # ---------------------------
        # Q値の分析用(プレイベース)
        self.arr_numQV     = []  # QV値のN数
        self.arr_maxQV     = []  # QV値の最大値
        self.arr_q25QV     = []  # QV値の4分の1値
        self.arr_q75QV     = []  # QV値の4分の3値
        # ---------------------------
        # ユークリッド距離の分析用(プレイベース)
        self.arr_maxED     = []  # ユークリッド距離
        
        #print("starting simulation")
        # --------------------------------------------------
        # エピソードを実行する
        for iCnt_play in range(num_episodes):       # num_episodes
        
            # ゲームする
            maxturn, maxscore, flag_goal, maxEDist = self.get_episode(iCnt_play)

            # ----------
            # DQN-Experience Replay学習  
            val_epsilon, val_loss = dql_solver.replay_experience(BATCH_SIZE)
            # 結果の出力
            print("iCnt_play:{0}, maxturn:{1}, maxscore:{2}, flag_goal:{3}, epsilon:{4}, loss:{5}".format(iCnt_play, maxturn, maxscore, flag_goal, val_epsilon, val_loss))

            # ----------
            # 記録用パラメタ類(プレイベース)の追加
            self.arr_iplay.append(iCnt_play)        # count game play    プレイ番号
            self.arr_maxturn.append(maxturn)        # max_turn   ゲームのターン数
            self.arr_maxscore.append(maxscore)      # rl_score game play    最終プレイスコア
            self.arr_victory.append(flag_goal)      # victory    勝利したか
            self.arr_loss.append(val_loss)          # DQN-Experience Replay学習
            self.arr_epsilon.append(val_epsilon)    # イプシロン       
            # ----------
            # Q値の保管(プレイベース)
            self.arr_numQV.append(maxturn)      # QV値のN数
            self.arr_maxQV.append(np.max(self.arr_predQV))                 # QV値の最大値
            self.arr_q25QV.append(np.percentile(self.arr_predQV, q=25))    # QV値の4分の1値
            self.arr_q75QV.append(np.percentile(self.arr_predQV, q=75))    # QV値の4分の3値
            # ユークリッド距離の分析用(プレイベース)
            self.arr_maxED.append(maxEDist)     # ユークリッド距離
            # ----------
            # しばらくすれば表示が消えます
            if iCnt_play%50 == 0:
                time.sleep(SLEEP_TIME)
                clear_output(wait=True)

        # --------------------------------------------------
        # 学習履歴を出力する
        fig = plt.figure(figsize=(14, 6))
        ax1 = fig.add_subplot(1, 2, 1)
        ax1.set_title('learning transition : epsilon')
        ax1.plot(self.arr_iplay, self.arr_epsilon, label="epsilon", color="blue")
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('epsilon')
        ax1.legend(loc='best')
        # -----
        ax2 = fig.add_subplot(1, 2, 2)
        ax2.set_title('learning transition : Learning loss')
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('Loss amount')
        ax2.plot(self.arr_iplay, self.arr_loss, label="loss", color="blue")
        ax2.legend(loc='best')
        plt.show()

        # --------------------------------------------------
        # グラフを表示する
        self.show_graph()
        

    # --------------------------------------------------
    # エピソードを運用する
    def get_episode(self, iCnt_play):

        # ---------------------------
        # 機械学習用のパラメタ
        a_row, a_col, Qvalue = -1, -1, 0
        reward, done, flag_goal = 0, False, False
        number_state, number_state_next, maxturn, maxscore = 0, 0, 0, 0
        state, state_next = np.zeros(51), np.zeros(51)
        arr_Dturn, arr_Dturn_next = torch.zeros(20000), torch.zeros(20000)

        # ---------------------------
        # 記録用パラメタ類(ターンベース)
        self.arr_iturn     = []    # ターン・カウンタリスト
        self.orders_row    = []    # 指示リスト(row)
        self.orders_col    = []    # 指示リスト(col)
        self.arr_orders    = []    # 指示リスト(アドレス)
        self.arr_acType    = []    # 指示のタイプ
        self.arr_scores    = []    # ゲームスコアリスト
        self.arr_dones     = []    # ゲームオーバーフラグ
        self.mx_state      = []    # 状態マトリックス
        self.arr_EuDist    = []    # ユーグリッド距離のリスト
        self.arr_predQV    = []    # Q値のリスト

        # ---------------------------
        # ゲームをリセットする
        number_state = env.reset()
        # 状態(state)を数字->ONEHOT(48)に変更する
        temp_state = calc_address(number_state)

        # ---------------------------
        # 学習の進捗を表示する（一次元アドレスと二次元アドレスに変換する）
        a_row, a_col = calc_XYcoord(number_state) 
        #print("iCnt_play:{}, iCnt_turn:{}, a_row:{}, a_col:{}, a_order:{}, reward:{}".format(iCnt_play, iCnt_turn, a_row, a_col, a_order, reward))

        # ---------------------------
        # スタートとゴールからの距離を計算する
        EuDist_S = np.sqrt((a_row - S[0])**2 + (a_col - S[1])**2)
        EuDist_G = np.sqrt((a_row - G[0])**2 + (a_col - G[1])**2)
        EuDist_T = EuDist_S + EuDist_G

        # ---------------------------
        arr_Dturn[0]    = EuDist_T
        arr_Dturn_next  = arr_Dturn

        # =================================================
        # ユーグリッド距離累積ベクトルを生成する(0のとき)
        # =================================================
        arr_Dbase, arr_Dbase_next = torch.zeros(20000), torch.zeros(20000)
        temp_Dbase   = [11,12.60180166,11.91464985,11.6241438,11.49351226,11.4309124,11.40481169,11.40481169,11.4309124,11.49351226,11.6241438,11.91464985,12.60180166,11]
        arr_Dbase[0] = temp_Dbase[0]
        arr_Dbase_next  = arr_Dbase

        # ---------------------------
        # マンハッタン距離を計算する
        L1_dist = torch.nn.L1Loss()
        # calculate distance
        #print("arr_Dturn: ",arr_Dturn)
        #print("arr_Dbase: ",arr_Dbase)
        MH_dist = L1_dist(arr_Dturn, arr_Dbase)

        # ---------------------------
        # 状態(STATE_NEXT)を生成する
        state = np.hstack([temp_state, [MH_dist, EuDist_S, EuDist_G]])

        # ---------------------------
        # ゲームをプレイする
        iCnt_turn = 0
        while True:

            # 命令(a_order)と状態(state)を作成する
            a_order, acType, Qvalue = dql_solver.choose_action(state, number_state, iCnt_turn)

            # ---------------------------
            # ゲームをもう一歩進める
            number_state_next, reward, done, _ = env.step(a_order)
            temp_state_next = calc_address(number_state_next)

            # ---------------------------
            # 学習の進捗を表示する（一次元アドレスと二次元アドレスに変換する）
            a_row_next, a_col_next = calc_XYcoord(number_state_next) 
            #print("iCnt_play:{}, iCnt_turn:{}, a_row:{}, a_col:{}, a_order:{}, reward:{}".format(iCnt_play, iCnt_turn, a_row, a_col, a_order, reward))

            # ---------------------------
            # スタートとゴールからの距離を計算する
            EuDist_S_next = np.sqrt((a_row_next - S[0])**2 + (a_col_next - S[1])**2)
            EuDist_G_next = np.sqrt((a_row_next - G[0])**2 + (a_col_next - G[1])**2)
            EuDist_T_next = EuDist_S_next + EuDist_G_next

            # ---------------------------
            # [計測]ユーグリッド累積距離ベクトルを計算する(iCnt_turn+1のとき)
            arr_Dturn_next[iCnt_turn + 1] = EuDist_T_next

            # [基準]ユーグリッド累積距離ベクトルを計算する(iCnt_turn+1のとき)
            if iCnt_turn < len(temp_Dbase)-1:
                arr_Dbase_next[iCnt_turn + 1]  = temp_Dbase[iCnt_turn + 1]
            else:
                arr_Dbase_next[iCnt_turn + 1]  = temp_Dbase[len(temp_Dbase)-1]

            # ---------------------------
            # マンハッタン距離を計算する
            MH_dist_next = L1_dist(arr_Dturn, arr_Dbase)

            # ---------------------------
            # 状態(STATE_NEXT)を生成する
            state_next = np.hstack([temp_state_next, [MH_dist_next, EuDist_S_next, EuDist_G_next]])

            # ---------------------------
            # 記録用リストを追記する
            self.arr_iturn.append(iCnt_turn)   # ターン・カウンタリスト
            self.orders_row.append(a_row)       # 指示リスト(ROW)
            self.orders_col.append(a_col)       # 指示リスト(COL)
            self.arr_orders.append(a_order)     # 指示リスト(アドレス)
            self.arr_acType.append(acType)      # 指示のタイプ
            self.arr_scores.append(reward)      # ゲームスコアリスト   
            self.arr_dones.append(done)     # ゲームオーバーフラグ
            if iCnt_turn == 0:
                self.mx_state       = [state]            # 状態マトリックス(Current)  
                self.mx_state_next  = [state_next]       # 状態マトリックス(Next) 
            else:
                self.mx_state = np.concatenate([self.mx_state, [state]], axis=0)       # 状態マトリックス(Current)   
                self.mx_state_next = np.concatenate([self.mx_state_next, [state_next]], axis=0) # 状態マトリックス(Next) 
            self.arr_EuDist.append(EuDist_T)    # ユーグリッド距離のリスト
            self.arr_predQV.append(Qvalue)      # Q値のリスト
            # ----------------
            # Experience Replay配列に保管する
            dql_solver.remember_memory(state, a_order, reward, state_next, number_state_next, done)
            # ----------------
            # 実行継続するか分岐
            if iCnt_play < 100 and (done == True or iCnt_turn > 19995):
                # Game over
                break # (game over rip)
            elif iCnt_play >= 100 and (done == True or iCnt_turn > 19995 or reward < -90):
                # Game over
                break # (game over rip)
            else:
                # count of game turn
                a_row   = a_row_next
                a_col   = a_col_next
                EuDist_S   = EuDist_S_next
                EuDist_G   = EuDist_G_next
                EuDist_T   = EuDist_T_next
                number_state  = number_state_next
                state      = state_next
                iCnt_turn  = iCnt_turn + 1

        # --------------------------------------------------
        # ユーグリッド距離のリスト
        maxEDist   = np.amax(self.arr_EuDist)
        # ゲームが完了しているのか
        flag_goal  = done
        # プレイデータの引継ぎ
        maxturn, maxscore = iCnt_turn, np.sum(self.arr_scores)

        return maxturn, maxscore, flag_goal, maxEDist

    # ----------------
    # 学習結果のグラフ化
    def show_graph(self):

        fig2 = plt.figure(figsize=(14, 12))
        # -----
        ax1 = fig2.add_subplot(2, 2, 1)
        ax1.set_title('DQN parameter transition: QValue')
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('QValue')
        ax1.plot(self.arr_iplay, self.arr_maxQV, label="max_QV", color="blue")
        ax1.plot(self.arr_iplay, self.arr_q25QV, label="q25_QV", color="red")
        ax1.plot(self.arr_iplay, self.arr_q75QV, label="q75_QV", color="green")
        ax1.grid(True)
        ax1.legend(loc='best')
        # -----
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_maxED).rolling(window=12, center=True).mean()
        ax2 = fig2.add_subplot(2, 2, 2)
        ax2.set_title('learning transition : maxEDist')
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('maxEDist')
        ax2.grid(True)
        ax2.plot(self.arr_iplay, self.arr_maxED, label="original", color="blue")
        ax2.plot(self.arr_iplay, y_rolling, label="moving", color="red")
        ax2.legend(loc='best')
        # -----
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_maxturn).rolling(window=12, center=True).mean()
        ax3 = fig2.add_subplot(2, 2, 3)
        ax3.set_title('learning transition : maxturn')
        ax3.set_xlabel('#episode')
        ax3.set_ylabel('maxturn')
        ax3.grid(True)
        ax3.plot(self.arr_iplay, self.arr_maxturn, label="original", color="blue")
        ax3.plot(self.arr_iplay, y_rolling, label="moving", color="red")
        ax3.legend(loc='best')
        # -----
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_maxscore).rolling(window=12, center=True).mean()
        ax4 = fig2.add_subplot(2, 2, 4)
        ax4.set_title('learning transition : maxscore')
        ax4.set_xlabel('#episode')
        ax4.set_ylabel('maxscore')
        ax4.grid(True)
        ax4.plot(self.arr_iplay, self.arr_maxscore, label="original", color="blue")
        ax4.plot(self.arr_iplay, y_rolling, label="moving", color="red")
        ax4.legend(loc='best')
        # -----
        fig2.tight_layout()
        #fig.savefig("./AAC_img.png")
        plt.show()

#=================================================
# main function            
#=================================================
if __name__ == "__main__":

    # ---------------------------
    # ゲームの初期化
    ROWS = 4
    COLS = 12
    S = (3, 0)
    G = (3, 11)

    # ---------------------------
    # ゲームの初期化
    env = CliffWalkingEnv()

    # ---------------------------
    # パラメタ設定の初期化
    state_size     = 48 + 3
    action_size    = 4
    dim_input      = 48 + 3 + 1
    dim_output     = 1

    # ---------------------------
    # ハイパーパラメタ
    BATCH_SIZE = 128*4                      # サンプルサイズ
    LRATE   = 0.005                         # 学習率
    EPSILON = 0.99                          # greedy policy
    EPDECAY = 0.999
    EPMIN   = 0.01
    GAMMA   = 0.999                         # reward discount
    TARGET_REPLACE_ITER = 100               # 目标网络更新频率
    MEMORY_CAPACITY = 128*8*4               # メモリ容量
    # ---------------------------
    SLEEP_TIME      = 0.01
    num_episodes    = 5000                  # 繰り返し回数

    # ---------------------------
    # フォルダ名の指定
    #foldername = "./ML_csvout/"  # My project folder

    # ---------------------------
    # dql_solverのインスタンス化
    dql_solver  = DQN_Solver(state_size, action_size)

    # ---------------------------
    # 出力用:Pytorchモデルのファイル名
    #comment_output_model = "initial"
    #code_output_model = "model_cliffwalk_DQNER_{0}.pt".format(comment_output_model)  # モデルの指定
    #file_output_model = foldername + code_output_model  # ファイルパス名の生成

    # ---------------------------
    # DQN-ERで学習する
    Agent()

```

QEU:FOUNDER ： “結果を前回と比較しましょう。”

**（マンハッタン距離）**

![imageRL3-4-2](/2022-07-06-QEUR21_RMCLF3/imageRL3-4-2.jpg)

**(SG距離)**

![imageRL3-4-3](/2022-07-06-QEUR21_RMCLF3/imageRL3-4-3.jpg)

D先生 : “そうですね、でもうまくいくかなぁ・・・。”

**（マンハッタン距離）**

![imageRL3-4-4](/2022-07-06-QEUR21_RMCLF3/imageRL3-4-4.jpg)

**(SG距離)**

![imageRL3-4-5](/2022-07-06-QEUR21_RMCLF3/imageRL3-4-5.jpg)

QEU:FOUNDER ： “この２つ・・・。はたして差があるのかなァ？”

D先生 : “今回のメトリックスを加えて良くなっているはずだと思います。でも、よくわかりませんね・・・（笑）。”

QEU:FOUNDER ： “さっき、何か言いかけたことありませんでしたか？”

**（再掲）**

- **当初　：　ゲーム盤の大きさ（48）**
- **現在　：　ゲーム盤の大きさ（48）　+　マンハッタン距離(1)　+　S距離(1)　+　G距離(1)**

D先生 : “ニューラルネットのストラクチャの定義において、ゲーム盤のために48次元もとっているのは、無駄だなァと・・・。”

## ～　まとめ　～

### ・・・　つづきです　・・・

QEU:FOUNDER ： “あの人、また動画を出したよ・・・。”

[![MOVIE1](http://img.youtube.com/vi/SY32qkiWuhQ/0.jpg)](http://www.youtube.com/watch?v=SY32qkiWuhQ "【れいわに強力な助っ人登場!!】もはやユーチューバーの域を超えた？ついに覚醒した伊達一詔の応援演説とは？（れいわ新選組 参院選 全国比例 大島九州男）")

D先生 : “「草の根」としてのTuberは重要であることはよくわかりました。「切り取り画像」も立派なものです。さすがに、これは紹介しないけど・・・。”

![imageRL3-4-6](/2022-07-06-QEUR21_RMCLF3/imageRL3-4-6.jpg)

QEU:FOUNDER ： “もう時間がないよなァ・・・。ターゲットを変えないとね・・・。もうターゲットは与党じゃないでしょ・・・。”

D先生 : “じゃあ、なにがターゲットなんですか？“

[![MOVIE2](http://img.youtube.com/vi/pKxf_tqpYjI/0.jpg)](http://www.youtube.com/watch?v=pKxf_tqpYjI "親は子どものお手伝いさん。いつもお手伝いありがとう。とても嬉しいです〜と子どもに言われて親は一人前。あるべき政治の未来について語ります。福島瑞穂社民党党首＆安冨歩東大教授。一月万冊")

QEU:FOUNDER ： “申し訳ないが、少なくとも今回は**この党（↑）に票を出すのは無駄じゃないのか？**いずれ消滅するんでしょ？”

D先生 : “イケメンの党との違いが、投票者の年齢層以外によくわかりません。”

[![MOVIE3](http://img.youtube.com/vi/4qTiRGmsPDQ/0.jpg)](http://www.youtube.com/watch?v=4qTiRGmsPDQ "【政見放送】西みゆか【埼玉選挙区】れいわ新選組公認候補")

QEU:FOUNDER ： “だから小生は当初から、この人（↑）に注目していたんです。ちゃんと**「後継できるだろうな」**って・・・。こういう部分を大きくアピールしてほしいよね。もちろん、こういうアピールは難しいけど・・・。”

D先生 : “Y先生は**「人は生き残るべきで、組織は消滅してもかまわない」**と言っていました。それは正しいですよね。”

QEU:FOUNDER ： “ちゃんとやれば、組織は消滅しても人はますます輝けると思いますけどね。”
