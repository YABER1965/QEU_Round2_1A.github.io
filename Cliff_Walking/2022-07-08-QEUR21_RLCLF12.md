---
title: QEUR21_RLCLF12 – 「崖歩き(Cliff_Walking)」でAACを使う(その2)
date: 2022-07-08
tags: ["QEUシステム", "メトリックス", "Python言語", "強化学習", "Cliff_Walking", "ディープラーニング"]
excerpt: Python言語によるCliff_Walkingの強化学習
---

## QEUR21_RLCLF12 – 「崖歩き(Cliff_Walking)」でAACを使う(その2)

## ～　つぎのステップへ、行くの・・・？　～

QEU:FOUNDER ： “Cliff_Walkingのシリーズも最後です。状態(state)にメトリックスを入れ込んでみました。「アンチョコ」もつけて・・・。”

D先生 : “えっ？アンチョコもつけるの・・・？”

QEU:FOUNDER ： “プログラムをドン！”

```python
# ----------------
# Cliff WalkingゲームのAAC学習システム
# step12 : より高度のシステム構築
# step12 : ディープラーニングでプレイデータをAACで学習させる
# step12 : AAC_cliffwalking_Simu(metrics_tith tips).py
# 注意：　アンチョコを駆使して遊びましょう！
# ---------------- 
# 数値計算のﾗｲﾌﾞﾗﾘ読み込み
import math
import numpy as np
import random
import itertools
#from collections import namedtuple, defaultdict
# -----
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
# -----
import matplotlib.pyplot as plt
#import seaborn as sns
#%matplotlib inline

# =================================================
# difinition of function
# =================================================
# 状態の表記を変更(oneHOTへ)
def get_screen(L1_dist, number_state, tsr_Dturn, tsr_Dbase, iCnt_turn):

    # ---------------------------
    # 状態を初期化する
    state = np.zeros(in_dim)
    temp_Dbase   = [11,12.60180166,11.91464985,11.6241438,11.49351226,11.4309124,11.40481169,11.40481169,11.4309124,11.49351226,11.6241438,11.91464985,12.60180166,11]

    # ---------------------------
    # 学習の進捗を表示する（一次元アドレスと二次元アドレスに変換する）
    a_row, a_col = calc_XYcoord(number_state) 
    #print("iCnt_play:{}, iCnt_turn:{}, a_row:{}, a_col:{}, a_order:{}, reward:{}".format(iCnt_play, iCnt_turn, a_row, a_col, a_order, reward))

    # ---------------------------
    # スタートとゴールからの距離を計算する
    EuDist_S = np.sqrt((a_row - S[0])**2 + (a_col - S[1])**2)
    EuDist_G = np.sqrt((a_row - G[0])**2 + (a_col - G[1])**2)
    EuDist_T = EuDist_S + EuDist_G

    # ---------------------------
    # tsr_Dbaseを計算する
    if iCnt_turn <= len(temp_Dbase)-1:
        tsr_Dbase[iCnt_turn]  = temp_Dbase[iCnt_turn]
    else:
        tsr_Dbase[iCnt_turn]  = temp_Dbase[len(temp_Dbase)-1]

    # tsr_Dturnを計算する
    tsr_Dturn[iCnt_turn]    = EuDist_T

    # ---------------------------
    # マンハッタン距離を計算する
    MH_dist = L1_dist(tsr_Dturn, tsr_Dbase)
    #print("tsr_Dturn: ",tsr_Dturn)
    #print("tsr_Dbase: ",tsr_Dbase)

    # メトリックスのリストを生成する
    list_metrics = [a_row, a_col, MH_dist, EuDist_S, EuDist_G]

    # ---------------------------
    # 状態(STATE)を生成する
    state[number_state] = 1.0
    state[in_dim - 3] = MH_dist
    state[in_dim - 2] = EuDist_S
    state[in_dim - 1] = EuDist_G
    
    # メトリックスのリストを生成する
    tsr_state = torch.tensor(state).float()

    return tsr_state, tsr_Dbase, tsr_Dturn, list_metrics

# 状態の表記を変更(2桁のアドレス番号からi,jへ)
def calc_XYcoord(number_st):

    i = int(number_st/COLS + 0.01)
    j = int(number_st - i*COLS + 0.01)

    return int(i), int(j)

#=================================================
# Game Environment class
#=================================================
# Cliff_Walkingゲームを動かす（環境）
class Cliff:

    def __init__(self):
        self.end = False
        self.pos = S
        self.board = np.zeros([4, 12])
        # add cliff marked as -1
        self.board[3, 1:11] = -1

    def nxtPosition(self, action):
        if action == 0: # 上向き
            nxtPos = (self.pos[0] - 1, self.pos[1])
        elif action == 2: # 下向き
            nxtPos = (self.pos[0] + 1, self.pos[1])
        elif action == 3: # 左向き
            nxtPos = (self.pos[0], self.pos[1] - 1)
        else: # 右向き
            nxtPos = (self.pos[0], self.pos[1] + 1)
        # check legitimacy
        if nxtPos[0] >= 0 and nxtPos[0] <= 3:
            if nxtPos[1] >= 0 and nxtPos[1] <= 11:
                self.pos = nxtPos
        # -----
        if self.pos == G:
            self.end = True
            #print("Game ends reaching goal")
        if self.board[self.pos] == -1:
            #self.end = True
            self.pos = S
            #print("Game ends falling off cliff and return to Start")
        # -----
        state = 12*self.pos[0] + self.pos[1]

        return self.pos, state, self.end

    def giveReward(self):
        # give reward
        val_reward = -1
        if self.pos == G:
            val_reward = -1
        elif self.board[self.pos] == 0:
            val_reward = -1
        elif self.board[self.pos] == -1:
            val_reward = -100

        return val_reward


#=================================================
# Calculation class(1) : Actor_Critic
#=================================================
class ActorCriticModel(nn.Module):

    def __init__(self):
        super(ActorCriticModel, self).__init__()
        self.fc1 = nn.Linear(in_dim, 64)
        self.fc2 = nn.Linear(64, 32)
        self.action = nn.Linear(32, out_dim)
        self.value = nn.Linear(32, 1)
        
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        action_probs = F.softmax(self.action(x), dim=-1)
        state_values = self.value(x)
        return action_probs, state_values


#=================================================
# Calculation class(2) : Agent
#=================================================
class Agent():
    # -----
    def __init__(self):

        # インスタンス化
        # 環境とエージェント
        self.cliff = Cliff()
        ActorCritic = ActorCriticModel()
        self.optimizer = torch.optim.Adam(ActorCritic.parameters(), learning_rate)  # 学習率の大きさは外で定義
        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=10, gamma=0.98)

        # マンハッタン距離を計算する
        self.L1_dist = torch.nn.L1Loss()

        # ゲームを実行する
        self.get_episode(ActorCritic)

        # ----------------
        # 学習曲線のグラフ化
        fig = plt.figure(figsize=(14, 6))
        # -----
        ax1 = fig.add_subplot(1, 2, 1)
        ax1.set_title('learning transition : loss_actor')
        ax1.set_xlabel('episode')
        ax1.set_ylabel('loss')
        ax1.grid(True)
        ax1.plot(self.arr_episode, self.loss_actors)
        # -----
        ax2 = fig.add_subplot(1, 2, 2)
        ax2.set_title('learning transition : loss_critic')
        ax2.set_xlabel('episode')
        ax2.set_ylabel('loss')
        ax2.grid(True)
        ax2.plot(self.arr_episode, self.loss_critics)
        # -----
        fig.tight_layout()
        #fig.savefig("./AAC_img.png")
        plt.show()

        # グラフを表示する
        self.show_graph()

        # ---------------------------
        # モデルの保存
        #torch.save(ActorCritic.state_dict(), file_output_model)

    # -----
    def get_episode(self, ActorCritic):
    
        # 配列の初期化
        tsr_Dturn, tsr_Dturn_next = torch.zeros(10000), torch.zeros(10000)
        tsr_Dbase, tsr_Dbase_next = torch.zeros(10000), torch.zeros(10000)

        # reward、length、lossの変化を記録する
        # リストの初期化
        self.arr_episode       = np.zeros(num_episodes + 1)
        self.episode_lengths   = np.zeros(num_episodes + 1)
        self.episode_rewards   = np.zeros(num_episodes + 1)
        self.loss_actors       = np.zeros(num_episodes + 1)       
        self.loss_critics      = np.zeros(num_episodes + 1)
        # -----
        self.arr_maxMHdist    = np.zeros(num_episodes + 1)
        self.arr_maxGDist     = np.zeros(num_episodes + 1)
        
        # -----
        for i_episode in range(1, num_episodes + 1):
        
            # ----------
            # 環境をﾘｾｯﾄする
            self.cliff.pos, number_state = self.reset()  # 環境をﾘｾｯﾄする
            # メトリックスを生成する(Current)
            get_state, tsr_Dbase, tsr_Dturn, list_metrics = get_screen(self.L1_dist, number_state, tsr_Dturn, tsr_Dbase, 0)  # state値をone-hotのtensorに変換する

            # メトリックスのリストを生成する(Current)
            a_row = list_metrics[0]
            a_col = list_metrics[1]
            MH_dist = list_metrics[2]
            EuDist_S = list_metrics[3]
            EuDist_G = list_metrics[4]

            # ----------
            log_probs = []
            rewards = []
            state_values = []
            MHdist_values = []
            GDist_values = []
            # ----------
            for t in itertools.count():
            
                action_probs, state_value = ActorCritic(get_state.squeeze(0))  # stateにおける各種actionの確率
                action = torch.multinomial(action_probs, 1).item()  # actionを一つ選ぶ
                log_prob = torch.log(action_probs[action])

                # アンチョコ(1)
                arr_actions = [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]
                
                if i_episode == 0 or i_episode == 5 or i_episode == 10:
                    if t < len(arr_actions):
                        action = arr_actions[t]

                # アンチョコ(2)
                arr_actions2 = [0, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2]
                
                if i_episode == 1 or i_episode == 7 or i_episode == 13:
                    if t < len(arr_actions2):
                        action = arr_actions2[t]

                # ---------------------------
                # ゲームを実行する
                self.cliff.pos, number_state_next, done = self.cliff.nxtPosition(action)
                reward = self.cliff.giveReward()       
                reward = torch.tensor([reward], device=device)

                # 統計データを計算する(reward, length)
                self.episode_rewards[i_episode] += reward  # 累計報酬を計算する

                # メトリックスを生成する(Next)
                get_state_next, tsr_Dbase_next, tsr_Dturn_next, list_metrics_next = get_screen(self.L1_dist, number_state_next, tsr_Dturn, tsr_Dbase, t+1)  # state値をone-hotのtensorに変換する

                # メトリックスのリストを生成する(Next)
                a_row_next = list_metrics_next[0]
                a_col_next = list_metrics_next[1]
                MH_dist_next = list_metrics_next[2]
                EuDist_S_next = list_metrics_next[3]
                EuDist_G_next = list_metrics_next[4]

                # データをListに保管する
                log_probs.append(log_prob.view(-1))
                rewards.append(reward)
                state_values.append(state_value)
                MHdist_values.append(MH_dist)
                GDist_values.append(EuDist_G)
            
                # -----
                if done or t > 4999:  # ゲームを1回完了後に学習を行う
                
                    returns, Gt, pw = [], 0, 0
                    # print(rewards)
                    for reward in rewards[::-1]:
                        # Gt += (gamma ** pw) * reward
                        Gt = Gt + (gamma ** pw) * reward  
                        # print(Gt)
                        pw += 1
                        returns.append(Gt)

                    returns = returns[::-1]
                    returns = torch.cat(returns)
                    returns = (returns - returns.mean()) / (returns.std() + 1e-9)
                    # print(returns)
                    log_probs = torch.cat(log_probs)
                    state_values = torch.cat(state_values)
                    # print(returns)
                    # print(log_probs)
                    # print(state_values)

                    advantage = returns.detach() - state_values
                    critic_loss = F.smooth_l1_loss(state_values, returns.detach())
                    actor_loss = (-log_probs * advantage.detach()).mean()
                    loss = critic_loss + actor_loss

                    # 統計データを計算する(loss_actor, loss_critic)
                    self.arr_episode[i_episode]     = i_episode  # ゲーム番号
                    self.loss_actors[i_episode]     = critic_loss  # クリティックの損失
                    self.loss_critics[i_episode]    = actor_loss  # アクターの損失
                    self.episode_lengths[i_episode] = t  # 繰り返し回数を入力する
                    # -----
                    self.arr_maxMHdist[i_episode]   = np.amax(MHdist_values)
                    self.arr_maxGDist[i_episode]    = np.amax(GDist_values)
        
                    # criticを更新する
                    self.optimizer.zero_grad()
                    loss.backward()
                    self.optimizer.step()
                    if i_episode%10 == 0:
                        print('Episode: {}, total steps: {}'.format(i_episode, t))
                    if i_episode == 0 or ( i_episode > 20 and t < 20 ):
                        self.scheduler.step()
                    break

                else:
                    tsr_Dbase = tsr_Dbase_next
                    tsr_Dturn = tsr_Dturn_next
                    a_row = a_row_next
                    a_col = a_col_next
                    MH_dist = MH_dist_next
                    EuDist_S = EuDist_S_next
                    EuDist_G = EuDist_G_next
                    get_state = get_state_next

    # ----------------
    # ゲームをﾘｾｯﾄする
    def reset(self):   
        # ----------
        self.cliff = Cliff()
        return S, 36

    # ----------------
    # パフォーマンス推移のグラフ化
    def show_graph(self):

        fig2 = plt.figure(figsize=(14, 12))
        # -----
        ax1 = fig2.add_subplot(2, 2, 1)
        ax1.set_title('learning transition : Manhattan_distance')
        ax1.set_xlabel('episode')
        ax1.set_ylabel('distance')
        ax1.grid(True)
        ax1.plot(self.arr_episode, self.arr_maxMHdist)
        # -----
        ax2 = fig2.add_subplot(2, 2, 2)
        ax2.set_title('learning transition : Goal_distance')
        ax2.set_xlabel('episode')
        ax2.set_ylabel('distance')
        ax2.grid(True)
        ax2.plot(self.arr_episode, self.arr_maxGDist)
        # -----
        ax3 = fig2.add_subplot(2, 2, 3)
        ax3.set_title('learning transition : length')
        ax3.set_xlabel('episode')
        ax3.set_ylabel('length')
        ax3.grid(True)
        ax3.plot(self.arr_episode, self.episode_lengths)
        # -----
        ax4 = fig2.add_subplot(2, 2, 4)
        ax4.set_title('learning transition : reward')
        ax4.set_xlabel('episode')
        ax4.set_ylabel('reward')
        ax4.grid(True)
        ax4.plot(self.arr_episode, self.episode_rewards)
        # -----
        fig2.tight_layout()
        #fig.savefig("./AAC_img.png")
        plt.show()


#=================================================
# main function            
#=================================================
if __name__ == "__main__":

    # ---------------------------
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    #print(device)

    # ----------
    # 環境の導入
    in_dim  = 48 + 3
    out_dim = 4

    # ---------------------------
    num_episodes    = 3000
    learning_rate   = 0.01
    gamma   = 0.98
    ROWS    = 4
    COLS    = 12
    S   = (3, 0)
    G   = (3, 11)

    # ---------------------------
    # フォルダ名の指定
    foldername = "./"  # My project folder

    # ---------------------------
    # 出力用Pytorchモデルのファイル名
    comment_output_model = "cliffwalking_AAC"
    code_output_model = "model_cliffwalking_AAC.pt"
    file_output_model = foldername + code_output_model  # ファイルパス名の生成

    # ---------------------------
    # 学習の実行
    Agent()

```

D先生 : “プログラムにはアンチョコが2か所ありますね・・・。”

QEU：FOUNDER ： “‘まずは、アンチョコがない状態でやってみましょう。”

**（学習曲線）**

![imageRL3-12-1](/2022-07-08-QEUR22_RLCLF12/imageRL3-12-1.jpg)

**（パフォーマンス推移）**

![imageRL3-12-2](/2022-07-08-QEUR22_RLCLF12/imageRL3-12-2.jpg)

D先生 : “このプログラムはターンの上限を5000回にしているんですよね。

QEU:FOUNDER ： “学習の初めのエピソードではターン数5000回が連発しているでしょう。その後は安定しているけど・・・。つぎは、アンチョコを1件だけ加えてみましょう。ベストシナリオを3回だけ強制注入しています。”

**（学習曲線）**

![imageRL3-12-3](/2022-07-08-QEUR22_RLCLF12/imageRL3-12-3.jpg)

**（パフォーマンス推移）**

![imageRL3-12-4](/2022-07-08-QEUR22_RLCLF12/imageRL3-12-4.jpg)

D先生 : “やっぱり、アンチョコは効くなァ・・・。”

QEU:FOUNDER ： “Cliff_Walkingやメイズのように最適解がわかっている環境では使わない「テ」はないよねぇ・・・。最後にアンチョコを2つ使ってみましょうか・・・。合計6回の強制的な命令注入です。”

**（学習曲線）**

![imageRL3-12-5](/2022-07-08-QEUR22_RLCLF12/imageRL3-12-5.jpg)

**（パフォーマンス推移）**

![imageRL3-12-6](/2022-07-08-QEUR22_RLCLF12/imageRL3-12-6.jpg)

D先生 : “初期のターン数をみればわかります、学習の収束がメチャ早くなります。こういう「反則技」は教科書ではさすがに話しません (笑)。あの・・・、もうひとつやってみたいことがあります。”

QEU:FOUNDER ： “もう見当がつきます、どうぞ・・・。”

- **当初 ： ゲーム盤の大きさ（48）**
- **現在 ： ゲーム盤の大きさ（48） + マンハッタン距離(1) + S距離(1) + G距離(1)**

D先生 : “現在の状態(STATE)の次元は51次元です。ゲーム盤を48次元からXY座標の2次元にしたらどうなりますか？AACは超安定のスキームですから、やれないことはないかもしれないと思って・・・。”

## ～　まとめ　～

### ・・・　生き残るか？イケメン？？　・・・

QEU:FOUNDER ： “いやぁ・・・、どうなるんだろうね。”

![imageRL3-12-7](/2022-07-08-QEUR22_RLCLF12/imageRL3-12-7.jpg)

C部長 : “全く、先が読めません。”

[![MOVIE1](http://img.youtube.com/vi/yhh8NcGPfKU/0.jpg)](http://www.youtube.com/watch?v=yhh8NcGPfKU "【れいわの国会質問 ダイジェスト】ややこしい人たちを国会に送り込むとどうなる？【れいわ新選組】")

D先生 : “そういえば前回もドラマティックでしたよね？”

![imageRL3-12-8](/2022-07-08-QEUR22_RLCLF12/imageRL3-12-8.jpg)

QEU:FOUNDER ： “イケメンの党も、あれから強化されているはずなのだが、一向に楽になっていないという現実・・・。”

D先生 : “あぁ・・・、選挙に出る人を尊敬するわ・・・。あっ、すべての候補者に対してね・・・。”
