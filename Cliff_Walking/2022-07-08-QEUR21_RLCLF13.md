---
title: QEUR21_RLCLF13 – 「崖歩き(Cliff_Walking)」でAACを使う(その3)
date: 2022-07-08
tags: ["QEUシステム", "メトリックス", "Python言語", "強化学習", "Cliff_Walking", "ディープラーニング"]
excerpt: Python言語によるCliff_Walkingの強化学習
---

## QEUR21_RLCLF13 – 「崖歩き(Cliff_Walking)」でAACを使う(その3)

## ～　「メイズ（迷路）」プロジェクトへの知見を得る・・・　～

QEU:FOUNDER ： “Cliff_Walkingのシリーズも最後です。状態(state)をシンプル化します。今回が（シリーズ）最後でいいよね？”

- **前回 ： ゲーム盤の大きさ（48） + マンハッタン距離(1) + S距離(1) + G距離(1)**
- **今回 ： xy座標（2） + マンハッタン距離(1) + S距離(1) + G距離(1)**

D先生 : “う～ん。。。まあ。。。いいですよ・・・。”

QEU:FOUNDER ： “じゃあ、プログラムの変更部分だけをドン！”

```python
# ----------------
# Cliff WalkingゲームのAAC学習システム
# step13 : より高度のシステム構築
# step13 : ディープラーニングでプレイデータをAACで学習させる
# step13 : AAC_cliffwalking_Simu(simple).py
# 注意：　STATEは強烈に単純化、アンチョコは強化されているよ
# ---------------- 
# 数値計算のﾗｲﾌﾞﾗﾘ読み込み
import math
import numpy as np
import random
import itertools
#from collections import namedtuple, defaultdict
# -----
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
# -----
import matplotlib.pyplot as plt
#import seaborn as sns
#%matplotlib inline

# =================================================
# difinition of function
# =================================================
# 状態の表記を変更(oneHOTへ)
def get_screen(L1_dist, number_state, tsr_Dturn, tsr_Dbase, iCnt_turn):

    # ---------------------------
    # 状態を初期化する
    state = np.zeros(in_dim)
    temp_Dbase   = [11,12.60180166,11.91464985,11.6241438,11.49351226,11.4309124,11.40481169,11.40481169,11.4309124,11.49351226,11.6241438,11.91464985,12.60180166,11]

    # ---------------------------
    # 学習の進捗を表示する（一次元アドレスと二次元アドレスに変換する）
    a_row, a_col = calc_XYcoord(number_state) 
    #print("iCnt_play:{}, iCnt_turn:{}, a_row:{}, a_col:{}, a_order:{}, reward:{}".format(iCnt_play, iCnt_turn, a_row, a_col, a_order, reward))

    # ---------------------------
    # スタートとゴールからの距離を計算する
    EuDist_S = np.sqrt((a_row - S[0])**2 + (a_col - S[1])**2)
    EuDist_G = np.sqrt((a_row - G[0])**2 + (a_col - G[1])**2)
    EuDist_T = EuDist_S + EuDist_G

    # ---------------------------
    # tsr_Dbaseを計算する
    if iCnt_turn <= len(temp_Dbase)-1:
        tsr_Dbase[iCnt_turn]  = temp_Dbase[iCnt_turn]
    else:
        tsr_Dbase[iCnt_turn]  = temp_Dbase[len(temp_Dbase)-1]

    # tsr_Dturnを計算する
    tsr_Dturn[iCnt_turn]    = EuDist_T

    # ---------------------------
    # マンハッタン距離を計算する
    MH_dist = L1_dist(tsr_Dturn, tsr_Dbase)
    #print("tsr_Dturn: ",tsr_Dturn)
    #print("tsr_Dbase: ",tsr_Dbase)

    # メトリックスのリストを生成する
    list_metrics = [a_row, a_col, MH_dist, EuDist_S, EuDist_G]

    # ---------------------------
    # 状態(STATE)を生成する
    state[0] = a_row
    state[1] = a_col
    state[2] = MH_dist
    state[3] = EuDist_S
    state[4] = EuDist_G
    
    # メトリックスのリストを生成する
    tsr_state = torch.tensor(state).float()

    return tsr_state, tsr_Dbase, tsr_Dturn, list_metrics

中略


                # ------
                # 学習を早くするためのアンチョコ
                if t < 100:
                    epsilon = 0.8
                elif t >= 100 and t < 300:
                    epsilon = 0.5  
                elif t >= 300 and t < 1000:
                    epsilon = 0.3  
                else:
                    epsilon = 0.01              
                # ----------
                # 移動命令
                #	0, UP
                #	1, RIGHT
                #	2, DOWN
                #	3, LEFT
                # ----------
                if np.random.random() < epsilon:
                    # 状態の表記を変更(2桁番号からi,jへ)
                    if a_col == 0:
                        action = 0
                    elif a_row == 0:
                        action = 2  
                    elif a_col == 11:
                        action = 2  
                    else:
                        action = 1  
                #print(state)

```

D先生 : “アンチョコはかなり強化されています。さすがに、これで簡単に学習収束するでしょう・・・。”

QEU：FOUNDER ： “さぁて・・・、どうかな？結果をドン！！”

**（学習曲線）**

![imageRL3-13-1](/2022-07-08-QEUR21_RLCLF13/imageRL3-13-1.jpg)

**（パフォーマンス推移）**

![imageRL3-13-2](/2022-07-08-QEUR21_RLCLF13/imageRL3-13-2.jpg)

D先生 : “5次元の状態(STATE)表現ではうまく学習収束できないことを、とても素直に表現しています（笑）。

QEU:FOUNDER ： “良い実験であったと・・・。じゃあ、この結果でD先生も満足しましたか？”

- **前回 ： xy座標（2） + マンハッタン距離(1) + S距離(1) + G距離(1)**
- **今回 ： xy座標（2） + RTメトリックス(2) + S距離(1) + G距離(1)**

D先生 : “もう一本、（実験を）お願いします・・・。マンハッタン距離の代わりに**RTメトリックス（感度、SN比）**に変えられませんか？”

![imageRL3-13-3](/2022-07-08-QEUR21_RLCLF13/imageRL3-13-3.jpg)

QEU:FOUNDER ： “じゃあ、やってみましょう。プログラムの変更部分のみを紹介します。”

```python
# newRTメトリックスを計算する(テンソル活用版)
def calc_newRT(L1_loss, tsr_sig_array, tsr_tani_array): 

    y = tsr_sig_array
    x = tsr_tani_array

    xx = torch.dot(x,x)
    xy = torch.dot(x,y)
    beta = xy/xx
    #print("i:{}, beta:{}".format(i,beta))

    mDistance   = L1_loss(y, beta*x)
    #print("i:{}, beta:{}".format(i,beta))
    #print("mDistance: ", mDistance.item())
    #print("yres:{}".format(yres))

    return beta.item(), mDistance.item()


# 状態の表記を変更(oneHOTへ)
def get_screen(L1_loss, number_state, tsr_Dturn, tsr_Dbase, iCnt_turn):

    # ---------------------------
    # 状態を初期化する
    state = np.zeros(in_dim)
    temp_Dbase   = [11,12.60180166,11.91464985,11.6241438,11.49351226,11.4309124,11.40481169,11.40481169,11.4309124,11.49351226,11.6241438,11.91464985,12.60180166,11]

    # ---------------------------
    # 学習の進捗を表示する（一次元アドレスと二次元アドレスに変換する）
    a_row, a_col = calc_XYcoord(number_state) 
    #print("iCnt_play:{}, iCnt_turn:{}, a_row:{}, a_col:{}, a_order:{}, reward:{}".format(iCnt_play, iCnt_turn, a_row, a_col, a_order, reward))

    # ---------------------------
    # スタートとゴールからの距離を計算する
    EuDist_S = np.sqrt((a_row - S[0])**2 + (a_col - S[1])**2)
    EuDist_G = np.sqrt((a_row - G[0])**2 + (a_col - G[1])**2)
    EuDist_T = EuDist_S + EuDist_G

    # ---------------------------
    # tsr_Dbaseを計算する
    if iCnt_turn <= len(temp_Dbase)-1:
        tsr_Dbase[iCnt_turn]  = temp_Dbase[iCnt_turn]
    else:
        tsr_Dbase[iCnt_turn]  = temp_Dbase[len(temp_Dbase)-1]

    # tsr_Dturnを計算する
    tsr_Dturn[iCnt_turn]    = EuDist_T

    # ---------------------------
    #print("tsr_Dturn: ",tsr_Dturn)
    #print("tsr_Dbase: ",tsr_Dbase)
    # newRTメトリックスを計算する(テンソル活用版)
    beta, snr = calc_newRT(L1_loss, tsr_Dturn, tsr_Dbase)

    # メトリックスのリストを生成する
    list_metrics = [a_row, a_col, beta*snr, EuDist_S, EuDist_G]

    # ---------------------------
    # 状態(STATE)を生成する
    state[0] = a_row
    state[1] = a_col
    state[2] = beta
    state[3] = snr
    state[4] = EuDist_S
    state[5] = EuDist_G
    
    # メトリックスのリストを生成する
    tsr_state = torch.tensor(state).float()

    return tsr_state, tsr_Dbase, tsr_Dturn, list_metrics

```

QEU:FOUNDER ： “それでは結果を見てみましょう。吉が出るのか・・・。”

**（学習曲線）**

![imageRL3-13-4](/2022-07-08-QEUR21_RLCLF13/imageRL3-13-4.jpg)

**（パフォーマンス推移）**

![imageRL3-13-5](/2022-07-08-QEUR21_RLCLF13/imageRL3-13-5.jpg)

D先生 : “やっぱりXY座標だけじゃ足らないんですよね。でも、RTメトリックスは学習の収束にそれなりに貢献していますね。この検討をメイズ（迷路）・プロジェクト前にやっておいてよかったですね。”

QEU:FOUNDER ： “は～、**XY座標だけじゃ無理**なのか・・・。全部のセルで状態(state)を使うのもいやだし・・・。”

D先生 : “などなど、さまざまな課題を抱えつつ、Cliff_Walkingのシリーズを終わります（笑）。”

## ～　まとめ　～

QEU:FOUNDER ： “大変ことになったよね。”

[![MOVIE1](http://img.youtube.com/vi/eOTJGtL2iiM/0.jpg)](http://www.youtube.com/watch?v=eOTJGtL2iiM "安倍元首相銃撃・容疑者は元海上自衛隊員。226，515事件との共通性と違い。犯人は銃器をカメラに偽装か？SPが油断した可能性も。元朝日新聞・記者佐藤章さんと一月万冊")

C部長 : “何で、元自〇隊の人が・・・。”

D先生 : “この演説スケジュールは急に決まったらしいんだが、なぜ・・・？”

QEU:FOUNDER ： “選挙のときに起きたのはまずいよね。今回のことが選挙結果に影響しないことを祈ります。昔、**TWで起きたことを思い出した**わ・・・。”

[![MOVIE2](http://img.youtube.com/vi/0LcEKXNTzs4/0.jpg)](http://www.youtube.com/watch?v=0LcEKXNTzs4 "時周：319槍擊疑案 子彈從扁車內射出")

D先生 : “今回とはかなり状況が違います。”

QEU:FOUNDER ： “ただし、**「こういうことが起きる時代になった」**と言えるよね。”
