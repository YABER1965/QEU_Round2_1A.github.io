---
title: QEUR21_RLCLF10 – 「崖歩き(Cliff_Walking)」でREINFORCEを使う (その1)
date: 2022-07-07
tags: ["QEUシステム", "メトリックス", "Python言語", "強化学習", "Cliff_Walking", "ディープラーニング"]
excerpt: Python言語によるCliff_Walkingの強化学習
---

## QEUR21_RLCLF10 – 「崖歩き(Cliff_Walking)」でREINFORCEを使う (その1)

## ～　そもそも、REINFORCE法の魅力は？　～

QEU:FOUNDER ： “POLICY-BASED法であるREINFORCEをやってみますか？”

![imageRL3-10-1](/2022-07-07-QEUR21_RMCLF10/imageRL3-10-1.jpg)

D先生 : “REINFORCEは記録することを目的にしましょう。前回と内容をあまり変えずに・・・。”

QEU:FOUNDER ： “今回の事例を通じて**「テクノメトリックスを使って、具体的に何ができるのか？」**の問いがかなり見えてきました。いま改めて見直すと、結果としてREINFORCEにはそれほど魅力がないですね。”

D先生 : “あれ？昔と言うことが変わりましたね・・・。”

QEU:FOUNDER ： “REINFORCEは**低いスペックの計算機上で軽快に動く**のが最大のメリットです。あとは、**正解の動作条件を直接インプットできる**ことかな？我々は、正解の動作条件を「アンチョコ」と呼んでいますが・・・（笑）。・・・でも、メトリックスをうまく使えば、DQN-ER(DQN with Experience Replay)でもかなりのことができるでしょう？しかも、それほど重くないし・・・。”

D先生 : “個人的には、REINFORCEにはまだまだ期待していますがね・・・。軽快なのが一番！！”

QEU:FOUNDER ： “じゃあもう一度、その有用性を検証しましょうか？プログラムをドン！！”

```python
# ----------------
# Cliff WalkingゲームのDL学習システム
# stepA0 : 強化学習をシンプルに使って行こう！！
# stepA0 : POLICY BASED法を有効に活用する
# stepA0_REINFORCE_cliffwalking_Simu(original).py
# REINFORCEに「アンチョコ」を加え、鉛筆をなめなめして使います（笑）
# ---------------- 
# 数値計算のﾗｲﾌﾞﾗﾘ読み込み
import gym
from gym.envs.toy_text import CliffWalkingEnv
import numpy as np
import random, time
import matplotlib.pyplot as plt
#%matplotlib inline
# -----
import torch
from torch.distributions import Categorical
import torch.nn as nn
import torch.optim as optim

# =================================================
# difinition of function
# =================================================
# 状態の表記を変更(oneHOTへ)
def calc_address(number_st):

    state = np.zeros(48)
    state[number_st] = 1.0

    return state

# 状態の表記を変更(2桁番号からi,jへ)
def calc_XYcoord(number_st):

    i = int(number_st/COLS + 0.01)
    j = int(number_st - i*COLS + 0.01)

    return int(i),int(j)

#=================================================
# Calculation class(1) : Pi
#=================================================
class Pi(nn.Module):
    def __init__(self, in_dim, out_dim):
        super(Pi, self).__init__()
        layers = [
            nn.Linear(in_dim,48),
            nn.ReLU(),
            nn.Linear(48, out_dim),
        ]
        self.model = nn.Sequential(*layers)
        self.onpolicy_reset()
        self.train()
        
    def onpolicy_reset(self):
        self.log_probs = []
        self.rewards = []

    def forward(self, x):
        pdparam = self.model(x)
        return pdparam
        
    def act(self, state):
        x = torch.from_numpy(state.astype(np.float32))
        pdparam = self.forward(x)
        pd = Categorical(logits=pdparam)
        action = pd.sample()
        log_prob = pd.log_prob(action)
        self.log_probs.append(log_prob)
        return action.item()

#=================================================
# Calculation class(2) : Agent
#=================================================
class Agent():
    def __init__(self):

        pi = Pi(in_dim, out_dim)
        optimizer = optim.Adam(pi.parameters(), lr=0.01)

        # ----------
        self.arr_loss = []
        self.arr_length = []
        self.arr_reward = []
        for epi in range(num_episodes):
            temp_state = env.reset()
            state = calc_address(temp_state)
            iStep = 0
            flag = False
            while flag == False:
                action = pi.act(state)
                
                # ------
                # 学習を早くするための<アンチョコ>
                if epi<100:
                    epsilon = 0.8
                elif epi >= 100 and epi < 300:
                    epsilon = 0.5  
                elif epi >= 300 and epi < 2000:
                    epsilon = 0.3  
                else:
                    epsilon = 0.01              

                # ----------
                # 移動命令
                #	0, UP
                #	1, RIGHT
                #	2, DOWN
                #	3, LEFT
                # ----------
                if np.random.random() < epsilon:
                    # 状態の表記を変更(2桁番号からi,jへ)
                    a_row, a_col = calc_XYcoord(temp_state) 
                    if a_col == 0:
                        action = 0
                    elif a_row == 0:
                        action = 2  
                    elif a_col == 11:
                        action = 2  
                    else:
                        action = 1  
                #print(state)
                
                temp_state, reward, done, _ = env.step(action)
                state = calc_address(temp_state)
                pi.rewards.append(reward)
                #env.render()
                if done or iStep > 300:
                    flag = True
                else:
                    iStep = iStep + 1
            loss = self.train(pi, optimizer)
            total_reward = sum(pi.rewards)
            # ----------
            self.arr_loss.append(loss)
            self.arr_length.append(iStep)
            self.arr_reward.append(total_reward)
            solved = total_reward > -20
            pi.onpolicy_reset()
            if epi%50 == 0:
                print(f"Episode {epi}, iStep: {iStep}, loss: {loss}, total_reward: {total_reward}, solved; {solved}")

        # ----------------
        # 学習結果のグラフ化
        self.show_graph()

        # ---------------------------
        # モデルの保存
        torch.save(pi.model.state_dict(), file_output_model)

    # ----------------
    # 学習(REINFORCE)
    def train(self, pi, optimizer):
        # Inner gradient-ascent loop of REINFORCE algorithm
        T = len(pi.rewards)
        rets = np.empty(T, dtype=np.float32)
        future_ret = 0.0
        
        # compute the returns efficiently
        for t in reversed(range(T)):
            future_ret = pi.rewards[t] + gamma * future_ret
            rets[t] = future_ret
            
        rets = torch.tensor(rets)
        log_probs = torch.stack(pi.log_probs)
        loss = -log_probs * rets
        loss = torch.sum(loss)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        return loss

    # ----------------
    # 学習結果のグラフ化
    def show_graph(self):
    
        x = list(range(num_episodes))
        #print(x)

        fig = plt.figure(figsize=(12,8))
        # -----
        ax1 = fig.add_subplot(2,2,1)
        ax1.set_title('learning transition : loss')
        ax1.set_xlabel('episode')
        ax1.set_ylabel('loss')
        ax1.grid(True)
        ax1.plot(x, self.arr_loss)

        # -----
        ax2 = fig.add_subplot(2,2,2)
        ax2.set_title('learning transition : length')
        ax2.set_xlabel('episode')
        ax2.set_ylabel('length')
        ax2.grid(True)
        ax2.plot(x, self.arr_length)

        # -----
        ax3 = fig.add_subplot(2,2,3)
        ax3.set_title('learning transition : reward')
        ax3.set_xlabel('episode')
        ax3.set_ylabel('reward')
        ax3.grid(True)
        ax3.plot(x, self.arr_reward)

        # -----
        fig.tight_layout()
        #fig.savefig("./REINFORCE_img.png")
        plt.show()

#=================================================
# main function            
#=================================================
if __name__ == "__main__":

    # ----------
    num_episodes = 3000
    ROWS = 4
    COLS = 12
    S = (3, 0)
    G = (3, 11)

    # ----------
    # 割引率の設定
    gamma = 0.99

    # ----------
    # 環境の導入
    env = CliffWalkingEnv()
    in_dim = 48
    out_dim = 4
    
    # ---------------------------
    # フォルダ名の指定
    foldername = "./"  # My project folder

    # --------------------------------------------------
    # 出力用Pytorchモデルのファイル名
    comment_output_model = "cliffwalking_REINFORCE"
    code_output_model = "model_cliffwalking_REINFORCE.pt"
    file_output_model = foldername + code_output_model  # ファイルパス名の生成

    # ----------
    # 学習の実行
    Agent()

```

QEU:FOUNDER ： “それでは、学習した結果を見てみましょう。いやぁ・・・、軽快だ・・・。”

**（学習曲線）**

![imageRL3-10-2](/2022-07-07-QEUR21_RMCLF10/imageRL3-10-2.jpg)

D先生 : “これは、「アンチョコあり」でしょ？じゃあ、**「アンチョコなし」じゃどうなる**んですか？”

QEU:FOUNDER ： “全然収束しない(笑)。Cliff_Walkingみたいに簡単な環境でだよ・・・。さすがに、こういうのはいただけない。ここら辺が、REINFORCEについてちょっと否定的になった理由なんですよね。”

D先生 : “でも、今は我々はメトリックスを前よりもうまく使えるでしょ？DQN-ERのプロジェクトのように関数を変えて学習を収束させられませんか？例えば…。”

- **当初　：　ゲーム盤の大きさ（48）**
- **現在　：　ゲーム盤の大きさ（48）　+　マンハッタン距離(1)　+　S距離(1)　+　G距離(1)**

QEU:FOUNDER ： “ 次につづく(笑)。”

## ～　まとめ　～

### ・・・　前回の続きです　・・・

QEU:FOUNDER ： “EUにおけるクルマの電動（EV）化は2035年まで延びました。・・・ということは、あと13年しかない。”

[![MOVIE1](http://img.youtube.com/vi/akXi3p9rC0A/0.jpg)](http://www.youtube.com/watch?v=akXi3p9rC0A "(中字)日本時日在倒數！要求更改碳排目標因關乎國家發展前途！背後被車商要挾控制？！日本汽車行業領先全球為何純電動卻如此落後？轉型失敗造車工業將面臨滅亡！【預告】")

D先生 : “**EV化するとサプライチェーンは全く変わります**からね・・・。”

QEU:FOUNDER ： “**13年は短い**ねぇ・・・。組織はEV化に適応して変われるのかね？”

D先生 : “当然、生存が無理な組織もあるでしょう。FOUNDERのお気に入りのメディアはどういっていましたか？”

QEU:FOUNDER ： “J国には、以下のアドバンテージがあるそうです。”

- **自動車**
- **エレクトロニクス**
- **工作機械**
- **観光（インバウンド）**

D先生 : “なに？エレクトロニクス？”

QEU:FOUNDER ： “小生も動画を見ていてビックリしたが・・・。まあ、これについてコメントはやめよう。何とも言えない・・・。工作機械もアドバンテージがあるのは精密加工機のみでしょう。自動化機器は逆に弱いんじゃないかね。”

D先生 : “自動化機器となると、どうしてもITやDigital twinsが絡んでくるから・・・。”

![imageRL3-10-3](/2022-07-07-QEUR21_RMCLF10/imageRL3-10-3.jpg)

QEU:FOUNDER ： “結局、問題点は「そういうこと」になるんじゃないかなぁ・・・。”
